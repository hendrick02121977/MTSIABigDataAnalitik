{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 5: Praproses Data (Data Preprocessing)\n",
    "\n",
    "**Mata Kuliah:** Big Data Analitik  \n",
    "**Topik:** Cleaning, Transformasi, Normalisasi, dan Feature Engineering  \n",
    "**Tujuan:** Mahasiswa mampu membersihkan dan mempersiapkan data kotor menjadi data siap analisis\n",
    "\n",
    "---\n",
    "\n",
    "*Week 5 Lab: Data Preprocessing*  \n",
    "*Topics covered: Missing values, duplicates, outliers, normalization, encoding, sklearn Pipeline*"
   ],
   "id": "c1b2c3d4e5f60001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Backend non-interaktif untuk kompatibilitas\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pengaturan tampilan\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Library berhasil diimport!\")\n",
    "print(f\"pandas  : {pd.__version__}\")\n",
    "print(f\"numpy   : {np.__version__}\")"
   ],
   "id": "c1b2c3d4e5f60002"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Membuat Dataset Kotor (Simulasi)\n",
    "\n",
    "Kita akan membuat dataset **karyawan perusahaan** yang sengaja mengandung berbagai masalah kualitas data yang umum ditemukan di dunia nyata:\n",
    "- Missing values\n",
    "- Data duplikat\n",
    "- Outlier ekstrem\n",
    "- Inkonsistensi format"
   ],
   "id": "c1b2c3d4e5f60003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "# Generate data dasar\n",
    "departemen_list = [\"IT\", \"HR\", \"Finance\", \"Marketing\", \"Operations\"]\n",
    "kota_list = [\"Jakarta\", \"Bandung\", \"Surabaya\", \"Medan\", \"Semarang\"]\n",
    "pendidikan_list = [\"SMA\", \"D3\", \"S1\", \"S2\", \"S3\"]\n",
    "status_list = [\"Aktif\", \"Cuti\", \"Kontrak\"]\n",
    "gender_list = [\"Laki-laki\", \"Perempuan\"]\n",
    "\n",
    "df_kotor = pd.DataFrame({\n",
    "    \"id_karyawan\": range(1001, 1001 + n),\n",
    "    \"nama\": [f\"Karyawan_{i:03d}\" for i in range(1, n + 1)],\n",
    "    \"usia\": np.random.randint(22, 58, n).astype(float),\n",
    "    \"gender\": np.random.choice(gender_list, n),\n",
    "    \"departemen\": np.random.choice(departemen_list, n),\n",
    "    \"kota\": np.random.choice(kota_list, n),\n",
    "    \"pendidikan\": np.random.choice(pendidikan_list, n),\n",
    "    \"gaji\": np.random.normal(8_000_000, 3_000_000, n).round(-3),\n",
    "    \"tahun_bergabung\": np.random.randint(2010, 2024, n),\n",
    "    \"skor_kinerja\": np.random.uniform(60, 100, n).round(1),\n",
    "    \"status\": np.random.choice(status_list, n),\n",
    "})\n",
    "\n",
    "# Suntikkan masalah kualitas data\n",
    "\n",
    "# 1. Missing values acak (~12% dari data)\n",
    "for col, rate in [(\"usia\", 0.08), (\"gaji\", 0.10), (\"skor_kinerja\", 0.07), (\"pendidikan\", 0.05)]:\n",
    "    idx = np.random.choice(df_kotor.index, size=int(n * rate), replace=False)\n",
    "    df_kotor.loc[idx, col] = np.nan\n",
    "\n",
    "# 2. Outlier ekstrem (nilai tidak masuk akal)\n",
    "df_kotor.loc[np.random.choice(df_kotor.index, 5), \"usia\"] = np.random.choice([150, 200, -5, 999], 5)\n",
    "df_kotor.loc[np.random.choice(df_kotor.index, 4), \"gaji\"] = np.random.choice([500_000_000, -1_000_000], 4)\n",
    "\n",
    "# 3. Inkonsistensi format\n",
    "idx_inkon = np.random.choice(df_kotor.index, 30)\n",
    "df_kotor.loc[idx_inkon[:10], \"kota\"] = df_kotor.loc[idx_inkon[:10], \"kota\"].str.upper()\n",
    "df_kotor.loc[idx_inkon[10:20], \"kota\"] = df_kotor.loc[idx_inkon[10:20], \"kota\"].str.lower()\n",
    "df_kotor.loc[idx_inkon[20:25], \"gender\"] = df_kotor.loc[idx_inkon[20:25], \"gender\"].map(\n",
    "    {\"Laki-laki\": \"L\", \"Perempuan\": \"P\"}\n",
    ")\n",
    "df_kotor.loc[idx_inkon[25:], \"status\"] = df_kotor.loc[idx_inkon[25:], \"status\"].map(\n",
    "    {\"Aktif\": \"aktif\", \"Cuti\": \"cuti\", \"Kontrak\": \"kontrak\"}\n",
    ")\n",
    "\n",
    "# 4. Duplikat (salin 15 baris)\n",
    "idx_dup = np.random.choice(df_kotor.index, 15, replace=False)\n",
    "df_kotor = pd.concat([df_kotor, df_kotor.loc[idx_dup]], ignore_index=True)\n",
    "\n",
    "# 5. Spasi berlebih di nama kota\n",
    "idx_spasi = np.random.choice(df_kotor.index, 10)\n",
    "df_kotor.loc[idx_spasi, \"kota\"] = \"  \" + df_kotor.loc[idx_spasi, \"kota\"] + \"  \"\n",
    "\n",
    "print(f\"Dataset kotor berhasil dibuat!\")\n",
    "print(f\"Shape: {df_kotor.shape}  (baris x kolom)\")\n",
    "print(\"\\n5 baris pertama:\")\n",
    "print(df_kotor.head())"
   ],
   "id": "c1b2c3d4e5f60004"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Eksplorasi Awal Data Kotor\n",
    "\n",
    "Sebelum membersihkan data, kita perlu memahami masalah yang ada melalui **Exploratory Data Analysis (EDA)**."
   ],
   "id": "c1b2c3d4e5f60005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  EKSPLORASI AWAL DATA KOTOR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- df.info() ---\")\n",
    "df_kotor.info()\n",
    "\n",
    "print(\"\\n--- df.describe() (kolom numerik) ---\")\n",
    "print(df_kotor.describe())\n",
    "\n",
    "print(\"\\n--- Jumlah Missing Values per Kolom ---\")\n",
    "missing = df_kotor.isnull().sum()\n",
    "missing_persen = (missing / len(df_kotor) * 100).round(2)\n",
    "df_missing = pd.DataFrame({\n",
    "    \"jumlah_missing\": missing,\n",
    "    \"persentase (%)\": missing_persen\n",
    "})\n",
    "print(df_missing[df_missing[\"jumlah_missing\"] > 0])\n",
    "\n",
    "print(f\"\\n--- Jumlah Duplikat: {df_kotor.duplicated().sum()} baris ---\")\n",
    "\n",
    "print(\"\\n--- Nilai Unik Kolom Kategorik ---\")\n",
    "for col in [\"gender\", \"kota\", \"status\"]:\n",
    "    print(f\"  {col:15} : {sorted(df_kotor[col].dropna().unique())}\")\n",
    "\n",
    "print(\"\\n--- Statistik Nilai Ekstrem ---\")\n",
    "print(f\"  Usia: min={df_kotor['usia'].min()}, max={df_kotor['usia'].max()}\")\n",
    "print(f\"  Gaji: min={df_kotor['gaji'].min():,.0f}, max={df_kotor['gaji'].max():,.0f}\")\n",
    "\n",
    "# Visualisasi missing values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart missing values\n",
    "missing_nonzero = missing[missing > 0]\n",
    "axes[0].bar(missing_nonzero.index, missing_nonzero.values, color='coral', edgecolor='black')\n",
    "axes[0].set_title(\"Jumlah Missing Values per Kolom\", fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Kolom\")\n",
    "axes[0].set_ylabel(\"Jumlah\")\n",
    "for i, v in enumerate(missing_nonzero.values):\n",
    "    axes[0].text(i, v + 0.3, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Distribusi gaji sebelum cleaning\n",
    "gaji_valid = df_kotor['gaji'].dropna()\n",
    "axes[1].hist(gaji_valid, bins=30, color='steelblue', edgecolor='white')\n",
    "axes[1].set_title(\"Distribusi Gaji (Sebelum Cleaning)\", fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Gaji (Rp)\")\n",
    "axes[1].set_ylabel(\"Frekuensi\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"eksplorasi_awal.png\", dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nVisualisasi disimpan: eksplorasi_awal.png\")"
   ],
   "id": "c1b2c3d4e5f60006"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Menangani Missing Values\n",
    "\n",
    "Berbagai strategi untuk menangani nilai kosong, dari yang paling sederhana (hapus baris) hingga yang lebih canggih (KNN Imputation)."
   ],
   "id": "c1b2c3d4e5f60007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_kotor.copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  PENANGANAN MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sebelum\n",
    "print(f\"\\nShape sebelum: {df_clean.shape}\")\n",
    "print(f\"Total missing : {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# --- Teknik 1: dropna (hapus baris dengan missing values) ---\n",
    "df_dropna = df_clean.dropna()\n",
    "print(f\"\\nSetelah dropna()       : {df_dropna.shape} baris (kehilangan {len(df_clean) - len(df_dropna)} baris)\")\n",
    "\n",
    "# --- Teknik 2: fillna dengan statistik ---\n",
    "df_fill = df_clean.copy()\n",
    "\n",
    "# Mean untuk kolom numerik yang relatif normal\n",
    "mean_skor = df_fill['skor_kinerja'].mean()\n",
    "df_fill['skor_kinerja'] = df_fill['skor_kinerja'].fillna(mean_skor)\n",
    "print(f\"\\nfillna(mean) untuk skor_kinerja: {mean_skor:.2f}\")\n",
    "\n",
    "# Median lebih robust terhadap outlier\n",
    "median_usia = df_fill['usia'].median()\n",
    "df_fill['usia'] = df_fill['usia'].fillna(median_usia)\n",
    "print(f\"fillna(median) untuk usia      : {median_usia}\")\n",
    "\n",
    "# Mode untuk kolom kategorik\n",
    "mode_pendidikan = df_fill['pendidikan'].mode()[0]\n",
    "df_fill['pendidikan'] = df_fill['pendidikan'].fillna(mode_pendidikan)\n",
    "print(f\"fillna(mode) untuk pendidikan  : {mode_pendidikan}\")\n",
    "\n",
    "# Forward fill untuk kolom gaji\n",
    "df_fill['gaji'] = df_fill['gaji'].ffill()\n",
    "print(f\"ffill() untuk gaji             : menggunakan nilai sebelumnya\")\n",
    "\n",
    "# --- Teknik 3: SimpleImputer dari sklearn ---\n",
    "print(\"\\n--- SimpleImputer (sklearn) ---\")\n",
    "df_impute = df_clean.copy()\n",
    "kolom_numerik_impute = [\"usia\", \"gaji\", \"skor_kinerja\"]\n",
    "\n",
    "imputer_median = SimpleImputer(strategy=\"median\")\n",
    "df_impute[kolom_numerik_impute] = imputer_median.fit_transform(df_impute[kolom_numerik_impute])\n",
    "\n",
    "imputer_mode = SimpleImputer(strategy=\"most_frequent\")\n",
    "df_impute[[\"pendidikan\"]] = imputer_mode.fit_transform(df_impute[[\"pendidikan\"]])\n",
    "\n",
    "print(f\"SimpleImputer diterapkan pada: {kolom_numerik_impute + ['pendidikan']}\")\n",
    "print(f\"Missing values tersisa       : {df_impute.isnull().sum().sum()}\")\n",
    "\n",
    "# Gunakan df_fill sebagai basis untuk langkah selanjutnya\n",
    "df_clean = df_fill.copy()\n",
    "print(f\"\\nShape setelah imputasi: {df_clean.shape}\")\n",
    "print(f\"Total missing tersisa : {df_clean.isnull().sum().sum()}\")"
   ],
   "id": "c1b2c3d4e5f60008"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Menangani Duplikat & Outlier\n",
    "\n",
    "Penghapusan duplikat dan deteksi outlier menggunakan **metode IQR** dan **Z-Score**."
   ],
   "id": "c1b2c3d4e5f60009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  MENANGANI DUPLIKAT & OUTLIER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- 1. Hapus Duplikat ---\n",
    "print(f\"\\nJumlah duplikat sebelum: {df_clean.duplicated().sum()}\")\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f\"Jumlah duplikat setelah: {df_clean.duplicated().sum()}\")\n",
    "print(f\"Shape setelah hapus duplikat: {df_clean.shape}\")\n",
    "\n",
    "# --- 2. Perbaiki Inkonsistensi Sebelum Outlier ---\n",
    "print(\"\\n--- Perbaiki Inkonsistensi Format ---\")\n",
    "# Normalisasi kota\n",
    "df_clean['kota'] = df_clean['kota'].str.strip().str.title()\n",
    "# Normalisasi gender\n",
    "df_clean['gender'] = df_clean['gender'].replace({'L': 'Laki-laki', 'P': 'Perempuan'})\n",
    "# Normalisasi status\n",
    "df_clean['status'] = df_clean['status'].str.capitalize()\n",
    "print(f\"  Kota unik : {sorted(df_clean['kota'].unique())}\")\n",
    "print(f\"  Gender unik: {sorted(df_clean['gender'].unique())}\")\n",
    "print(f\"  Status unik: {sorted(df_clean['status'].unique())}\")\n",
    "\n",
    "# --- 3. Deteksi & Hapus Outlier — Metode IQR ---\n",
    "print(\"\\n--- Metode IQR (Interquartile Range) ---\")\n",
    "\n",
    "def deteksi_outlier_iqr(series, nama_kolom):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    batas_bawah = Q1 - 1.5 * IQR\n",
    "    batas_atas  = Q3 + 1.5 * IQR\n",
    "    outlier = (series < batas_bawah) | (series > batas_atas)\n",
    "    print(f\"  {nama_kolom:20} | Q1={Q1:.1f} | Q3={Q3:.1f} | IQR={IQR:.1f} \"\n",
    "          f\"| Batas: [{batas_bawah:.1f}, {batas_atas:.1f}] \"\n",
    "          f\"| Outlier: {outlier.sum()}\")\n",
    "    return outlier, batas_bawah, batas_atas\n",
    "\n",
    "for kolom in [\"usia\", \"gaji\", \"skor_kinerja\"]:\n",
    "    outlier_mask, bb, ba = deteksi_outlier_iqr(df_clean[kolom].dropna(), kolom)\n",
    "\n",
    "# Hapus outlier pada kolom usia dan gaji\n",
    "print(f\"\\nShape sebelum hapus outlier: {df_clean.shape}\")\n",
    "for kolom in [\"usia\", \"gaji\"]:\n",
    "    Q1 = df_clean[kolom].quantile(0.25)\n",
    "    Q3 = df_clean[kolom].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_clean = df_clean[\n",
    "        (df_clean[kolom] >= Q1 - 1.5 * IQR) &\n",
    "        (df_clean[kolom] <= Q3 + 1.5 * IQR)\n",
    "    ]\n",
    "print(f\"Shape setelah hapus outlier : {df_clean.shape}\")\n",
    "\n",
    "# --- 4. Deteksi Outlier — Metode Z-Score ---\n",
    "print(\"\\n--- Metode Z-Score ---\")\n",
    "z_skor_gaji = np.abs(stats.zscore(df_clean['gaji'].dropna()))\n",
    "n_outlier_zscore = (z_skor_gaji > 3).sum()\n",
    "print(f\"  Outlier gaji (|z| > 3) : {n_outlier_zscore} baris\")\n",
    "\n",
    "z_skor_usia = np.abs(stats.zscore(df_clean['usia'].dropna()))\n",
    "n_outlier_usia = (z_skor_usia > 3).sum()\n",
    "print(f\"  Outlier usia (|z| > 3) : {n_outlier_usia} baris\")\n",
    "\n",
    "# Visualisasi sebelum vs setelah\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].hist(df_kotor['usia'].dropna(), bins=30, color='coral', edgecolor='white')\n",
    "axes[0, 0].set_title('Usia — Sebelum Cleaning')\n",
    "axes[0, 1].hist(df_clean['usia'].dropna(), bins=30, color='mediumseagreen', edgecolor='white')\n",
    "axes[0, 1].set_title('Usia — Setelah Cleaning')\n",
    "axes[1, 0].hist(df_kotor['gaji'].dropna(), bins=30, color='coral', edgecolor='white')\n",
    "axes[1, 0].set_title('Gaji — Sebelum Cleaning')\n",
    "axes[1, 1].hist(df_clean['gaji'].dropna(), bins=30, color='mediumseagreen', edgecolor='white')\n",
    "axes[1, 1].set_title('Gaji — Setelah Cleaning')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_ylabel('Frekuensi')\n",
    "plt.suptitle('Perbandingan Distribusi: Sebelum vs Setelah Cleaning', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outlier_cleaning.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nVisualisasi disimpan: outlier_cleaning.png\")"
   ],
   "id": "c1b2c3d4e5f60010"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformasi & Normalisasi\n",
    "\n",
    "Transformasi data untuk memastikan semua fitur numerik berada pada skala yang sebanding dan fitur kategorik dikonversi ke format numerik."
   ],
   "id": "c1b2c3d4e5f60011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform = df_clean.copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  TRANSFORMASI & NORMALISASI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "kolom_num = [\"usia\", \"gaji\", \"skor_kinerja\"]\n",
    "\n",
    "# --- 1. Min-Max Normalization ---\n",
    "print(\"\\n--- Min-Max Normalization (rentang [0, 1]) ---\")\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(df_transform[kolom_num]),\n",
    "    columns=[f\"{c}_minmax\" for c in kolom_num]\n",
    ")\n",
    "print(\"Sebelum (5 baris):\")\n",
    "print(df_transform[kolom_num].head().to_string())\n",
    "print(\"\\nSetelah Min-Max (5 baris):\")\n",
    "print(df_minmax.head().to_string())\n",
    "print(f\"\\nRange setelah Min-Max — min: {df_minmax.min().round(4).to_dict()}\")\n",
    "print(f\"Range setelah Min-Max — max: {df_minmax.max().round(4).to_dict()}\")\n",
    "\n",
    "# --- 2. Z-Score Standardization ---\n",
    "print(\"\\n--- Z-Score Standardization (mean=0, std=1) ---\")\n",
    "scaler_std = StandardScaler()\n",
    "df_zscore = pd.DataFrame(\n",
    "    scaler_std.fit_transform(df_transform[kolom_num]),\n",
    "    columns=[f\"{c}_zscore\" for c in kolom_num]\n",
    ")\n",
    "print(\"Statistik setelah Z-Score:\")\n",
    "print(df_zscore.describe().loc[[\"mean\", \"std\"]].round(4))\n",
    "\n",
    "# --- 3. Label Encoding ---\n",
    "print(\"\\n--- Label Encoding (untuk variabel ordinal) ---\")\n",
    "urutan_pendidikan = {\"SMA\": 0, \"D3\": 1, \"S1\": 2, \"S2\": 3, \"S3\": 4}\n",
    "df_transform['pendidikan_encoded'] = df_transform['pendidikan'].map(urutan_pendidikan)\n",
    "print(df_transform[[\"pendidikan\", \"pendidikan_encoded\"]].drop_duplicates().sort_values(\"pendidikan_encoded\"))\n",
    "\n",
    "# --- 4. One-Hot Encoding ---\n",
    "print(\"\\n--- One-Hot Encoding (untuk variabel nominal) ---\")\n",
    "print(f\"Kolom sebelum OHE: {list(df_transform.columns)}\")\n",
    "df_ohe = pd.get_dummies(df_transform, columns=[\"departemen\", \"kota\"], prefix=[\"dept\", \"kota\"], dtype=int)\n",
    "kolom_ohe_baru = [c for c in df_ohe.columns if c.startswith(\"dept_\") or c.startswith(\"kota_\")]\n",
    "print(f\"Kolom OHE baru   : {kolom_ohe_baru}\")\n",
    "print(f\"Shape setelah OHE: {df_ohe.shape}\")\n",
    "\n",
    "# Visualisasi perbandingan distribusi\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, (col, scaler_name) in enumerate([\n",
    "    (\"gaji\", \"Original\"),\n",
    "    (\"gaji_minmax\", \"Min-Max\"),\n",
    "    (\"gaji_zscore\", \"Z-Score\")\n",
    "]):\n",
    "    data_plot = df_transform[\"gaji\"] if col == \"gaji\" else (df_minmax[col] if \"minmax\" in col else df_zscore[col])\n",
    "    axes[i].hist(data_plot.dropna(), bins=25, edgecolor='white',\n",
    "                 color=['steelblue', 'darkorange', 'mediumseagreen'][i])\n",
    "    axes[i].set_title(f\"Gaji — {scaler_name}\", fontweight='bold')\n",
    "    axes[i].set_ylabel('Frekuensi')\n",
    "plt.suptitle('Perbandingan Normalisasi Kolom Gaji', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('normalisasi_perbandingan.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nVisualisasi disimpan: normalisasi_perbandingan.png\")"
   ],
   "id": "c1b2c3d4e5f60012"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ringkasan Pipeline Praproses\n",
    "\n",
    "Menggabungkan semua langkah praproses ke dalam **scikit-learn Pipeline** yang dapat direproduksi. Pipeline memastikan tidak ada **data leakage** dan mempermudah penerapan ke data baru."
   ],
   "id": "c1b2c3d4e5f60013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  PIPELINE PRAPROSES LENGKAP (sklearn)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Siapkan dataset bersih untuk pipeline\n",
    "df_pipeline_input = df_kotor.copy()\n",
    "\n",
    "# Perbaiki inkonsistensi terlebih dahulu (tahap pra-pipeline)\n",
    "df_pipeline_input['kota'] = df_pipeline_input['kota'].str.strip().str.title()\n",
    "df_pipeline_input['gender'] = df_pipeline_input['gender'].replace({'L': 'Laki-laki', 'P': 'Perempuan'})\n",
    "df_pipeline_input['status'] = df_pipeline_input['status'].str.capitalize()\n",
    "df_pipeline_input = df_pipeline_input.drop_duplicates()\n",
    "\n",
    "# Definisi kolom berdasarkan tipe\n",
    "kolom_numerik   = [\"usia\", \"gaji\", \"skor_kinerja\"]\n",
    "kolom_kategorik = [\"departemen\", \"kota\", \"gender\", \"status\"]\n",
    "\n",
    "# Pilih hanya kolom yang akan diproses\n",
    "X = df_pipeline_input[kolom_numerik + kolom_kategorik].copy()\n",
    "print(f\"\\nInput shape: {X.shape}\")\n",
    "print(f\"Kolom numerik  : {kolom_numerik}\")\n",
    "print(f\"Kolom kategorik: {kolom_kategorik}\")\n",
    "\n",
    "# Definisi sub-pipeline\n",
    "pipeline_numerik = Pipeline(steps=[\n",
    "    (\"imputasi\",     SimpleImputer(strategy=\"median\")),\n",
    "    (\"normalisasi\",  MinMaxScaler())\n",
    "])\n",
    "\n",
    "pipeline_kategorik = Pipeline(steps=[\n",
    "    (\"imputasi\",  SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoding\",  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Gabungkan dengan ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", pipeline_numerik,   kolom_numerik),\n",
    "        (\"cat\", pipeline_kategorik, kolom_kategorik),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Fit dan Transform\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Dapatkan nama kolom hasil\n",
    "nama_kolom_cat = preprocessor.named_transformers_['cat']['encoding'].get_feature_names_out(kolom_kategorik)\n",
    "nama_kolom_hasil = kolom_numerik + list(nama_kolom_cat)\n",
    "\n",
    "df_hasil_pipeline = pd.DataFrame(X_processed, columns=nama_kolom_hasil)\n",
    "\n",
    "print(f\"\\nOutput shape   : {df_hasil_pipeline.shape}\")\n",
    "print(f\"Missing values : {df_hasil_pipeline.isnull().sum().sum()}\")\n",
    "print(\"\\n5 baris pertama (kolom numerik):\")\n",
    "print(df_hasil_pipeline[kolom_numerik].head().to_string())\n",
    "print(\"\\n5 baris pertama (kolom OHE departemen):\")\n",
    "kolom_dept = [c for c in df_hasil_pipeline.columns if c.startswith(\"departemen_\")]\n",
    "print(df_hasil_pipeline[kolom_dept].head().to_string())\n",
    "\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"  RINGKASAN PERBANDINGAN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset awal   : {df_kotor.shape}\")\n",
    "print(f\"Dataset bersih : {df_clean.shape}\")\n",
    "print(f\"Dataset hasil pipeline: {df_hasil_pipeline.shape}\")\n",
    "print(f\"\\nMissing values awal   : {df_kotor.isnull().sum().sum()}\")\n",
    "print(f\"Missing values bersih : {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Missing values pipeline: {df_hasil_pipeline.isnull().sum().sum()}\")\n",
    "print(f\"\\nDuplikat awal         : {df_kotor.duplicated().sum()}\")\n",
    "print(f\"Duplikat bersih       : {df_clean.duplicated().sum()}\")\n",
    "print(\"\\n✓ Pipeline praproses data berhasil dijalankan!\")"
   ],
   "id": "c1b2c3d4e5f60014"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Kerjakan soal-soal berikut secara mandiri:\n",
    "\n",
    "**Soal 1 — Analisis Missing Values:**  \n",
    "Buat dataset baru dengan minimal 150 baris dan 6 kolom (campuran numerik & kategorik). Suntikkan missing values dengan pola berbeda (MCAR vs simulasi MAR). Bandingkan hasil imputasi menggunakan: (a) mean/mode, (b) median, dan (c) KNNImputer. Evaluasi mana yang paling mempertahankan distribusi asli.\n",
    "\n",
    "**Soal 2 — Outlier Detection:**  \n",
    "Gunakan dataset dari Soal 1. Deteksi outlier pada semua kolom numerik menggunakan:\n",
    "- Metode IQR (batas 1.5×IQR dan 3×IQR)\n",
    "- Metode Z-Score (ambang batas 2 dan 3)\n",
    "- Visualisasikan dengan boxplot sebelum dan sesudah penghapusan outlier\n",
    "\n",
    "**Soal 3 — Feature Engineering:**  \n",
    "Dengan dataset karyawan yang sudah dibuat, lakukan feature engineering:\n",
    "- Buat kolom `lama_kerja` (tahun saat ini − tahun_bergabung)\n",
    "- Buat kolom `kelompok_usia` (kategori: Muda 22-30, Dewasa 31-45, Senior 46+)\n",
    "- Buat kolom `grade_gaji` berdasarkan kuartil gaji (Q1=Bronze, Q2=Silver, Q3=Gold, Q4=Platinum)\n",
    "\n",
    "**Soal 4 — Pipeline Lengkap:**  \n",
    "Buat sklearn Pipeline yang mencakup: imputasi missing values → penghapusan outlier → normalisasi → encoding. Terapkan pada 80% data (training set) dan transformasikan 20% sisanya (test set) menggunakan pipeline yang sudah di-fit. Pastikan tidak ada data leakage.\n",
    "\n",
    "**Soal 5 — Studi Kasus End-to-End:**  \n",
    "Download dataset publik dari kaggle atau UCI Repository (contoh: Titanic, Iris, atau House Prices). Terapkan seluruh tahap praproses yang telah dipelajari: eksplorasi → cleaning → transformasi → feature engineering. Dokumentasikan setiap keputusan yang Anda buat beserta alasannya."
   ],
   "id": "c1b2c3d4e5f60015"
  }
 ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Minggu02_Hadoop_MapReduce.ipynb"
    },
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üêò Praktikum Minggu 2: Simulasi Hadoop MapReduce dengan Python\n",
        "**Tujuan**: Memahami paradigma MapReduce dengan mengimplementasikannya menggunakan Python murni di Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 1. Implementasi MapReduce: Word Count"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from functools import reduce as functools_reduce\n",
        "import re\n",
        "\n",
        "# ==== IMPLEMENTASI MAPREDUCE ====\n",
        "\n",
        "def mapper_word_count(document):\n",
        "    \"\"\"MAP: Memecah dokumen menjadi pasangan (kata, 1)\"\"\"\n",
        "    words = re.findall(r'\\b[a-zA-Z]+\\b', document.lower())\n",
        "    return [(word, 1) for word in words]\n",
        "\n",
        "def shuffler(mapped_results):\n",
        "    \"\"\"SHUFFLE & SORT: Mengelompokkan nilai berdasarkan key\"\"\"\n",
        "    grouped = defaultdict(list)\n",
        "    for key, value in mapped_results:\n",
        "        grouped[key].append(value)\n",
        "    return dict(grouped)\n",
        "\n",
        "def reducer_word_count(key, values):\n",
        "    \"\"\"REDUCE: Menjumlahkan semua nilai untuk setiap kata\"\"\"\n",
        "    return (key, sum(values))\n",
        "\n",
        "# Data input (simulasi dokumen terdistribusi)\n",
        "documents = [\n",
        "    \"Big data is the future of analytics and data science\",\n",
        "    \"Machine learning and deep learning are part of data science\",\n",
        "    \"Big data analytics uses Hadoop Spark and cloud computing\",\n",
        "    \"Data science requires statistics machine learning and big data skills\"\n",
        "]\n",
        "\n",
        "print('=== INPUT DOCUMENTS ===')\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f'Doc {i}: {doc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAP PHASE\n",
        "print('=== MAP PHASE ===')\n",
        "all_mapped = []\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    mapped = mapper_word_count(doc)\n",
        "    all_mapped.extend(mapped)\n",
        "    print(f'Mapper {i}: {mapped[:5]}...')  # tampilkan 5 pertama\n",
        "\n",
        "print(f'\\nTotal key-value pairs: {len(all_mapped)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHUFFLE & SORT PHASE\n",
        "print('=== SHUFFLE & SORT PHASE ===')\n",
        "shuffled = shuffler(all_mapped)\n",
        "print('Kata unik ditemukan:', len(shuffled))\n",
        "print('\\nContoh hasil shuffle (5 kata pertama):')\n",
        "for i, (k, v) in enumerate(list(shuffled.items())[:5]):\n",
        "    print(f'  {k}: {v}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# REDUCE PHASE\n",
        "print('=== REDUCE PHASE ===')\n",
        "results = [reducer_word_count(k, v) for k, v in shuffled.items()]\n",
        "\n",
        "# Urutkan berdasarkan frekuensi\n",
        "results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "df_wc = pd.DataFrame(results_sorted, columns=['Kata', 'Frekuensi'])\n",
        "print('Top 15 kata terbanyak:')\n",
        "print(df_wc.head(15).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualisasi Word Count\n",
        "top15 = df_wc.head(15)\n",
        "plt.figure(figsize=(12, 5))\n",
        "bars = plt.bar(top15['Kata'], top15['Frekuensi'], color='steelblue', alpha=0.8)\n",
        "plt.title('Hasil MapReduce: Top 15 Kata Terbanyak', fontsize=13, fontweight='bold')\n",
        "plt.xlabel('Kata')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "for bar, val in zip(bars, top15['Frekuensi']):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "             str(val), ha='center', va='bottom', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 2. MapReduce: Analisis Penjualan"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Generate dataset penjualan\n",
        "random.seed(42)\n",
        "products = ['Laptop', 'Smartphone', 'Tablet', 'Headphone', 'Monitor']\n",
        "regions  = ['Jakarta', 'Surabaya', 'Bandung', 'Medan', 'Bali']\n",
        "\n",
        "sales_data = []\n",
        "base_date = datetime(2024, 1, 1)\n",
        "for i in range(1000):\n",
        "    sales_data.append({\n",
        "        'tanggal':  (base_date + timedelta(days=random.randint(0, 364))).strftime('%Y-%m-%d'),\n",
        "        'produk':   random.choice(products),\n",
        "        'region':   random.choice(regions),\n",
        "        'qty':      random.randint(1, 10),\n",
        "        'harga':    random.choice([5_000_000, 8_000_000, 3_500_000, 500_000, 4_000_000])\n",
        "    })\n",
        "\n",
        "df_sales = pd.DataFrame(sales_data)\n",
        "df_sales['total'] = df_sales['qty'] * df_sales['harga']\n",
        "print(f'Dataset penjualan: {len(df_sales)} transaksi')\n",
        "print(df_sales.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MapReduce: Total penjualan per produk per region\n",
        "\n",
        "def mapper_sales(record):\n",
        "    \"\"\"MAP: Key = (produk, region), Value = total penjualan\"\"\"\n",
        "    key   = (record['produk'], record['region'])\n",
        "    value = record['total']\n",
        "    return (key, value)\n",
        "\n",
        "def reducer_sales(key, values):\n",
        "    \"\"\"REDUCE: Jumlahkan total penjualan\"\"\"\n",
        "    return (key, sum(values))\n",
        "\n",
        "# MAP\n",
        "mapped_sales = [mapper_sales(r) for r in sales_data]\n",
        "\n",
        "# SHUFFLE\n",
        "shuffled_sales = defaultdict(list)\n",
        "for k, v in mapped_sales:\n",
        "    shuffled_sales[k].append(v)\n",
        "\n",
        "# REDUCE\n",
        "results_sales = [reducer_sales(k, v) for k, v in shuffled_sales.items()]\n",
        "\n",
        "# Format hasil\n",
        "df_result = pd.DataFrame([(k[0], k[1], v) for k, v in results_sales],\n",
        "                          columns=['Produk', 'Region', 'Total_Penjualan'])\n",
        "df_pivot = df_result.pivot(index='Produk', columns='Region', values='Total_Penjualan').fillna(0)\n",
        "df_pivot = df_pivot.applymap(lambda x: f'Rp {x/1e6:.1f}M')\n",
        "\n",
        "print('=== Hasil MapReduce: Total Penjualan per Produk per Region ===')\n",
        "print(df_pivot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tugas Praktikum Minggu 2\n",
        "1. Implementasikan MapReduce untuk menghitung **jumlah transaksi per kategori per bulan** dari dataset penjualan di atas.\n",
        "2. Implementasikan MapReduce untuk menemukan **produk dengan penjualan tertinggi di setiap region**.\n",
        "3. Buat visualisasi dari hasil analisis nomor 1 dan 2.\n",
        "\n",
        "**Hint**: Modifikasi fungsi `mapper_sales` dan `reducer_sales` sesuai kebutuhan."
      ]
    }
  ]
}

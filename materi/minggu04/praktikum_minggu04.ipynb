{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 4: Pengumpulan Data (Data Collection)\n",
    "\n",
    "**Mata Kuliah:** Big Data Analitik  \n",
    "**Topik:** Web Scraping, REST API, dan Simulasi Data Streaming  \n",
    "**Tujuan:** Mahasiswa mampu mengumpulkan data dari berbagai sumber menggunakan Python\n",
    "\n",
    "---\n",
    "\n",
    "*Week 4 Lab: Data Collection*  \n",
    "*Topics covered: Web scraping with BeautifulSoup, REST API with Requests, streaming simulation, data storage*"
   ],
   "id": "b1b2c3d4e5f60001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 lxml"
   ],
   "id": "b1b2c3d4e5f60002"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Scraping dengan Requests & BeautifulSoup\n",
    "\n",
    "Kita akan melakukan scraping dari **quotes.toscrape.com** — situs yang dirancang khusus untuk latihan web scraping. Situs ini berisi kutipan-kutipan beserta informasi penulis dan tag.\n",
    "\n",
    "**Alur kerja web scraping:**\n",
    "1. Kirim HTTP GET request ke URL target\n",
    "2. Terima respons HTML\n",
    "3. Parse HTML menggunakan BeautifulSoup\n",
    "4. Ekstrak data yang diinginkan menggunakan CSS selector atau tag HTML\n",
    "5. Simpan dalam format terstruktur"
   ],
   "id": "b1b2c3d4e5f60003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def scrape_quotes(base_url, max_pages=3):\n",
    "    \"\"\"Scrape kutipan dari quotes.toscrape.com.\"\"\"\n",
    "    semua_data = []\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 BigDataCourse-Scraper/1.0\"}\n",
    "\n",
    "    for halaman in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}/page/{halaman}/\"\n",
    "        try:\n",
    "            respons = requests.get(url, headers=headers, timeout=10)\n",
    "            respons.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Gagal mengakses halaman {halaman}: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(respons.text, \"lxml\")\n",
    "        kutipan_list = soup.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "        for kutipan in kutipan_list:\n",
    "            teks = kutipan.find(\"span\", class_=\"text\").get_text(strip=True)\n",
    "            penulis = kutipan.find(\"small\", class_=\"author\").get_text(strip=True)\n",
    "            tags = [tag.get_text(strip=True)\n",
    "                    for tag in kutipan.find_all(\"a\", class_=\"tag\")]\n",
    "            semua_data.append({\n",
    "                \"teks\": teks,\n",
    "                \"penulis\": penulis,\n",
    "                \"tags\": \", \".join(tags),\n",
    "                \"jumlah_tag\": len(tags),\n",
    "                \"halaman\": halaman\n",
    "            })\n",
    "\n",
    "        print(f\"  Halaman {halaman}: {len(kutipan_list)} kutipan ditemukan\")\n",
    "        time.sleep(0.5)  # Etika: jeda antar permintaan\n",
    "\n",
    "    return semua_data\n",
    "\n",
    "print(\"Memulai scraping dari quotes.toscrape.com ...\")\n",
    "data_kutipan = scrape_quotes(\"http://quotes.toscrape.com\", max_pages=3)\n",
    "\n",
    "df_kutipan = pd.DataFrame(data_kutipan)\n",
    "print(f\"\\nTotal kutipan berhasil di-scrape: {len(df_kutipan)}\")\n",
    "print(\"\\n=== 5 Kutipan Pertama ===\")\n",
    "print(df_kutipan[[\"penulis\", \"teks\", \"jumlah_tag\"]].head())\n",
    "\n",
    "print(\"\\n=== Penulis dengan Kutipan Terbanyak ===\")\n",
    "print(df_kutipan[\"penulis\"].value_counts().head(5))\n",
    "\n",
    "print(\"\\n=== Statistik Jumlah Tag ===\")\n",
    "print(df_kutipan[\"jumlah_tag\"].describe())"
   ],
   "id": "b1b2c3d4e5f60004"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing HTML Lokal / Simulasi\n",
    "\n",
    "Kadang kita perlu mem-parsing HTML yang sudah tersimpan secara lokal atau yang dihasilkan oleh aplikasi kita sendiri. Contoh ini mensimulasikan parsing tabel HTML."
   ],
   "id": "b1b2c3d4e5f60005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh HTML lokal dengan tabel data produk\n",
    "html_contoh = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>Daftar Produk Toko Online</title></head>\n",
    "<body>\n",
    "  <h1>Katalog Produk</h1>\n",
    "  <div id=\"info\">\n",
    "    <p class=\"update\">Terakhir diperbarui: 2024-01-15</p>\n",
    "    <p class=\"total\">Total produk: 5 item</p>\n",
    "  </div>\n",
    "  <table id=\"tabel-produk\">\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>ID</th><th>Nama Produk</th><th>Kategori</th>\n",
    "        <th>Harga (Rp)</th><th>Stok</th><th>Rating</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr><td>P001</td><td>Laptop UltraBook Pro</td><td>Elektronik</td><td>12500000</td><td>15</td><td>4.7</td></tr>\n",
    "      <tr><td>P002</td><td>Smartphone X200</td><td>Elektronik</td><td>3800000</td><td>42</td><td>4.5</td></tr>\n",
    "      <tr><td>P003</td><td>Meja Belajar Ergonomis</td><td>Furnitur</td><td>850000</td><td>8</td><td>4.3</td></tr>\n",
    "      <tr><td>P004</td><td>Headphone Noise Cancelling</td><td>Elektronik</td><td>1200000</td><td>25</td><td>4.6</td></tr>\n",
    "      <tr><td>P005</td><td>Buku Python Data Science</td><td>Buku</td><td>185000</td><td>100</td><td>4.8</td></tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "  <ul class=\"promo-list\">\n",
    "    <li data-promo=\"DISKON10\">Diskon 10% untuk pembelian pertama</li>\n",
    "    <li data-promo=\"GRATIS_ONGKIR\">Gratis ongkir untuk pembelian di atas Rp 200.000</li>\n",
    "  </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_contoh, \"lxml\")\n",
    "\n",
    "# Ekstrak informasi umum\n",
    "print(\"=== Informasi Halaman ===\")\n",
    "print(f\"Judul  : {soup.title.get_text()}\")\n",
    "print(f\"Update : {soup.find('p', class_='update').get_text()}\")\n",
    "print(f\"Total  : {soup.find('p', class_='total').get_text()}\")\n",
    "\n",
    "# Ekstrak tabel produk\n",
    "print(\"\\n=== Ekstraksi Tabel Produk ===\")\n",
    "tabel = soup.find(\"table\", id=\"tabel-produk\")\n",
    "header = [th.get_text(strip=True) for th in tabel.find_all(\"th\")]\n",
    "baris_data = []\n",
    "for tr in tabel.find(\"tbody\").find_all(\"tr\"):\n",
    "    nilai = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "    baris_data.append(dict(zip(header, nilai)))\n",
    "\n",
    "df_produk = pd.DataFrame(baris_data)\n",
    "df_produk[\"Harga (Rp)\"] = df_produk[\"Harga (Rp)\"].astype(int)\n",
    "df_produk[\"Stok\"] = df_produk[\"Stok\"].astype(int)\n",
    "df_produk[\"Rating\"] = df_produk[\"Rating\"].astype(float)\n",
    "print(df_produk.to_string(index=False))\n",
    "\n",
    "# Ekstrak daftar promo\n",
    "print(\"\\n=== Kode Promo ===\")\n",
    "for li in soup.find_all(\"li\", attrs={\"data-promo\": True}):\n",
    "    print(f\"  [{li['data-promo']}] {li.get_text(strip=True)}\")\n",
    "\n",
    "# Analisis sederhana\n",
    "print(\"\\n=== Analisis Produk ===\")\n",
    "print(f\"Rata-rata harga  : Rp {df_produk['Harga (Rp)'].mean():,.0f}\")\n",
    "print(f\"Produk termahal  : {df_produk.loc[df_produk['Harga (Rp)'].idxmax(), 'Nama Produk']}\")\n",
    "print(f\"Rating tertinggi : {df_produk.loc[df_produk['Rating'].idxmax(), 'Nama Produk']}\")"
   ],
   "id": "b1b2c3d4e5f60006"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mengakses REST API Publik\n",
    "\n",
    "Kita akan menggunakan **JSONPlaceholder** (https://jsonplaceholder.typicode.com) — API palsu gratis untuk testing dan prototyping. API ini menyediakan data posts, comments, users, todos, dll."
   ],
   "id": "b1b2c3d4e5f60007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "BASE_URL = \"https://jsonplaceholder.typicode.com\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# --- GET: Ambil semua posts ---\n",
    "print(\"=== GET /posts ===\")\n",
    "resp_posts = requests.get(f\"{BASE_URL}/posts\", headers=headers, timeout=10)\n",
    "print(f\"Status Code : {resp_posts.status_code}\")\n",
    "print(f\"Content-Type: {resp_posts.headers.get('content-type', 'N/A')}\")\n",
    "\n",
    "posts = resp_posts.json()\n",
    "df_posts = pd.DataFrame(posts)\n",
    "print(f\"Jumlah posts: {len(df_posts)}\")\n",
    "print(df_posts.head(3).to_string())\n",
    "\n",
    "# --- GET: Ambil satu post berdasarkan ID ---\n",
    "print(\"\\n=== GET /posts/1 ===\")\n",
    "resp_satu = requests.get(f\"{BASE_URL}/posts/1\", timeout=10)\n",
    "print(json.dumps(resp_satu.json(), indent=2))\n",
    "\n",
    "# --- GET: Ambil comments untuk post tertentu ---\n",
    "print(\"\\n=== GET /posts/1/comments ===\")\n",
    "resp_comments = requests.get(f\"{BASE_URL}/posts/1/comments\", timeout=10)\n",
    "df_comments = pd.DataFrame(resp_comments.json())\n",
    "print(f\"Jumlah komentar: {len(df_comments)}\")\n",
    "print(df_comments[[\"id\", \"name\", \"email\"]].to_string(index=False))\n",
    "\n",
    "# --- GET: Ambil data users ---\n",
    "print(\"\\n=== GET /users ===\")\n",
    "resp_users = requests.get(f\"{BASE_URL}/users\", timeout=10)\n",
    "df_users = pd.DataFrame(resp_users.json())\n",
    "print(f\"Jumlah users: {len(df_users)}\")\n",
    "print(df_users[[\"id\", \"name\", \"username\", \"email\"]].to_string(index=False))\n",
    "\n",
    "# --- Analisis: Jumlah post per user ---\n",
    "print(\"\\n=== Analisis: Jumlah Post per User ===\")\n",
    "posts_per_user = df_posts.groupby(\"userId\")[\"id\"].count().reset_index()\n",
    "posts_per_user.columns = [\"userId\", \"jumlah_post\"]\n",
    "print(posts_per_user.to_string(index=False))\n",
    "\n",
    "# --- Analisis: Panjang rata-rata judul post per user ---\n",
    "df_posts[\"panjang_judul\"] = df_posts[\"title\"].str.len()\n",
    "print(\"\\n=== Rata-rata Panjang Judul per User (5 user pertama) ===\")\n",
    "print(df_posts.groupby(\"userId\")[\"panjang_judul\"].mean().head().round(1))"
   ],
   "id": "b1b2c3d4e5f60008"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulasi Data Streaming\n",
    "\n",
    "Kita akan mensimulasikan data streaming menggunakan **generator function** Python yang menghasilkan rekaman data secara bertahap, seperti sensor IoT atau log server yang terus menghasilkan data."
   ],
   "id": "b1b2c3d4e5f60009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import time as time_module\n",
    "\n",
    "# --- Generator: Simulasi sensor IoT ---\n",
    "def generator_sensor_iot(n_records=20, device_count=5):\n",
    "    \"\"\"Generator yang mensimulasikan data dari beberapa sensor IoT.\"\"\"\n",
    "    random.seed(42)\n",
    "    waktu_mulai = datetime.now()\n",
    "    device_ids = [f\"SENSOR_{i:03d}\" for i in range(1, device_count + 1)]\n",
    "    lokasi_map = {\n",
    "        \"SENSOR_001\": \"Ruang Server\",\n",
    "        \"SENSOR_002\": \"Lab Komputer A\",\n",
    "        \"SENSOR_003\": \"Lab Komputer B\",\n",
    "        \"SENSOR_004\": \"Ruang Kuliah\",\n",
    "        \"SENSOR_005\": \"Koridor\",\n",
    "    }\n",
    "\n",
    "    for i in range(n_records):\n",
    "        device = random.choice(device_ids)\n",
    "        record = {\n",
    "            \"record_id\": i + 1,\n",
    "            \"timestamp\": (waktu_mulai + timedelta(seconds=i * 3)).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"device_id\": device,\n",
    "            \"lokasi\": lokasi_map[device],\n",
    "            \"suhu_celsius\": round(random.uniform(18.0, 35.0), 1),\n",
    "            \"kelembaban_persen\": round(random.uniform(40.0, 90.0), 1),\n",
    "            \"status\": random.choices([\"normal\", \"warning\", \"critical\"], weights=[0.7, 0.2, 0.1])[0]\n",
    "        }\n",
    "        yield record\n",
    "\n",
    "# --- Kumpulkan data dari generator ---\n",
    "print(\"Mengumpulkan data dari stream sensor IoT ...\")\n",
    "stream_buffer = []\n",
    "for i, record in enumerate(generator_sensor_iot(n_records=20)):\n",
    "    stream_buffer.append(record)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  [{record['timestamp']}] Diterima {i + 1} record dari stream\")\n",
    "\n",
    "df_stream = pd.DataFrame(stream_buffer)\n",
    "\n",
    "print(f\"\\nTotal record terkumpul: {len(df_stream)}\")\n",
    "print(\"\\n=== Sampel Data Streaming ===\")\n",
    "print(df_stream.to_string(index=False))\n",
    "\n",
    "# --- Analisis data streaming ---\n",
    "print(\"\\n=== Analisis Real-time ===\")\n",
    "print(\"\\nRata-rata suhu per lokasi:\")\n",
    "print(df_stream.groupby(\"lokasi\")[\"suhu_celsius\"].mean().round(2).to_string())\n",
    "\n",
    "print(\"\\nDistribusi status sensor:\")\n",
    "print(df_stream[\"status\"].value_counts().to_string())\n",
    "\n",
    "print(\"\\nRecord dengan status CRITICAL:\")\n",
    "df_critical = df_stream[df_stream[\"status\"] == \"critical\"]\n",
    "if len(df_critical) > 0:\n",
    "    print(df_critical[[\"timestamp\", \"device_id\", \"lokasi\", \"suhu_celsius\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"  Tidak ada record critical\")\n",
    "\n",
    "print(f\"\\nSuhu tertinggi  : {df_stream['suhu_celsius'].max()}°C ({df_stream.loc[df_stream['suhu_celsius'].idxmax(), 'lokasi']})\")\n",
    "print(f\"Kelembaban rata-rata: {df_stream['kelembaban_persen'].mean():.1f}%\")"
   ],
   "id": "b1b2c3d4e5f60010"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Menyimpan Data yang Dikumpulkan\n",
    "\n",
    "Setelah mengumpulkan data, langkah berikutnya adalah menyimpannya dalam format yang sesuai untuk analisis lebih lanjut. Format umum: **CSV** (tabular), **JSON** (semi-structured), **Parquet** (columnar, efisien)."
   ],
   "id": "b1b2c3d4e5f60011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"output_data\", exist_ok=True)\n",
    "\n",
    "# --- Simpan ke CSV ---\n",
    "path_csv = \"output_data/data_sensor.csv\"\n",
    "df_stream.to_csv(path_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"Disimpan ke CSV: {path_csv} ({os.path.getsize(path_csv)} bytes)\")\n",
    "\n",
    "# --- Simpan ke JSON ---\n",
    "path_json = \"output_data/data_sensor.json\"\n",
    "df_stream.to_json(path_json, orient=\"records\", indent=2, force_ascii=False)\n",
    "print(f\"Disimpan ke JSON: {path_json} ({os.path.getsize(path_json)} bytes)\")\n",
    "\n",
    "# --- Simpan kutipan ke CSV jika tersedia ---\n",
    "if 'df_kutipan' in dir() and len(df_kutipan) > 0:\n",
    "    path_kutipan = \"output_data/kutipan_scraped.csv\"\n",
    "    df_kutipan.to_csv(path_kutipan, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Disimpan ke CSV: {path_kutipan} ({os.path.getsize(path_kutipan)} bytes)\")\n",
    "\n",
    "# --- Baca ulang dan verifikasi CSV ---\n",
    "print(\"\\n=== Verifikasi — Baca Ulang CSV ===\")\n",
    "df_verif_csv = pd.read_csv(path_csv)\n",
    "print(f\"Shape: {df_verif_csv.shape}\")\n",
    "print(f\"Kolom: {list(df_verif_csv.columns)}\")\n",
    "print(df_verif_csv.head(3).to_string(index=False))\n",
    "\n",
    "# --- Baca ulang dan verifikasi JSON ---\n",
    "print(\"\\n=== Verifikasi — Baca Ulang JSON ===\")\n",
    "df_verif_json = pd.read_json(path_json)\n",
    "print(f\"Shape: {df_verif_json.shape}\")\n",
    "print(f\"Kolom: {list(df_verif_json.columns)}\")\n",
    "print(df_verif_json.head(3).to_string(index=False))\n",
    "\n",
    "# --- Perbandingan integritas data ---\n",
    "print(\"\\n=== Integritas Data ===\")\n",
    "assert len(df_verif_csv) == len(df_stream), \"Jumlah baris CSV tidak sesuai!\"\n",
    "assert len(df_verif_json) == len(df_stream), \"Jumlah baris JSON tidak sesuai!\"\n",
    "print(f\"CSV  : {len(df_verif_csv)} baris — OK\")\n",
    "print(f\"JSON : {len(df_verif_json)} baris — OK\")\n",
    "print(\"Semua data tersimpan dengan integritas penuh!\")"
   ],
   "id": "b1b2c3d4e5f60012"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Kerjakan soal-soal berikut secara mandiri:\n",
    "\n",
    "**Soal 1 — Web Scraping Multi-halaman:**  \n",
    "Lakukan scraping dari `http://quotes.toscrape.com` untuk **semua halaman** yang tersedia (hingga tidak ada lagi tombol \"Next\"). Analisis: siapa penulis dengan kutipan terbanyak? Tag apa yang paling sering muncul?\n",
    "\n",
    "**Soal 2 — REST API Lanjutan:**  \n",
    "Gunakan JSONPlaceholder API untuk:\n",
    "- Ambil data `/todos` dan hitung persentase tugas yang sudah selesai (`completed=True`) per user\n",
    "- Buat visualisasi bar chart dari hasil tersebut menggunakan matplotlib\n",
    "\n",
    "**Soal 3 — Scraping dengan BeautifulSoup:**  \n",
    "Buat HTML string baru berisi tabel jadwal kuliah dengan kolom: `hari`, `jam`, `mata_kuliah`, `dosen`, `ruangan`. Isi dengan minimal 10 baris. Ekstrak datanya menggunakan BeautifulSoup dan simpan ke CSV.\n",
    "\n",
    "**Soal 4 — Simulasi Streaming Lanjutan:**  \n",
    "Modifikasi generator sensor IoT untuk:\n",
    "- Menghasilkan 100 rekaman\n",
    "- Mendeteksi secara real-time jika ada suhu > 30°C dan kelembaban > 80% secara bersamaan (kondisi kritis)\n",
    "- Hitung rata-rata suhu bergerak (rolling average) setiap 10 rekaman"
   ],
   "id": "b1b2c3d4e5f60013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ringkasan hasil praktikum\n",
    "print(\"=\" * 55)\n",
    "print(\"   RINGKASAN PRAKTIKUM MINGGU 4 — DATA COLLECTION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "hasil_summary = {\n",
    "    \"Web Scraping\": f\"{len(df_kutipan) if 'df_kutipan' in dir() else 0} kutipan berhasil di-scrape\",\n",
    "    \"Parsing HTML Lokal\": f\"{len(df_produk)} produk berhasil diekstrak\",\n",
    "    \"REST API (posts)\": f\"{len(df_posts)} posts diambil dari JSONPlaceholder\",\n",
    "    \"REST API (users)\": f\"{len(df_users)} users diambil dari JSONPlaceholder\",\n",
    "    \"Data Streaming\": f\"{len(df_stream)} rekaman sensor IoT dikumpulkan\",\n",
    "    \"File CSV\": f\"Disimpan di {path_csv}\",\n",
    "    \"File JSON\": f\"Disimpan di {path_json}\",\n",
    "}\n",
    "\n",
    "for topik, hasil in hasil_summary.items():\n",
    "    print(f\"  ✓ {topik:<25}: {hasil}\")\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"Praktikum Minggu 4 selesai!\")"
   ],
   "id": "b1b2c3d4e5f60014"
  }
 ]
}

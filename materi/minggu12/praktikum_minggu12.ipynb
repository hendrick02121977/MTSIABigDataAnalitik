{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 12: Stream Processing & Real-time Analytics\n",
    "## *Week 12 Lab: Stream Processing & Real-time Analytics*\n",
    "\n",
    "**Mata Kuliah / Course:** Big Data Analytics  \n",
    "**Topik / Topic:** Stream Processing, Apache Kafka Simulation, Spark Structured Streaming  \n",
    "\n",
    "---\n",
    "### Deskripsi\n",
    "Pada praktikum ini kita akan mensimulasikan stream processing menggunakan Python, termasuk:\n",
    "- Generator data stream IoT\n",
    "- Windowed aggregations\n",
    "- Real-time anomaly detection\n",
    "- Simulasi Kafka Producer-Consumer dengan Queue\n",
    "- Konsep Spark Structured Streaming"
   ],
   "id": "cell-md-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install pyspark kafka-python --quiet\n",
    "\n",
    "# Core imports\n",
    "import time\n",
    "import random\n",
    "import queue\n",
    "import threading\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "\n",
    "print('Libraries loaded successfully!')"
   ],
   "id": "cell-imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulasi Data Stream dengan Generator Python\n",
    "\n",
    "Kita akan membuat generator yang mensimulasikan sensor IoT yang mengirimkan data setiap 0.1 detik.\n",
    "Setiap record berisi: timestamp, sensor_id, temperature, humidity."
   ],
   "id": "cell-md-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iot_sensor_stream(num_sensors=5, delay=0.1):\n",
    "    \"\"\"Generator that simulates IoT sensor data stream.\"\"\"\n",
    "    sensor_ids = [f'SENSOR_{i:03d}' for i in range(1, num_sensors + 1)]\n",
    "    base_temps = {s: random.uniform(20, 35) for s in sensor_ids}\n",
    "    base_humidity = {s: random.uniform(40, 70) for s in sensor_ids}\n",
    "\n",
    "    while True:\n",
    "        for sensor_id in sensor_ids:\n",
    "            # Simulate slight drift and noise\n",
    "            temperature = base_temps[sensor_id] + random.gauss(0, 1.5)\n",
    "            humidity = base_humidity[sensor_id] + random.gauss(0, 3)\n",
    "            # Occasionally inject anomaly\n",
    "            if random.random() < 0.03:\n",
    "                temperature += random.choice([-15, 20])  # spike/drop\n",
    "\n",
    "            record = {\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f'),\n",
    "                'sensor_id': sensor_id,\n",
    "                'temperature': round(temperature, 2),\n",
    "                'humidity': round(max(0, min(100, humidity)), 2)\n",
    "            }\n",
    "            yield record\n",
    "        time.sleep(delay)\n",
    "\n",
    "# Collect 100 records from the stream\n",
    "print('Collecting 100 records from IoT sensor stream...')\n",
    "stream = iot_sensor_stream(num_sensors=5, delay=0.01)  # fast for demo\n",
    "records = []\n",
    "for i, record in enumerate(stream):\n",
    "    records.append(record)\n",
    "    if i >= 99:\n",
    "        break\n",
    "\n",
    "df_stream = pd.DataFrame(records)\n",
    "df_stream['timestamp'] = pd.to_datetime(df_stream['timestamp'])\n",
    "print(f'Collected {len(df_stream)} records')\n",
    "print(df_stream.head(10))"
   ],
   "id": "cell-iot-stream"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Windowed Aggregations pada Stream Data\n",
    "\n",
    "Windowing membagi stream menjadi segmen waktu untuk komputasi agregasi (rata-rata, max, min).\n",
    "Di sini kita menggunakan rolling window untuk menghitung statistik bergerak."
   ],
   "id": "cell-md-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign synthetic timestamps spaced 0.1s apart for windowing demo\n",
    "base_time = datetime(2024, 1, 15, 9, 0, 0)\n",
    "df_stream['event_time'] = [base_time + timedelta(seconds=i * 0.1) for i in range(len(df_stream))]\n",
    "\n",
    "# --- Tumbling window (10-second bins) ---\n",
    "df_stream['window_10s'] = df_stream['event_time'].dt.floor('10s')\n",
    "tumbling_agg = df_stream.groupby(['window_10s', 'sensor_id']).agg(\n",
    "    avg_temp=('temperature', 'mean'),\n",
    "    max_temp=('temperature', 'max'),\n",
    "    min_temp=('temperature', 'min'),\n",
    "    count=('temperature', 'count')\n",
    ").round(2).reset_index()\n",
    "\n",
    "print('=== Tumbling Window Aggregations (10-second bins) ===')\n",
    "print(tumbling_agg.head(15))\n",
    "\n",
    "# --- Rolling window (last 10 records per sensor) ---\n",
    "df_stream_sorted = df_stream.sort_values(['sensor_id', 'event_time'])\n",
    "df_stream_sorted['rolling_avg_temp'] = (\n",
    "    df_stream_sorted.groupby('sensor_id')['temperature']\n",
    "    .transform(lambda x: x.rolling(window=10, min_periods=1).mean())\n",
    ").round(2)\n",
    "\n",
    "print('\\n=== Rolling Average Temperature (window=10) ===')\n",
    "print(df_stream_sorted[['event_time', 'sensor_id', 'temperature', 'rolling_avg_temp']].head(20))\n",
    "\n",
    "# Visualize rolling average for one sensor\n",
    "sensor_data = df_stream_sorted[df_stream_sorted['sensor_id'] == 'SENSOR_001']\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(sensor_data['event_time'], sensor_data['temperature'], alpha=0.5, label='Raw Temperature')\n",
    "plt.plot(sensor_data['event_time'], sensor_data['rolling_avg_temp'], linewidth=2, label='Rolling Avg (10 records)')\n",
    "plt.xlabel('Event Time')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('SENSOR_001: Raw vs Rolling Average Temperature')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-windowed-agg"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deteksi Anomali Real-time\n",
    "\n",
    "Anomaly detection berbasis threshold sederhana: jika temperature menyimpang lebih dari\n",
    "3 standar deviasi dari rata-rata rolling, record tersebut dianggap anomali."
   ],
   "id": "cell-md-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_streaming(df, sensor_col='sensor_id', value_col='temperature',\n",
    "                                window=20, threshold_sigma=2.5):\n",
    "    \"\"\"Detect anomalies using rolling Z-score (threshold-based).\"\"\"\n",
    "    df = df.sort_values([sensor_col, 'event_time']).copy()\n",
    "\n",
    "    grp = df.groupby(sensor_col)[value_col]\n",
    "    df['rolling_mean'] = grp.transform(lambda x: x.rolling(window, min_periods=3).mean())\n",
    "    df['rolling_std']  = grp.transform(lambda x: x.rolling(window, min_periods=3).std())\n",
    "    df['z_score']      = ((df[value_col] - df['rolling_mean']) / df['rolling_std'].replace(0, 1)).abs()\n",
    "    df['is_anomaly']   = df['z_score'] > threshold_sigma\n",
    "    return df\n",
    "\n",
    "df_anomaly = detect_anomalies_streaming(df_stream_sorted)\n",
    "\n",
    "anomalies = df_anomaly[df_anomaly['is_anomaly']]\n",
    "print(f'Total records: {len(df_anomaly)}')\n",
    "print(f'Anomalies detected: {len(anomalies)} ({len(anomalies)/len(df_anomaly)*100:.1f}%)')\n",
    "print('\\nAnomaly records:')\n",
    "print(anomalies[['event_time', 'sensor_id', 'temperature', 'rolling_mean', 'z_score']].to_string())\n",
    "\n",
    "# Visualize anomalies\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "for sensor_id, grp in df_anomaly.groupby('sensor_id'):\n",
    "    ax.plot(grp['event_time'], grp['temperature'], alpha=0.4, linewidth=0.8)\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    ax.scatter(anomalies['event_time'], anomalies['temperature'],\n",
    "               color='red', zorder=5, s=80, label=f'Anomaly ({len(anomalies)})', marker='X')\n",
    "\n",
    "ax.set_title('Real-time Anomaly Detection — All Sensors')\n",
    "ax.set_xlabel('Event Time')\n",
    "ax.set_ylabel('Temperature (°C)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-anomaly-detection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulasi Kafka Producer-Consumer dengan Queue\n",
    "\n",
    "Kita mensimulasikan pola Kafka Producer-Consumer menggunakan `queue.Queue` Python.\n",
    "Producer mempublikasikan pesan ke topic; Consumer membaca dan memproses pesan."
   ],
   "id": "cell-md-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import threading\n",
    "\n",
    "# Kafka-like Topic implementation\n",
    "class KafkaTopic:\n",
    "    def __init__(self, name, num_partitions=3):\n",
    "        self.name = name\n",
    "        self.partitions = [queue.Queue() for _ in range(num_partitions)]\n",
    "        self.num_partitions = num_partitions\n",
    "\n",
    "    def publish(self, message, key=None):\n",
    "        partition_idx = hash(key or '') % self.num_partitions\n",
    "        self.partitions[partition_idx].put({'key': key, 'value': message,\n",
    "                                             'timestamp': datetime.now().isoformat()})\n",
    "\n",
    "    def consume(self, partition_idx=0, timeout=0.05):\n",
    "        try:\n",
    "            return self.partitions[partition_idx].get(timeout=timeout)\n",
    "        except queue.Empty:\n",
    "            return None\n",
    "\n",
    "# Producer function\n",
    "produced_messages = []\n",
    "consumed_messages = []\n",
    "\n",
    "def producer(topic, n_messages=30):\n",
    "    sensor_gen = iot_sensor_stream(num_sensors=3, delay=0)\n",
    "    for i in range(n_messages):\n",
    "        record = next(sensor_gen)\n",
    "        topic.publish(record, key=record['sensor_id'])\n",
    "        produced_messages.append(record)\n",
    "    print(f'[Producer] Published {n_messages} messages to topic \"{topic.name}\"')\n",
    "\n",
    "# Consumer function\n",
    "def consumer(topic, consumer_id, partitions):\n",
    "    count = 0\n",
    "    for _ in range(100):  # poll iterations\n",
    "        for p in partitions:\n",
    "            msg = topic.consume(p)\n",
    "            if msg:\n",
    "                consumed_messages.append({'consumer': consumer_id, 'message': msg})\n",
    "                count += 1\n",
    "    print(f'[Consumer {consumer_id}] Consumed {count} messages from partitions {partitions}')\n",
    "\n",
    "# Create topic and simulate pub/sub\n",
    "sensor_topic = KafkaTopic('sensor-data', num_partitions=3)\n",
    "\n",
    "t_producer = threading.Thread(target=producer, args=(sensor_topic, 30))\n",
    "t_consumer1 = threading.Thread(target=consumer, args=(sensor_topic, 'C1', [0, 1]))\n",
    "t_consumer2 = threading.Thread(target=consumer, args=(sensor_topic, 'C2', [2]))\n",
    "\n",
    "t_producer.start(); t_consumer1.start(); t_consumer2.start()\n",
    "t_producer.join(); t_consumer1.join(); t_consumer2.join()\n",
    "\n",
    "print(f'\\nTotal produced: {len(produced_messages)}')\n",
    "print(f'Total consumed: {len(consumed_messages)}')\n",
    "c1_count = sum(1 for m in consumed_messages if m['consumer'] == 'C1')\n",
    "c2_count = sum(1 for m in consumed_messages if m['consumer'] == 'C2')\n",
    "print(f'Consumer C1 consumed: {c1_count} | Consumer C2 consumed: {c2_count}')\n",
    "print('\\nSample consumed messages:')\n",
    "for m in consumed_messages[:3]:\n",
    "    print(f\"  {m['consumer']}: {m['message']}\")"
   ],
   "id": "cell-kafka-simulation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spark Structured Streaming (Simulasi dengan Micro-batch)\n",
    "\n",
    "Spark Structured Streaming memperlakukan data streaming sebagai tabel yang terus berkembang.\n",
    "Di sini kita mensimulasikan konsep micro-batch processing menggunakan PySpark."
   ],
   "id": "cell-md-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('StreamProcessingDemo') \\\n",
    "    .master('local[2]') \\\n",
    "    .config('spark.sql.shuffle.partitions', '4') \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "print(f'Spark version: {spark.version}')\n",
    "\n",
    "# Simulate micro-batch: process stream records in batches\n",
    "BATCH_SIZE = 20\n",
    "print('\\n=== Micro-batch Streaming Simulation ===')\n",
    "for batch_id in range(3):\n",
    "    # Each micro-batch = subset of stream records\n",
    "    batch_records = records[batch_id * BATCH_SIZE:(batch_id + 1) * BATCH_SIZE]\n",
    "    schema = StructType([\n",
    "        StructField('timestamp', StringType(), True),\n",
    "        StructField('sensor_id', StringType(), True),\n",
    "        StructField('temperature', DoubleType(), True),\n",
    "        StructField('humidity', DoubleType(), True)\n",
    "    ])\n",
    "    batch_df = spark.createDataFrame(batch_records, schema=schema)\n",
    "    result = batch_df.groupBy('sensor_id').agg(\n",
    "        F.round(F.avg('temperature'), 2).alias('avg_temp'),\n",
    "        F.round(F.max('temperature'), 2).alias('max_temp'),\n",
    "        F.count('*').alias('count')\n",
    "    ).orderBy('sensor_id')\n",
    "    print(f'\\n--- Batch {batch_id} ({len(batch_records)} records) ---')\n",
    "    result.show()\n",
    "\n",
    "# Demonstrate readStream API concept (rate source)\n",
    "print('=== Structured Streaming API Concept (Rate Source) ===')\n",
    "print(\"\"\"\n",
    "# In a real Spark Structured Streaming job:\n",
    "\n",
    "df = spark.readStream \\\\\n",
    "    .format(\"kafka\") \\\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\\\n",
    "    .option(\"subscribe\", \"sensor-data\") \\\\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON and aggregate\n",
    "result = df \\\\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\\\n",
    "    .groupBy(F.window(\"event_time\", \"5 minutes\"), \"sensor_id\") \\\\\n",
    "    .agg(F.avg(\"temperature\").alias(\"avg_temp\"))\n",
    "\n",
    "# Write to console\n",
    "query = result.writeStream \\\\\n",
    "    .outputMode(\"update\") \\\\\n",
    "    .format(\"console\") \\\\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\\\n",
    "    .start()\n",
    "query.awaitTermination()\n",
    "\"\"\")\n",
    "spark.stop()"
   ],
   "id": "cell-spark-streaming"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analitik Dashboard Real-time (Simulasi)\n",
    "\n",
    "Mensimulasikan dashboard real-time dengan snapshot metrics dari streaming data."
   ],
   "id": "cell-md-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate streaming dashboard snapshots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('Real-time Sensor Dashboard (Snapshot)', fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 5))\n",
    "\n",
    "# 1. Temperature time series per sensor\n",
    "ax = axes[0, 0]\n",
    "for i, (sensor_id, grp) in enumerate(df_anomaly.groupby('sensor_id')):\n",
    "    ax.plot(range(len(grp)), grp['temperature'], label=sensor_id, color=colors[i], alpha=0.8)\n",
    "ax.set_title('Temperature Over Time'); ax.set_xlabel('Record #'); ax.set_ylabel('Temp (°C)')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# 2. Average temp per sensor (bar chart)\n",
    "ax = axes[0, 1]\n",
    "avg_temps = df_anomaly.groupby('sensor_id')['temperature'].mean()\n",
    "ax.bar(avg_temps.index, avg_temps.values, color=colors)\n",
    "ax.set_title('Average Temperature per Sensor'); ax.set_ylabel('Avg Temp (°C)')\n",
    "ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "# 3. Humidity distribution\n",
    "ax = axes[0, 2]\n",
    "df_anomaly['humidity'].hist(bins=20, ax=ax, color='steelblue', edgecolor='white')\n",
    "ax.set_title('Humidity Distribution'); ax.set_xlabel('Humidity (%)')\n",
    "\n",
    "# 4. Anomaly count per sensor\n",
    "ax = axes[1, 0]\n",
    "anomaly_counts = df_anomaly.groupby('sensor_id')['is_anomaly'].sum()\n",
    "ax.bar(anomaly_counts.index, anomaly_counts.values, color='tomato')\n",
    "ax.set_title('Anomaly Count per Sensor'); ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "# 5. Scatter: temperature vs humidity\n",
    "ax = axes[1, 1]\n",
    "normal = df_anomaly[~df_anomaly['is_anomaly']]\n",
    "anom   = df_anomaly[df_anomaly['is_anomaly']]\n",
    "ax.scatter(normal['temperature'], normal['humidity'], s=15, alpha=0.5, label='Normal')\n",
    "if len(anom) > 0:\n",
    "    ax.scatter(anom['temperature'], anom['humidity'], s=60, color='red', marker='X', label='Anomaly')\n",
    "ax.set_title('Temperature vs Humidity'); ax.set_xlabel('Temp'); ax.set_ylabel('Humidity')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# 6. Records per window\n",
    "ax = axes[1, 2]\n",
    "window_counts = df_anomaly.groupby('window_10s').size()\n",
    "ax.bar(range(len(window_counts)), window_counts.values, color='mediumseagreen')\n",
    "ax.set_title('Records per 10s Window'); ax.set_xlabel('Window #'); ax.set_ylabel('Record Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('streaming_dashboard.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Dashboard saved as streaming_dashboard.png')"
   ],
   "id": "cell-dashboard"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Selesaikan tugas-tugas berikut dan kumpulkan notebook ini:\n",
    "\n",
    "1. **Tugas 1 — Multi-sensor Windowing**: Modifikasi kode windowed aggregation untuk menggunakan\n",
    "   *sliding window* (ukuran 15 record, step 5 record) dan hitung persentil ke-95 temperature\n",
    "   per sensor per window.\n",
    "\n",
    "2. **Tugas 2 — Adaptive Anomaly Detection**: Implementasikan anomaly detection yang\n",
    "   menggunakan **IQR (Interquartile Range)** sebagai pengganti Z-score. Bandingkan\n",
    "   jumlah anomali yang terdeteksi oleh kedua metode.\n",
    "\n",
    "3. **Tugas 3 — Multi-topic Kafka Simulation**: Perluas simulasi Kafka untuk mensimulasikan\n",
    "   dua topic berbeda (`sensor-temperature` dan `sensor-humidity`). Tambahkan satu Consumer Group\n",
    "   yang mengonsumsi dari kedua topic.\n",
    "\n",
    "4. **Tugas 4 — Real-time Alert System**: Buat fungsi `alert_system(record)` yang memeriksa\n",
    "   setiap record IoT dan mengirimkan alert (print ke konsol) jika:\n",
    "   - Temperature > 40°C atau < 0°C\n",
    "   - Humidity < 20% atau > 90%\n",
    "   - Rate of change temperature antar record > 10°C dalam satu langkah\n",
    "   \n",
    "   Integrasikan dengan generator stream dan tampilkan ringkasan alert setelah 200 records."
   ],
   "id": "cell-md-tugas"
  }
 ]
}

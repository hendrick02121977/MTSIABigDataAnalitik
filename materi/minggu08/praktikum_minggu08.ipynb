{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 8: Latihan UTS - Review Komprehensif\n",
    "## *Week 8 Lab: Midterm Exam Practice - Comprehensive Review*\n",
    "\n",
    "**Mata Kuliah:** Big Data Analytics  \n",
    "**Topik:** Review Minggu 1-7 | Apache Spark, Web Scraping, Preprocessing, EDA, Analisis Terintegrasi\n",
    "\n",
    "---\n",
    "### Petunjuk Pengerjaan\n",
    "- Kerjakan setiap soal secara berurutan\n",
    "- Baca komentar dalam kode untuk petunjuk tambahan\n",
    "- Setiap soal memiliki bobot nilai yang tertera\n",
    "- Waktu pengerjaan: 90 menit\n",
    "\n",
    "| Soal | Topik | Bobot |\n",
    "|------|-------|-------|\n",
    "| 1 | Apache Spark RDD Operations | 20% |\n",
    "| 2 | Web Scraping & API | 15% |\n",
    "| 3 | Data Preprocessing | 25% |\n",
    "| 4 | EDA & Visualisasi | 25% |\n",
    "| 5 | Analisis Data Terintegrasi | 15% |"
   ],
   "id": "cell-md-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import semua library yang diperlukan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "print('Semua library berhasil diimpor')\n",
    "print(f'  pandas  : {pd.__version__}')\n",
    "print(f'  numpy   : {np.__version__}')\n",
    "print(f'  sklearn : sesuai instalasi')"
   ],
   "id": "cell-code-imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soal 1: Apache Spark RDD Operations\n",
    "**Bobot: 20 poin**\n",
    "\n",
    "Demonstrasikan pemahaman Anda tentang Apache Spark dengan mengerjakan operasi RDD berikut.\n",
    "\n",
    "**Konsep yang diuji:**\n",
    "- Transformations vs Actions\n",
    "- Lazy evaluation\n",
    "- Operasi map, filter, reduceByKey, groupByKey\n",
    "- WordCount problem (klasik)"
   ],
   "id": "cell-md-soal1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark\n",
    "!pip install pyspark --quiet\n",
    "print('PySpark berhasil diinstall')"
   ],
   "id": "cell-code-pyspark-install"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Inisialisasi SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('UTS_BigData_Review') \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f'Spark Version: {spark.version}')\n",
    "print(f'Spark Master : {sc.master}')\n",
    "\n",
    "# ============================================================\n",
    "# Latihan 1a: Operasi Dasar RDD\n",
    "# ============================================================\n",
    "print('\\n=== 1a. Operasi Dasar RDD ===')\n",
    "\n",
    "# Buat RDD dari list\n",
    "data = [3, 7, 2, 9, 5, 1, 8, 4, 6, 10, 15, 12, 3, 7, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "print(f'Data asal  : {data}')\n",
    "print(f'Jumlah     : {rdd.count()}')\n",
    "print(f'Jumlah elemen: {rdd.sum()}')\n",
    "print(f'Rata-rata  : {rdd.mean():.2f}')\n",
    "print(f'Min / Max  : {rdd.min()} / {rdd.max()}')\n",
    "\n",
    "# Transformations\n",
    "rdd_genap = rdd.filter(lambda x: x % 2 == 0)\n",
    "rdd_kuadrat = rdd.map(lambda x: x ** 2)\n",
    "rdd_besar = rdd.filter(lambda x: x > 7)\n",
    "\n",
    "# Actions\n",
    "print(f'\\nBilangan genap : {sorted(rdd_genap.collect())}')\n",
    "print(f'Dikuadratkan   : {sorted(rdd_kuadrat.collect())}')\n",
    "print(f'Lebih dari 7   : {sorted(rdd_besar.collect())}')\n",
    "print(f'Jumlah kuadrat : {rdd_kuadrat.sum()}')\n",
    "\n",
    "# ============================================================\n",
    "# Latihan 1b: WordCount (Klasik MapReduce)\n",
    "# ============================================================\n",
    "print('\\n=== 1b. WordCount Problem ===')\n",
    "\n",
    "teks = [\n",
    "    'big data adalah era baru teknologi informasi',\n",
    "    'spark memproses big data dengan cepat',\n",
    "    'hadoop adalah fondasi ekosistem big data',\n",
    "    'data science menggunakan big data untuk analisis',\n",
    "    'spark lebih cepat dari hadoop untuk iterasi',\n",
    "]\n",
    "\n",
    "rdd_text = sc.parallelize(teks)\n",
    "\n",
    "word_count = (\n",
    "    rdd_text\n",
    "    .flatMap(lambda line: line.split(' '))   # Pecah tiap baris menjadi kata\n",
    "    .map(lambda word: (word, 1))             # Buat pasangan (kata, 1)\n",
    "    .reduceByKey(lambda a, b: a + b)         # Jumlahkan per kata\n",
    "    .sortBy(lambda x: x[1], ascending=False) # Urutkan descending\n",
    ")\n",
    "\n",
    "print('Top 10 kata yang paling sering muncul:')\n",
    "for word, count in word_count.take(10):\n",
    "    bar = '#' * count\n",
    "    print(f'  {word:15s}: {count:2d} {bar}')\n",
    "\n",
    "# ============================================================\n",
    "# Latihan 1c: RDD dengan Pasangan Key-Value\n",
    "# ============================================================\n",
    "print('\\n=== 1c. RDD Key-Value: Analisis Nilai Mahasiswa ===')\n",
    "\n",
    "nilai_data = [\n",
    "    ('Informatika', 85), ('Informatika', 92), ('Informatika', 78),\n",
    "    ('Sistem Informasi', 88), ('Sistem Informasi', 75), ('Sistem Informasi', 91),\n",
    "    ('Data Science', 95), ('Data Science', 87), ('Data Science', 93),\n",
    "    ('Informatika', 80), ('Sistem Informasi', 82), ('Data Science', 89),\n",
    "]\n",
    "\n",
    "rdd_nilai = sc.parallelize(nilai_data)\n",
    "\n",
    "# Rata-rata per prodi\n",
    "avg_per_prodi = (\n",
    "    rdd_nilai\n",
    "    .groupByKey()\n",
    "    .map(lambda x: (x[0], sum(x[1]) / len(list(x[1]))))\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "print('Rata-rata nilai per prodi:')\n",
    "for prodi, avg in avg_per_prodi.collect():\n",
    "    print(f'  {prodi:20s}: {avg:.2f}')\n",
    "\n",
    "# Mahasiswa dengan nilai di atas 90\n",
    "nilai_tinggi = rdd_nilai.filter(lambda x: x[1] >= 90)\n",
    "print(f'\\nJumlah nilai >= 90 per prodi:')\n",
    "count_tinggi = nilai_tinggi.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "for prodi, count in count_tinggi.collect():\n",
    "    print(f'  {prodi:20s}: {count}')\n",
    "\n",
    "spark.stop()\n",
    "print('\\nSparkSession dihentikan')"
   ],
   "id": "cell-code-soal1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soal 2: Web Scraping & API\n",
    "**Bobot: 15 poin**\n",
    "\n",
    "Demonstrasikan kemampuan pengumpulan data melalui REST API dan manipulasi data JSON.\n",
    "\n",
    "**Konsep yang diuji:**\n",
    "- HTTP GET request\n",
    "- Parsing JSON response\n",
    "- Transformasi data ke DataFrame\n",
    "- Analisis data yang dikumpulkan"
   ],
   "id": "cell-md-soal2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# Latihan 2a: Mengambil Data dari JSONPlaceholder API\n",
    "# ============================================================\n",
    "print('=== 2a. Pengambilan Data via REST API ===')\n",
    "BASE_URL = 'https://jsonplaceholder.typicode.com'\n",
    "\n",
    "# Fungsi helper untuk GET request\n",
    "def api_get(endpoint, params=None):\n",
    "    \"\"\"Wrapper untuk GET request dengan error handling.\"\"\"\n",
    "    try:\n",
    "        url = f'{BASE_URL}{endpoint}'\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()  # raise exception untuk HTTP error\n",
    "        return response.json()\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f'  Tidak bisa terhubung ke API - menggunakan data simulasi')\n",
    "        return None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f'  Request timeout')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'  Error: {e}')\n",
    "        return None\n",
    "\n",
    "# Ambil daftar users\n",
    "print('\\n[1] Mengambil data Users...')\n",
    "users_raw = api_get('/users')\n",
    "\n",
    "if users_raw is None:\n",
    "    # Fallback: gunakan data simulasi\n",
    "    print('Menggunakan data simulasi...')\n",
    "    users_raw = [\n",
    "        {'id': i, 'name': f'User {i}', 'username': f'user{i}',\n",
    "         'email': f'user{i}@test.com',\n",
    "         'address': {'city': ['Jakarta', 'Bandung', 'Surabaya', 'Medan'][i % 4]},\n",
    "         'company': {'name': f'Company {chr(65 + i % 5)}'}}\n",
    "        for i in range(1, 11)\n",
    "    ]\n",
    "\n",
    "# Parse ke DataFrame\n",
    "users_df = pd.json_normalize(\n",
    "    users_raw,\n",
    "    sep='_'\n",
    ")[['id', 'name', 'username', 'email', 'address_city', 'company_name']]\n",
    "users_df.columns = ['id', 'nama', 'username', 'email', 'kota', 'perusahaan']\n",
    "\n",
    "print(f'Berhasil mengambil {len(users_df)} user')\n",
    "print(users_df.to_string(index=False))\n",
    "\n",
    "# Ambil posts\n",
    "print('\\n[2] Mengambil data Posts...')\n",
    "posts_raw = api_get('/posts')\n",
    "\n",
    "if posts_raw is None:\n",
    "    posts_raw = [\n",
    "        {'userId': (i % 10) + 1, 'id': i, 'title': f'Post judul {i}',\n",
    "         'body': ' '.join([f'kata{j}' for j in range(np.random.randint(5, 15))])}\n",
    "        for i in range(1, 101)\n",
    "    ]\n",
    "\n",
    "posts_df = pd.DataFrame(posts_raw)\n",
    "posts_df['word_count'] = posts_df['body'].str.split().str.len()\n",
    "print(f'Berhasil mengambil {len(posts_df)} post')\n",
    "\n",
    "# ============================================================\n",
    "# Latihan 2b: Analisis Data API\n",
    "# ============================================================\n",
    "print('\\n=== 2b. Analisis Data dari API ===')\n",
    "\n",
    "# Merge users dan posts\n",
    "posts_df = posts_df.rename(columns={'userId': 'id'})\n",
    "merged = posts_df.merge(users_df[['id', 'nama', 'kota']], on='id', how='left')\n",
    "\n",
    "# Statistik\n",
    "print('\\n[a] Jumlah post per user:')\n",
    "post_count = merged.groupby('nama')['id'].count().reset_index()\n",
    "post_count.columns = ['nama', 'jumlah_post']\n",
    "print(post_count.sort_values('jumlah_post', ascending=False).head(5).to_string(index=False))\n",
    "\n",
    "print('\\n[b] Rata-rata panjang post per user (top 5):')\n",
    "avg_words = merged.groupby('nama')['word_count'].mean().reset_index()\n",
    "avg_words.columns = ['nama', 'avg_word_count']\n",
    "print(avg_words.sort_values('avg_word_count', ascending=False).head(5).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# Latihan 2c: Error Handling dan Rate Limiting\n",
    "# ============================================================\n",
    "print('\\n=== 2c. Error Handling & Rate Limiting ===')\n",
    "\n",
    "def fetch_with_retry(endpoint, max_retries=3, delay=1):\n",
    "    \"\"\"Fetch dengan retry logic dan exponential backoff.\"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            result = api_get(endpoint)\n",
    "            if result is not None:\n",
    "                return result\n",
    "            print(f'  Percobaan {attempt}/{max_retries} gagal, tunggu {delay}s...')\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # exponential backoff\n",
    "        except Exception as e:\n",
    "            print(f'  Error pada percobaan {attempt}: {e}')\n",
    "    return None\n",
    "\n",
    "# Ambil beberapa todo secara selektif\n",
    "todos_raw = api_get('/todos?userId=1&_limit=10')\n",
    "if todos_raw is None:\n",
    "    todos_raw = [{'userId': 1, 'id': i, 'title': f'Task {i}',\n",
    "                  'completed': bool(i % 3 == 0)} for i in range(1, 11)]\n",
    "\n",
    "todos_df = pd.DataFrame(todos_raw)\n",
    "print(f'Todos user 1: {len(todos_df)} tasks')\n",
    "completed = todos_df['completed'].sum()\n",
    "print(f'  Selesai    : {completed} ({completed/len(todos_df)*100:.0f}%)')\n",
    "print(f'  Belum      : {len(todos_df) - completed}')\n",
    "print('\\nSelesai!')"
   ],
   "id": "cell-code-soal2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soal 3: Data Preprocessing\n",
    "**Bobot: 25 poin**\n",
    "\n",
    "Diberikan dataset kotor (dirty dataset) dengan berbagai masalah kualitas data. Lakukan preprocessing lengkap.\n",
    "\n",
    "**Masalah dalam dataset:**\n",
    "1. Missing values di beberapa kolom\n",
    "2. Format tidak konsisten (tanggal, huruf besar/kecil)\n",
    "3. Outlier ekstrem\n",
    "4. Duplikasi data\n",
    "5. Tipe data yang salah\n",
    "6. Variabel kategoris perlu di-encoding"
   ],
   "id": "cell-md-soal3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# Dataset Kotor - Simulasi data karyawan\n",
    "# ============================================================\n",
    "np.random.seed(42)\n",
    "\n",
    "dirty_data = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 11, 12, 13, 14, 15],   # ID 3 duplikat\n",
    "    'Nama': ['Budi', 'sari DEWI', 'Andi', None, 'maya', 'RIZKY', 'Dewi', 'Ahmad',\n",
    "              'Putri', 'Fajar', 'Andi', 'Lina', 'Hendra', 'wati', 'Bimo', None],\n",
    "    'Usia': [28, 35, 24, 42, None, 29, 31, 27, 150, 33, 24, 38, -5, 26, 45, 30],  # 150 & -5 = outlier\n",
    "    'Gaji': ['5500000', '7200000', '4800000', '9100000', '6300000', None,\n",
    "              '5100000', '6700000', '8200000', '7500000', '4800000',\n",
    "              '5900000', '12000000000', '6100000', '8900000', '5700000'],  # 12M juta = outlier\n",
    "    'Departemen': ['IT', 'hr', 'IT', 'Finance', 'HR', 'it', 'Finance', 'HR',\n",
    "                   'IT', 'Finance', 'IT', 'hr', 'IT', 'Finance', 'HR', 'IT'],\n",
    "    'Tgl_Masuk': ['01/03/2020', '15-06-2019', '20/11/2021', '03/01/2018',\n",
    "                  '2022-07-25', '10/09/2020', '28/02/2021', '14-04-2019',\n",
    "                  '05/08/2022', '17/12/2019', '20/11/2021', '09/03/2020',\n",
    "                  '22-10-2017', '11/05/2021', '30/01/2023', '07/06/2020'],\n",
    "    'Pendidikan': ['S1', 'S2', 'S1', 'S3', 'D3', 'S1', 'S2', 'S1',\n",
    "                   'S1', 'S2', 'S1', 'D3', 'S2', 'S1', 'S3', 'S1'],\n",
    "    'Status': ['Aktif', 'aktif', 'AKTIF', 'Aktif', 'Resign', 'Aktif', 'Resign',\n",
    "                'Aktif', 'Aktif', 'aktif', 'AKTIF', 'Aktif', 'Resign', 'Aktif', 'Aktif', None],\n",
    "})\n",
    "\n",
    "print('=== DATA KOTOR AWAL ===')\n",
    "print(f'Shape: {dirty_data.shape}')\n",
    "print(dirty_data.to_string())\n",
    "\n",
    "print('\\n=== MASALAH YANG TERDETEKSI ===')\n",
    "print(f'1. Duplikasi: {dirty_data.duplicated(subset=[\"ID\"]).sum()} baris duplikat')\n",
    "print(f'2. Missing values per kolom:')\n",
    "print(dirty_data.isnull().sum()[dirty_data.isnull().sum() > 0])\n",
    "print(f'3. Format tidak konsisten (Departemen): {dirty_data[\"Departemen\"].unique()}')\n",
    "print(f'4. Format tidak konsisten (Status): {dirty_data[\"Status\"].unique()}')"
   ],
   "id": "cell-code-soal3-data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SOLUSI: Preprocessing Lengkap\n",
    "# ============================================================\n",
    "df_clean = dirty_data.copy()\n",
    "\n",
    "print('LANGKAH PREPROCESSING:')\n",
    "print('=' * 55)\n",
    "\n",
    "# LANGKAH 1: Hapus duplikat\n",
    "before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset=['ID'], keep='first')\n",
    "print(f'[1] Hapus duplikat: {before} -> {len(df_clean)} baris (dihapus: {before - len(df_clean)})')\n",
    "\n",
    "# LANGKAH 2: Standarisasi huruf besar/kecil\n",
    "df_clean['Nama'] = df_clean['Nama'].str.title()\n",
    "df_clean['Departemen'] = df_clean['Departemen'].str.upper()\n",
    "df_clean['Status'] = df_clean['Status'].str.capitalize()\n",
    "print(f'[2] Standarisasi casing: Nama, Departemen, Status')\n",
    "\n",
    "# LANGKAH 3: Konversi tipe data Gaji\n",
    "df_clean['Gaji'] = pd.to_numeric(df_clean['Gaji'], errors='coerce')\n",
    "print(f'[3] Konversi Gaji ke numerik: {df_clean[\"Gaji\"].dtype}')\n",
    "\n",
    "# LANGKAH 4: Deteksi dan tangani outlier Usia\n",
    "usia_valid = df_clean['Usia'].between(18, 65)\n",
    "n_outlier_usia = (~usia_valid & df_clean['Usia'].notna()).sum()\n",
    "df_clean.loc[~usia_valid, 'Usia'] = np.nan\n",
    "print(f'[4] Outlier Usia (di luar 18-65): {n_outlier_usia} nilai -> set ke NaN')\n",
    "\n",
    "# LANGKAH 5: Deteksi dan tangani outlier Gaji menggunakan IQR\n",
    "Q1_gaji = df_clean['Gaji'].quantile(0.25)\n",
    "Q3_gaji = df_clean['Gaji'].quantile(0.75)\n",
    "IQR_gaji = Q3_gaji - Q1_gaji\n",
    "lower_g = Q1_gaji - 1.5 * IQR_gaji\n",
    "upper_g = Q3_gaji + 1.5 * IQR_gaji\n",
    "n_outlier_gaji = ((df_clean['Gaji'] < lower_g) | (df_clean['Gaji'] > upper_g)).sum()\n",
    "df_clean.loc[(df_clean['Gaji'] < lower_g) | (df_clean['Gaji'] > upper_g), 'Gaji'] = np.nan\n",
    "print(f'[5] Outlier Gaji (IQR method): {n_outlier_gaji} nilai -> set ke NaN')\n",
    "print(f'    Batas valid: Rp{lower_g:,.0f} - Rp{upper_g:,.0f}')\n",
    "\n",
    "# LANGKAH 6: Imputasi missing values\n",
    "median_usia = df_clean['Usia'].median()\n",
    "median_gaji = df_clean['Gaji'].median()\n",
    "mode_status = df_clean['Status'].mode()[0]\n",
    "\n",
    "df_clean['Usia'] = df_clean['Usia'].fillna(median_usia)\n",
    "df_clean['Gaji'] = df_clean['Gaji'].fillna(median_gaji)\n",
    "df_clean['Status'] = df_clean['Status'].fillna(mode_status)\n",
    "df_clean['Nama'] = df_clean['Nama'].fillna('Tidak Diketahui')\n",
    "print(f'[6] Imputasi: Usia=median({median_usia}), Gaji=median(Rp{median_gaji:,.0f}), Status=mode({mode_status})')\n",
    "\n",
    "# LANGKAH 7: Parsing tanggal\n",
    "def parse_flexible_date(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "    date_str = str(date_str).replace('-', '/')\n",
    "    for fmt in ['%d/%m/%Y', '%Y/%m/%d']:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.NaT\n",
    "\n",
    "df_clean['Tgl_Masuk'] = df_clean['Tgl_Masuk'].apply(parse_flexible_date)\n",
    "df_clean['Tahun_Masuk'] = df_clean['Tgl_Masuk'].dt.year\n",
    "df_clean['Lama_Kerja_Tahun'] = 2024 - df_clean['Tahun_Masuk']\n",
    "print(f'[7] Parsing tanggal: {df_clean[\"Tgl_Masuk\"].notna().sum()}/{len(df_clean)} berhasil')\n",
    "\n",
    "# LANGKAH 8: Label Encoding untuk variabel kategoris\n",
    "le_dept = LabelEncoder()\n",
    "le_pend = LabelEncoder()\n",
    "le_stat = LabelEncoder()\n",
    "\n",
    "df_clean['Departemen_Enc'] = le_dept.fit_transform(df_clean['Departemen'])\n",
    "df_clean['Pendidikan_Enc'] = le_pend.fit_transform(df_clean['Pendidikan'])\n",
    "df_clean['Status_Enc'] = le_stat.fit_transform(df_clean['Status'])\n",
    "\n",
    "print(f'[8] Label Encoding:')\n",
    "print(f'    Departemen: {dict(zip(le_dept.classes_, le_dept.transform(le_dept.classes_)))}')\n",
    "print(f'    Status    : {dict(zip(le_stat.classes_, le_stat.transform(le_stat.classes_)))}')\n",
    "\n",
    "# LANGKAH 9: Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "df_clean['Usia_Scaled'] = scaler.fit_transform(df_clean[['Usia']])\n",
    "df_clean['Gaji_Scaled'] = scaler.fit_transform(df_clean[['Gaji']])\n",
    "print(f'[9] StandardScaler diterapkan pada Usia dan Gaji')\n",
    "\n",
    "print('\\n=== HASIL AKHIR SETELAH CLEANING ===')\n",
    "print(df_clean[['ID', 'Nama', 'Usia', 'Gaji', 'Departemen', 'Status', 'Lama_Kerja_Tahun']].to_string(index=False))\n",
    "\n",
    "print(f'\\nRingkasan:')\n",
    "print(f'  Baris awal   : {len(dirty_data)}')\n",
    "print(f'  Baris akhir  : {len(df_clean)}')\n",
    "print(f'  Missing final: {df_clean[[\"Usia\",\"Gaji\",\"Nama\",\"Status\"]].isnull().sum().sum()}')"
   ],
   "id": "cell-code-soal3-solution"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soal 4: EDA & Visualisasi\n",
    "**Bobot: 25 poin**\n",
    "\n",
    "Lakukan EDA lengkap pada dataset Iris (classic ML dataset) dan Tips dataset. Buat visualisasi yang informatif."
   ],
   "id": "cell-md-soal4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy import stats\n",
    "\n",
    "# Load dataset Iris\n",
    "iris_raw = load_iris()\n",
    "iris_df = pd.DataFrame(iris_raw.data, columns=iris_raw.feature_names)\n",
    "iris_df['species'] = [iris_raw.target_names[t] for t in iris_raw.target]\n",
    "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "print('=== Dataset Iris ===')\n",
    "print(f'Shape: {iris_df.shape}')\n",
    "print(iris_df.head())\n",
    "print('\\nDistribusi spesies:')\n",
    "print(iris_df['species'].value_counts())\n",
    "\n",
    "# ============================================================\n",
    "# Statistik Deskriptif per Spesies\n",
    "# ============================================================\n",
    "print('\\n=== Statistik Deskriptif per Spesies ===')\n",
    "print(iris_df.groupby('species')[['sepal_length', 'petal_length']]\n",
    "             .agg(['mean', 'std', 'min', 'max'])\n",
    "             .round(2))\n",
    "\n",
    "# ============================================================\n",
    "# Visualisasi Komprehensif\n",
    "# ============================================================\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "fig.suptitle('EDA Dataset Iris - Analisis Komprehensif', fontsize=15, fontweight='bold', y=1.01)\n",
    "\n",
    "# 1. Pair plot (scatter matrix)\n",
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "# Plot 1: Distribusi setiap fitur\n",
    "ax1 = fig.add_subplot(3, 3, 1)\n",
    "for species in iris_df['species'].unique():\n",
    "    subset = iris_df[iris_df['species'] == species]['petal_length']\n",
    "    ax1.hist(subset, alpha=0.6, label=species, bins=15)\n",
    "ax1.set_title('Distribusi Petal Length')\n",
    "ax1.set_xlabel('Petal Length (cm)')\n",
    "ax1.legend(fontsize=8)\n",
    "\n",
    "# Plot 2: Box plot per spesies\n",
    "ax2 = fig.add_subplot(3, 3, 2)\n",
    "iris_df.boxplot(column='petal_width', by='species', ax=ax2)\n",
    "ax2.set_title('Petal Width per Spesies')\n",
    "ax2.set_xlabel('Spesies')\n",
    "plt.sca(ax2)\n",
    "plt.title('Petal Width per Spesies')\n",
    "\n",
    "# Plot 3: Scatter plot\n",
    "ax3 = fig.add_subplot(3, 3, 3)\n",
    "colors = {'setosa': 'blue', 'versicolor': 'orange', 'virginica': 'green'}\n",
    "for species, color in colors.items():\n",
    "    subset = iris_df[iris_df['species'] == species]\n",
    "    ax3.scatter(subset['sepal_length'], subset['petal_length'],\n",
    "               c=color, label=species, alpha=0.6)\n",
    "ax3.set_xlabel('Sepal Length')\n",
    "ax3.set_ylabel('Petal Length')\n",
    "ax3.set_title('Sepal vs Petal Length')\n",
    "ax3.legend(fontsize=8)\n",
    "\n",
    "# Plot 4: Heatmap korelasi\n",
    "ax4 = fig.add_subplot(3, 3, 4)\n",
    "corr = iris_df[features].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdYlGn', ax=ax4,\n",
    "            center=0, square=True, linewidths=0.5, annot_kws={'size': 8})\n",
    "ax4.set_title('Heatmap Korelasi')\n",
    "\n",
    "# Plot 5: Violin plot\n",
    "ax5 = fig.add_subplot(3, 3, 5)\n",
    "sns.violinplot(data=iris_df, x='species', y='sepal_length',\n",
    "               palette='muted', inner='quartile', ax=ax5)\n",
    "ax5.set_title('Violin Plot Sepal Length')\n",
    "ax5.set_xlabel('')\n",
    "\n",
    "# Plot 6: KDE plot\n",
    "ax6 = fig.add_subplot(3, 3, 6)\n",
    "for species in iris_df['species'].unique():\n",
    "    subset = iris_df[iris_df['species'] == species]['petal_width']\n",
    "    subset.plot.kde(ax=ax6, label=species)\n",
    "ax6.set_title('KDE Plot Petal Width')\n",
    "ax6.set_xlabel('Petal Width (cm)')\n",
    "ax6.legend(fontsize=8)\n",
    "\n",
    "# Plot 7: Mean per spesies (bar chart)\n",
    "ax7 = fig.add_subplot(3, 1, 3)\n",
    "mean_vals = iris_df.groupby('species')[features].mean()\n",
    "mean_vals.T.plot(kind='bar', ax=ax7, colormap='viridis', alpha=0.8, edgecolor='black')\n",
    "ax7.set_title('Rata-rata Setiap Fitur per Spesies')\n",
    "ax7.set_xlabel('Fitur')\n",
    "ax7.set_ylabel('Nilai rata-rata (cm)')\n",
    "ax7.legend(title='Spesies', loc='upper left', fontsize=9)\n",
    "ax7.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Analisis Korelasi\n",
    "# ============================================================\n",
    "print('\\n=== Analisis Korelasi (Pearson) ===')\n",
    "corr_matrix = iris_df[features].corr()\n",
    "print(corr_matrix.round(4))\n",
    "\n",
    "r, p = stats.pearsonr(iris_df['petal_length'], iris_df['petal_width'])\n",
    "print(f'\\nKorelasi petal_length ~ petal_width: r={r:.4f}, p={p:.2e}')\n",
    "print(f'Interpretasi: KORELASI SANGAT KUAT POSITIF (r > 0.9)')\n",
    "\n",
    "# ============================================================\n",
    "# Deteksi Outlier\n",
    "# ============================================================\n",
    "print('\\n=== Deteksi Outlier (IQR Method) ===')\n",
    "for feat in features:\n",
    "    Q1 = iris_df[feat].quantile(0.25)\n",
    "    Q3 = iris_df[feat].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    n_out = ((iris_df[feat] < Q1 - 1.5*IQR) | (iris_df[feat] > Q3 + 1.5*IQR)).sum()\n",
    "    print(f'  {feat:15s}: {n_out} outlier')"
   ],
   "id": "cell-code-soal4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soal 5: Analisis Data Terintegrasi\n",
    "**Bobot: 15 poin**\n",
    "\n",
    "Mini project end-to-end: mulai dari pembuatan dataset simulasi, cleaning, explorasi, hingga visualisasi dan insight bisnis."
   ],
   "id": "cell-md-soal5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# LANGKAH 1: LOAD / BUAT DATA\n",
    "# ============================================================\n",
    "print('LANGKAH 1: Membuat dataset penjualan e-commerce...')\n",
    "\n",
    "n = 1000\n",
    "cities = ['Jakarta', 'Bandung', 'Surabaya', 'Medan', 'Makassar', 'Yogyakarta']\n",
    "categories = ['Elektronik', 'Pakaian', 'Makanan', 'Kecantikan', 'Olahraga']\n",
    "months = list(range(1, 13))\n",
    "\n",
    "df_raw = pd.DataFrame({\n",
    "    'order_id': range(1001, 1001 + n),\n",
    "    'tanggal': pd.date_range('2024-01-01', periods=n, freq='8h'),\n",
    "    'kota': np.random.choice(cities, n, p=[0.35, 0.20, 0.20, 0.10, 0.08, 0.07]),\n",
    "    'kategori': np.random.choice(categories, n, p=[0.25, 0.30, 0.20, 0.15, 0.10]),\n",
    "    'harga': np.round(np.random.lognormal(mean=12.5, sigma=1.0, size=n), -3),\n",
    "    'qty': np.random.randint(1, 6, n),\n",
    "    'rating': np.clip(np.random.normal(4.0, 0.8, n), 1, 5).round(1),\n",
    "    'is_returned': np.random.choice([False, True], n, p=[0.93, 0.07]),\n",
    "})\n",
    "\n",
    "# Tambahkan noise (missing values)\n",
    "mask = np.random.choice([True, False], n, p=[0.03, 0.97])\n",
    "df_raw.loc[mask, 'rating'] = np.nan\n",
    "\n",
    "print(f'  Dataset: {df_raw.shape[0]} baris x {df_raw.shape[1]} kolom')\n",
    "\n",
    "# ============================================================\n",
    "# LANGKAH 2: CLEANING\n",
    "# ============================================================\n",
    "print('\\nLANGKAH 2: Cleaning...')\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Tangani missing\n",
    "n_missing = df['rating'].isnull().sum()\n",
    "df['rating'] = df['rating'].fillna(df.groupby('kategori')['rating'].transform('median'))\n",
    "print(f'  Missing rating: {n_missing} -> diisi dengan median per kategori')\n",
    "\n",
    "# Feature engineering\n",
    "df['bulan'] = df['tanggal'].dt.month\n",
    "df['hari_minggu'] = df['tanggal'].dt.day_name()\n",
    "df['revenue'] = df['harga'] * df['qty']\n",
    "df['revenue_bersih'] = df['revenue'] * (~df['is_returned']).astype(int)\n",
    "\n",
    "print(f'  Fitur baru: bulan, hari_minggu, revenue, revenue_bersih')\n",
    "\n",
    "# ============================================================\n",
    "# LANGKAH 3: EDA\n",
    "# ============================================================\n",
    "print('\\nLANGKAH 3: EDA...')\n",
    "\n",
    "total_revenue = df['revenue_bersih'].sum()\n",
    "avg_order = df['revenue'].mean()\n",
    "return_rate = df['is_returned'].mean() * 100\n",
    "avg_rating = df['rating'].mean()\n",
    "\n",
    "print(f'  Total revenue bersih: Rp{total_revenue:,.0f}')\n",
    "print(f'  Rata-rata order value: Rp{avg_order:,.0f}')\n",
    "print(f'  Return rate: {return_rate:.1f}%')\n",
    "print(f'  Rata-rata rating: {avg_rating:.2f}/5.0')\n",
    "\n",
    "# ============================================================\n",
    "# LANGKAH 4: VISUALISASI\n",
    "# ============================================================\n",
    "print('\\nLANGKAH 4: Membuat visualisasi...')\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "fig.suptitle('Dashboard Analisis Penjualan E-Commerce 2024', fontsize=15, fontweight='bold')\n",
    "\n",
    "# 1. Revenue per kategori\n",
    "rev_kategori = df.groupby('kategori')['revenue_bersih'].sum().sort_values(ascending=True)\n",
    "rev_kategori.plot(kind='barh', ax=axes[0, 0], color='steelblue', edgecolor='white')\n",
    "axes[0, 0].set_title('Total Revenue Bersih per Kategori')\n",
    "axes[0, 0].set_xlabel('Revenue (Rp)')\n",
    "for i, v in enumerate(rev_kategori.values):\n",
    "    axes[0, 0].text(v, i, f' Rp{v/1e6:.0f}jt', va='center', fontsize=8)\n",
    "\n",
    "# 2. Revenue per kota (pie chart)\n",
    "rev_kota = df.groupby('kota')['revenue_bersih'].sum()\n",
    "axes[0, 1].pie(rev_kota.values, labels=rev_kota.index, autopct='%1.1f%%',\n",
    "               startangle=90, colors=sns.color_palette('pastel'))\n",
    "axes[0, 1].set_title('Distribusi Revenue per Kota')\n",
    "\n",
    "# 3. Trend revenue per bulan\n",
    "monthly = df.groupby('bulan')['revenue_bersih'].sum()\n",
    "axes[0, 2].plot(monthly.index, monthly.values, marker='o', linewidth=2, color='coral')\n",
    "axes[0, 2].fill_between(monthly.index, monthly.values, alpha=0.2, color='coral')\n",
    "axes[0, 2].set_title('Trend Revenue per Bulan')\n",
    "axes[0, 2].set_xlabel('Bulan')\n",
    "axes[0, 2].set_ylabel('Revenue (Rp)')\n",
    "axes[0, 2].set_xticks(range(1, 13))\n",
    "\n",
    "# 4. Distribusi rating per kategori\n",
    "sns.boxplot(data=df, x='kategori', y='rating', palette='muted', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Distribusi Rating per Kategori')\n",
    "axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 5. Heatmap: revenue per kota per kategori\n",
    "pivot = df.pivot_table(values='revenue_bersih', index='kota', columns='kategori', aggfunc='sum')\n",
    "sns.heatmap(pivot / 1e6, annot=True, fmt='.0f', cmap='YlOrRd',\n",
    "            ax=axes[1, 1], annot_kws={'size': 8})\n",
    "axes[1, 1].set_title('Revenue (juta Rp): Kota vs Kategori')\n",
    "axes[1, 1].tick_params(axis='x', rotation=20)\n",
    "\n",
    "# 6. Return rate per kategori\n",
    "return_by_cat = df.groupby('kategori')['is_returned'].mean() * 100\n",
    "return_by_cat.sort_values(ascending=False).plot(\n",
    "    kind='bar', ax=axes[1, 2], color='salmon', edgecolor='white'\n",
    ")\n",
    "axes[1, 2].set_title('Return Rate per Kategori (%)')\n",
    "axes[1, 2].set_xlabel('')\n",
    "axes[1, 2].tick_params(axis='x', rotation=15)\n",
    "axes[1, 2].axhline(y=return_rate, color='red', linestyle='--', label=f'Avg={return_rate:.1f}%')\n",
    "axes[1, 2].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# LANGKAH 5: INSIGHTS & REKOMENDASI\n",
    "# ============================================================\n",
    "print('\\nLANGKAH 5: Insights & Rekomendasi Bisnis')\n",
    "print('=' * 60)\n",
    "\n",
    "top_kategori = rev_kategori.idxmax()\n",
    "top_kota = rev_kota.idxmax()\n",
    "best_rating_cat = df.groupby('kategori')['rating'].mean().idxmax()\n",
    "highest_return_cat = return_by_cat.idxmax()\n",
    "best_month = monthly.idxmax()\n",
    "\n",
    "print(f'TEMUAN UTAMA:')\n",
    "print(f'1. Kategori teratas (revenue): {top_kategori}')\n",
    "print(f'2. Kota dengan revenue terbesar: {top_kota}')\n",
    "print(f'3. Kategori dengan rating tertinggi: {best_rating_cat}')\n",
    "print(f'4. Kategori dengan return rate tertinggi: {highest_return_cat}')\n",
    "print(f'5. Bulan tersibuk (revenue tertinggi): Bulan ke-{best_month}')\n",
    "\n",
    "print(f'\\nREKOMENDASI:')\n",
    "print(f'- Fokus promosi pada kategori {top_kategori} di kota {top_kota}')\n",
    "print(f'- Perbaiki kualitas produk {highest_return_cat} untuk kurangi return rate')\n",
    "print(f'- Rencanakan stok lebih banyak menjelang bulan ke-{best_month}')"
   ],
   "id": "cell-code-soal5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kunci Jawaban & Penilaian\n",
    "\n",
    "Bagian ini berisi rubrik penilaian untuk setiap soal."
   ],
   "id": "cell-md-scoring"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubrik Penilaian UTS\n",
    "\n",
    "rubrik = {\n",
    "    'Soal 1 - Apache Spark RDD (20 poin)': {\n",
    "        'Berhasil inisialisasi SparkSession': 3,\n",
    "        'Membuat RDD dan operasi dasar (count, sum, mean)': 4,\n",
    "        'WordCount dengan flatMap + map + reduceByKey': 7,\n",
    "        'Key-Value RDD + groupByKey + analisis': 6,\n",
    "    },\n",
    "    'Soal 2 - Web Scraping & API (15 poin)': {\n",
    "        'GET request dengan error handling': 4,\n",
    "        'Parsing JSON ke DataFrame': 4,\n",
    "        'Analisis & agregasi data API': 4,\n",
    "        'Implementasi retry logic': 3,\n",
    "    },\n",
    "    'Soal 3 - Data Preprocessing (25 poin)': {\n",
    "        'Identifikasi semua masalah data': 5,\n",
    "        'Hapus duplikat': 2,\n",
    "        'Standarisasi format': 3,\n",
    "        'Handling outlier Usia dan Gaji': 5,\n",
    "        'Imputasi missing values dengan metode tepat': 5,\n",
    "        'Parsing tanggal + feature engineering': 3,\n",
    "        'Encoding + scaling': 2,\n",
    "    },\n",
    "    'Soal 4 - EDA & Visualisasi (25 poin)': {\n",
    "        'Statistik deskriptif per spesies': 5,\n",
    "        'Minimum 5 jenis visualisasi': 10,\n",
    "        'Analisis korelasi dengan interpretasi': 5,\n",
    "        'Deteksi outlier': 5,\n",
    "    },\n",
    "    'Soal 5 - Analisis Terintegrasi (15 poin)': {\n",
    "        'Pembuatan dan cleaning dataset': 3,\n",
    "        'Feature engineering': 3,\n",
    "        'EDA dengan statistik ringkasan': 4,\n",
    "        'Dashboard visualisasi (min 4 plot)': 3,\n",
    "        'Insight & rekomendasi bisnis': 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "print('╔══════════════════════════════════════════════════════════════╗')\n",
    "print('║                  RUBRIK PENILAIAN UTS                      ║')\n",
    "print('║              Big Data Analytics - Minggu 8                 ║')\n",
    "print('╠══════════════════════════════════════════════════════════════╣')\n",
    "\n",
    "total_bobot = 0\n",
    "for soal, kriteria in rubrik.items():\n",
    "    bobot_soal = sum(kriteria.values())\n",
    "    total_bobot += bobot_soal\n",
    "    print(f'\\n║ {soal}')\n",
    "    for k, v in kriteria.items():\n",
    "        print(f'║   [{v:2d} poin] {k}')\n",
    "\n",
    "print(f'\\n╠══════════════════════════════════════════════════════════════╣')\n",
    "print(f'║  TOTAL NILAI MAKSIMUM: {total_bobot} POIN                           ║')\n",
    "print(f'╠══════════════════════════════════════════════════════════════╣')\n",
    "print(f'║  Konversi Nilai:                                            ║')\n",
    "print(f'║  90-100 = A (Sangat Baik)                                  ║')\n",
    "print(f'║  80-89  = B (Baik)                                         ║')\n",
    "print(f'║  70-79  = C (Cukup)                                        ║')\n",
    "print(f'║  60-69  = D (Kurang)                                       ║')\n",
    "print(f'║  < 60   = E (Tidak Lulus)                                  ║')\n",
    "print(f'╚══════════════════════════════════════════════════════════════╝')"
   ],
   "id": "cell-code-rubrik"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Selesaikan soal-soal latihan UTS tambahan berikut:\n",
    "\n",
    "### Latihan 1: Review Hadoop & Spark\n",
    "Jelaskan dengan kode/pseudocode:\n",
    "- Bagaimana HDFS menyimpan file besar? Gambarkan dengan ASCII art\n",
    "- Apa perbedaan `reduceByKey()` vs `groupByKey()` di Spark dari sisi performa?\n",
    "- Buat contoh kode Spark untuk menghitung rata-rata nilai mahasiswa per fakultas\n",
    "\n",
    "### Latihan 2: Review Preprocessing\n",
    "Diberikan dataset berikut, lakukan preprocessing lengkap:\n",
    "```\n",
    "Nama, Usia, Nilai, Kota, Kategori\n",
    "Budi, 22, 85.5, jakarta, A\n",
    "Sari, , 92.0, BANDUNG, B\n",
    "Andi, 19, , surabaya, A\n",
    "Maya, 250, 78.3, Yogyakarta, C\n",
    "```\n",
    "- Tangani semua masalah yang ditemukan\n",
    "- Lakukan One-Hot Encoding untuk kolom Kota\n",
    "- Normalisasi kolom Nilai ke range [0, 1]\n",
    "\n",
    "### Latihan 3: Review EDA\n",
    "Gunakan dataset `diamonds` dari seaborn:\n",
    "- Lakukan EDA lengkap (statistik, distribusi, korelasi, outlier)\n",
    "- Temukan 3 insight menarik dari data\n",
    "- Buat minimal 4 visualisasi yang berbeda jenis\n",
    "\n",
    "### Latihan 4: Review NoSQL vs SQL\n",
    "Buat perbandingan tertulis (dalam markdown cell) dan kode simulasi:\n",
    "- Simpan data yang sama (data mahasiswa) dalam format SQL (SQLite) dan NoSQL (dict-based)\n",
    "- Tunjukkan query yang sama dilakukan di kedua sistem\n",
    "- Diskusikan kapan masing-masing lebih cocok digunakan\n",
    "\n",
    "### Latihan 5: Soal Essay UTS\n",
    "Jawab pertanyaan berikut dalam markdown cell:\n",
    "1. Sebuah startup fintech memiliki 10 juta transaksi per hari dengan 50+ atribut per transaksi. Rekomendasikan arsitektur penyimpanan data yang tepat. Jelaskan alasannya!\n",
    "2. Mengapa Spark lebih cepat dari Hadoop MapReduce untuk iterative algorithms (seperti ML training)? Jelaskan dengan diagram!\n",
    "3. Apa yang dimaksud dengan CAP Theorem? Berikan contoh sistem nyata untuk setiap kategori (CP, AP, CA)."
   ],
   "id": "cell-md-tasks"
  }
 ]
}

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 16: Panduan & Template Proyek Akhir\n",
    "## *Week 16 Lab: Final Project Guide & Template*\n",
    "\n",
    "**Mata Kuliah / Course:** Big Data Analytics  \n",
    "**Topik / Topic:** Final Project â€” End-to-End Data Analytics  \n",
    "\n",
    "---\n",
    "### Deskripsi / Description\n",
    "\n",
    "**ğŸ‡®ğŸ‡© Bahasa Indonesia:**  \n",
    "Notebook ini menyediakan panduan komprehensif, template kode, dan tiga contoh proyek\n",
    "lengkap sebagai referensi untuk Proyek Akhir mata kuliah Big Data Analytics.\n",
    "Gunakan notebook ini sebagai titik awal dan kembangkan sesuai topik proyek Anda.\n",
    "\n",
    "**ğŸ‡¬ğŸ‡§ English:**  \n",
    "This notebook provides a comprehensive guide, code templates, and three complete\n",
    "example projects as reference for the Big Data Analytics Final Project.\n",
    "Use this as a starting point and develop it according to your chosen project topic.\n",
    "\n",
    "---\n",
    "### Topik Contoh Proyek dalam Notebook Ini:\n",
    "1. Analisis Sentimen Review Produk (NLP)\n",
    "2. Prediksi Harga Rumah (Regression)\n",
    "3. Segmentasi Pelanggan / RFM Analysis (Clustering)"
   ],
   "id": "cell-md-title"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Proyek Akhir\n",
    "\n",
    "Gunakan template struktur kode berikut sebagai kerangka proyek Anda.\n",
    "Setiap bagian harus diisi dengan kode dan analisis yang relevan."
   ],
   "id": "cell-md-template"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEMPLATE PROYEK AKHIR BIG DATA ANALYTICS\n",
    "# ============================================================\n",
    "# Nama       : [Nama Anda]\n",
    "# NIM        : [NIM Anda]\n",
    "# Topik      : [Judul Proyek Anda]\n",
    "# Tanggal    : [Tanggal]\n",
    "# ============================================================\n",
    "\n",
    "# --- 0. IMPORTS & SETUP ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "# df = pd.read_csv('your_dataset.csv')  # load your data here\n",
    "\n",
    "# --- 2. EXPLORATORY DATA ANALYSIS (EDA) ---\n",
    "# print(df.head())\n",
    "# print(df.describe())\n",
    "# print(df.info())\n",
    "# [visualizations...]\n",
    "\n",
    "# --- 3. DATA PREPROCESSING ---\n",
    "# [handle missing values, encode categoricals, scale features...]\n",
    "\n",
    "# --- 4. FEATURE ENGINEERING ---\n",
    "# [create new features, select important features...]\n",
    "\n",
    "# --- 5. MODELING ---\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "# [train models, cross-validation, hyperparameter tuning...]\n",
    "\n",
    "# --- 6. EVALUATION ---\n",
    "# [metrics, confusion matrix, learning curves...]\n",
    "\n",
    "# --- 7. INSIGHTS & CONCLUSIONS ---\n",
    "# [business/research insights, limitations, future work...]\n",
    "\n",
    "print('Template structure loaded. Replace comments with your actual code!')\n",
    "print('\\nCRISP-DM Phases Checklist:')\n",
    "phases = [\n",
    "    'âœ… 1. Business Understanding â€” define problem & success criteria',\n",
    "    'âœ… 2. Data Understanding  â€” collect data & exploratory analysis',\n",
    "    'âœ… 3. Data Preparation    â€” cleaning, encoding, feature engineering',\n",
    "    'âœ… 4. Modeling            â€” select algorithm, train, tune',\n",
    "    'âœ… 5. Evaluation          â€” measure performance, check business goals',\n",
    "    'âœ… 6. Deployment          â€” save model, create API or dashboard'\n",
    "]\n",
    "for phase in phases:\n",
    "    print(f'  {phase}')"
   ],
   "id": "cell-template-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Contoh Proyek: Analisis Sentimen Review Produk\n",
    "\n",
    "**Masalah**: Sebuah platform e-commerce ingin memahami sentimen pelanggan terhadap\n",
    "produk-produknya secara otomatis dari ribuan review teks.\n",
    "\n",
    "**Pendekatan**: NLP pipeline â€” preprocessing teks â†’ TF-IDF features â†’ klasifikasi sentimen."
   ],
   "id": "cell-md-project1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# --- Generate synthetic product review dataset ---\n",
    "positive_reviews = [\n",
    "    \"Absolutely love this product! Best purchase ever.\",\n",
    "    \"Excellent quality and fast delivery. Very satisfied.\",\n",
    "    \"Works perfectly. Exceeded my expectations completely.\",\n",
    "    \"Fantastic item! Will definitely buy again.\",\n",
    "    \"Great value for money. Highly recommend to everyone.\",\n",
    "    \"Amazing product. The quality is outstanding.\",\n",
    "    \"Super happy with this purchase. Works as described.\",\n",
    "    \"Perfect gift. My family loves it very much.\",\n",
    "    \"Top quality product with great customer support.\",\n",
    "    \"Wonderful experience. Fast shipping and great packaging.\",\n",
    "    \"Really impressed with the build quality. Five stars!\",\n",
    "    \"Exactly what I needed. Very happy with the purchase.\",\n",
    "    \"Outstanding performance and durable material. Love it!\",\n",
    "    \"Brilliant product. Does everything I hoped for.\",\n",
    "    \"Superb quality. My second order and still satisfied!\",\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"Terrible quality. Broke after 3 days of use.\",\n",
    "    \"Complete waste of money. Nothing works as advertised.\",\n",
    "    \"Very disappointed with this product. Do not buy!\",\n",
    "    \"Poor quality materials. Feels extremely cheap and flimsy.\",\n",
    "    \"Worst purchase I've made. Will never buy again.\",\n",
    "    \"Arrived broken and seller refused to help. Terrible!\",\n",
    "    \"Stopped working after a week. Terrible durability.\",\n",
    "    \"Not worth the price at all. Very disappointed.\",\n",
    "    \"Awful product. Completely different from the description.\",\n",
    "    \"Returned immediately. Defective right out of the box.\",\n",
    "    \"Horrible experience. Packaging was damaged and item missing.\",\n",
    "    \"Garbage product. Falls apart immediately after use.\",\n",
    "    \"False advertising. The product looks nothing like photos.\",\n",
    "    \"Cheap and useless. Avoid at all costs.\",\n",
    "    \"Broken on arrival. Seller unresponsive. Waste of money.\",\n",
    "]\n",
    "\n",
    "neutral_reviews = [\n",
    "    \"It's okay. Does the job but nothing special about it.\",\n",
    "    \"Average product. Works as expected, nothing more.\",\n",
    "    \"Decent for the price. Not amazing but not bad either.\",\n",
    "    \"Mediocre quality. Some features work, others don't.\",\n",
    "    \"Reasonable product. Got what I paid for.\",\n",
    "    \"Fair quality. It serves its basic purpose adequately.\",\n",
    "    \"Standard item. Expected better but it's acceptable.\",\n",
    "    \"Not great, not terrible. Middle of the road product.\",\n",
    "    \"OK for casual use. Wouldn't rely on it heavily though.\",\n",
    "    \"Satisfactory. Does what it says on the box.\",\n",
    "]\n",
    "\n",
    "# Create dataset\n",
    "reviews = positive_reviews + negative_reviews + neutral_reviews\n",
    "sentiments = (['positive'] * len(positive_reviews) +\n",
    "              ['negative'] * len(negative_reviews) +\n",
    "              ['neutral']  * len(neutral_reviews))\n",
    "\n",
    "df_reviews = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
    "# Shuffle\n",
    "df_reviews = df_reviews.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f'Dataset: {len(df_reviews)} reviews')\n",
    "print(df_reviews['sentiment'].value_counts())\n",
    "\n",
    "# Text preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s']\", '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_reviews['cleaned'] = df_reviews['review'].apply(clean_text)\n",
    "\n",
    "# TF-IDF + Logistic Regression pipeline\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    df_reviews['cleaned'], df_reviews['sentiment'],\n",
    "    test_size=0.25, random_state=42, stratify=df_reviews['sentiment']\n",
    ")\n",
    "\n",
    "tfidf_r = TfidfVectorizer(ngram_range=(1, 2), max_features=500, min_df=1)\n",
    "X_train_tfidf_r = tfidf_r.fit_transform(X_train_r)\n",
    "X_test_tfidf_r  = tfidf_r.transform(X_test_r)\n",
    "\n",
    "lr_sentiment = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_sentiment.fit(X_train_tfidf_r, y_train_r)\n",
    "\n",
    "y_pred_r = lr_sentiment.predict(X_test_tfidf_r)\n",
    "acc = accuracy_score(y_test_r, y_pred_r)\n",
    "\n",
    "print(f'\\nSentiment Classification Accuracy: {acc:.2%}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test_r, y_pred_r))\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_r = confusion_matrix(y_test_r, y_pred_r, labels=['positive', 'neutral', 'negative'])\n",
    "sns.heatmap(cm_r, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['positive','neutral','negative'],\n",
    "            yticklabels=['positive','neutral','negative'])\n",
    "axes[0].set_title('Sentiment Analysis â€” Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Sentiment distribution in dataset\n",
    "df_reviews['sentiment'].value_counts().plot(kind='bar', ax=axes[1],\n",
    "    color=['tomato','steelblue','sandybrown'], edgecolor='white')\n",
    "axes[1].set_title('Sentiment Distribution in Dataset')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test on new reviews\n",
    "new_reviews = [\n",
    "    \"This product is incredible! Best thing I ever bought.\",\n",
    "    \"Broke after one use. Absolutely terrible quality.\",\n",
    "    \"It works fine, nothing exceptional.\"\n",
    "]\n",
    "new_cleaned = [clean_text(r) for r in new_reviews]\n",
    "new_tfidf = tfidf_r.transform(new_cleaned)\n",
    "new_preds = lr_sentiment.predict(new_tfidf)\n",
    "new_probas = lr_sentiment.predict_proba(new_tfidf)\n",
    "classes = lr_sentiment.classes_\n",
    "\n",
    "print('\\n=== Predictions on New Reviews ===')\n",
    "for rev, pred, proba in zip(new_reviews, new_preds, new_probas):\n",
    "    prob_dict = dict(zip(classes, proba.round(2)))\n",
    "    print(f'  Review: \"{rev[:50]}...\"')\n",
    "    print(f'  Prediction: {pred.upper()} | Probabilities: {prob_dict}\\n')"
   ],
   "id": "cell-project1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Contoh Proyek: Prediksi Harga Rumah\n",
    "\n",
    "**Masalah**: Membangun model machine learning untuk memprediksi harga rumah\n",
    "berdasarkan fitur-fitur properti.\n",
    "\n",
    "**Pendekatan**: Regression analysis â€” EDA â†’ feature engineering â†’ multiple regression models."
   ],
   "id": "cell-md-project2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Generate synthetic housing dataset ---\n",
    "def generate_housing_data(n=800, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    area_m2     = np.random.uniform(30, 500, n)\n",
    "    bedrooms    = np.random.choice([1, 2, 3, 4, 5], n, p=[0.05, 0.20, 0.40, 0.25, 0.10])\n",
    "    bathrooms   = np.clip(bedrooms - 1 + np.random.randint(0, 2, n), 1, 4)\n",
    "    age_years   = np.random.randint(0, 40, n)\n",
    "    distance_km = np.random.exponential(scale=10, n).clip(1, 50)  # distance to city center\n",
    "    location    = np.random.choice(['Premium', 'Standard', 'Suburban'], n, p=[0.2, 0.5, 0.3])\n",
    "    garage      = np.random.choice([0, 1], n, p=[0.35, 0.65])\n",
    "    pool        = np.random.choice([0, 1], n, p=[0.75, 0.25])\n",
    "\n",
    "    loc_multiplier = {'Premium': 1.5, 'Standard': 1.0, 'Suburban': 0.7}\n",
    "    loc_mult = np.array([loc_multiplier[l] for l in location])\n",
    "\n",
    "    # Price formula (in millions IDR)\n",
    "    price = (\n",
    "        5 * area_m2\n",
    "        + 150 * bedrooms\n",
    "        + 100 * bathrooms\n",
    "        - 15 * age_years\n",
    "        - 20 * distance_km\n",
    "        + 300 * garage\n",
    "        + 500 * pool\n",
    "        + np.random.normal(0, 100, n)\n",
    "    ) * loc_mult\n",
    "    price = price.clip(200, 15000)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'area_m2': area_m2.round(1), 'bedrooms': bedrooms, 'bathrooms': bathrooms,\n",
    "        'age_years': age_years, 'distance_km': distance_km.round(1),\n",
    "        'location': location, 'garage': garage, 'pool': pool,\n",
    "        'price_million_idr': price.round(1)\n",
    "    })\n",
    "\n",
    "df_house = generate_housing_data()\n",
    "print(f'Housing dataset: {df_house.shape}')\n",
    "print(df_house.describe().round(1))\n",
    "\n",
    "# Feature engineering\n",
    "df_house['price_per_m2'] = (df_house['price_million_idr'] / df_house['area_m2']).round(2)\n",
    "df_house['total_rooms']  = df_house['bedrooms'] + df_house['bathrooms']\n",
    "df_house['is_new']       = (df_house['age_years'] <= 5).astype(int)\n",
    "\n",
    "# Preprocessing\n",
    "df_house_enc = pd.get_dummies(df_house, columns=['location'], drop_first=False)\n",
    "feature_cols_h = ['area_m2', 'bedrooms', 'bathrooms', 'age_years', 'distance_km',\n",
    "                   'garage', 'pool', 'total_rooms', 'is_new',\n",
    "                   'location_Premium', 'location_Standard', 'location_Suburban']\n",
    "\n",
    "# Keep only existing columns\n",
    "feature_cols_h = [c for c in feature_cols_h if c in df_house_enc.columns]\n",
    "X_h = df_house_enc[feature_cols_h]\n",
    "y_h = df_house['price_million_idr']\n",
    "\n",
    "X_tr_h, X_te_h, y_tr_h, y_te_h = train_test_split(X_h, y_h, test_size=0.2, random_state=42)\n",
    "scaler_h = StandardScaler()\n",
    "X_tr_h_sc = scaler_h.fit_transform(X_tr_h)\n",
    "X_te_h_sc = scaler_h.transform(X_te_h)\n",
    "\n",
    "# Train regression models\n",
    "regression_models = {\n",
    "    'Ridge':             Ridge(alpha=10),\n",
    "    'Random Forest':     RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "print('\\n=== Regression Model Comparison ===')\n",
    "reg_results = []\n",
    "for name, model in regression_models.items():\n",
    "    X_tr_use = X_tr_h_sc if name == 'Ridge' else X_tr_h\n",
    "    X_te_use = X_te_h_sc if name == 'Ridge' else X_te_h\n",
    "    model.fit(X_tr_use, y_tr_h)\n",
    "    y_pred_h = model.predict(X_te_use)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te_h, y_pred_h))\n",
    "    mae  = mean_absolute_error(y_te_h, y_pred_h)\n",
    "    r2   = r2_score(y_te_h, y_pred_h)\n",
    "    reg_results.append({'Model': name, 'RMSE': round(rmse,1), 'MAE': round(mae,1), 'RÂ²': round(r2,4)})\n",
    "    print(f'{name:20s}: RMSE={rmse:.1f}M | MAE={mae:.1f}M | RÂ²={r2:.4f}')\n",
    "\n",
    "# Best model: Random Forest\n",
    "best_reg = regression_models['Random Forest']\n",
    "best_reg.fit(X_tr_h, y_tr_h)\n",
    "y_pred_best_h = best_reg.predict(X_te_h)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "ax = axes[0]\n",
    "ax.scatter(y_te_h, y_pred_best_h, alpha=0.5, s=20, color='steelblue')\n",
    "min_v, max_v = min(y_te_h.min(), y_pred_best_h.min()), max(y_te_h.max(), y_pred_best_h.max())\n",
    "ax.plot([min_v, max_v], [min_v, max_v], 'r--', linewidth=1.5, label='Perfect prediction')\n",
    "ax.set_xlabel('Actual Price (M IDR)'); ax.set_ylabel('Predicted Price (M IDR)')\n",
    "ax.set_title(f'Random Forest: Actual vs Predicted\\n(RÂ²={r2_score(y_te_h, y_pred_best_h):.4f})')\n",
    "ax.legend()\n",
    "\n",
    "# Feature importance\n",
    "ax = axes[1]\n",
    "fi = pd.Series(best_reg.feature_importances_, index=feature_cols_h)\n",
    "fi.sort_values(ascending=True).tail(8).plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title('Top Feature Importances â€” Random Forest')\n",
    "ax.set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-project2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contoh Proyek: Segmentasi Pelanggan\n",
    "\n",
    "**Masalah**: Sebuah retailer ingin memahami kelompok-kelompok pelanggan berdasarkan\n",
    "pola pembelian mereka untuk strategi pemasaran yang lebih tepat sasaran.\n",
    "\n",
    "**Pendekatan**: RFM Analysis + K-Means Clustering."
   ],
   "id": "cell-md-project3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Generate synthetic customer transaction dataset ---\n",
    "def generate_customer_data(n_customers=500, n_transactions=5000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    customer_ids = [f'C{i:04d}' for i in range(1, n_customers + 1)]\n",
    "\n",
    "    # Assign customer segments\n",
    "    segments = np.random.choice(['Champion', 'Loyal', 'At-Risk', 'Lost'],\n",
    "                                  n_customers, p=[0.15, 0.30, 0.30, 0.25])\n",
    "    segment_params = {\n",
    "        'Champion': (3, 80, 500),    # (recency_days, frequency, monetary)\n",
    "        'Loyal':    (20, 30, 250),\n",
    "        'At-Risk':  (60, 8, 150),\n",
    "        'Lost':     (180, 2, 80)\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    today = datetime(2024, 12, 31)\n",
    "    for cid, seg in zip(customer_ids, segments):\n",
    "        rec_base, freq_base, mon_base = segment_params[seg]\n",
    "        last_purchase = today - timedelta(days=max(1, int(np.random.exponential(rec_base))))\n",
    "        frequency = max(1, int(np.random.poisson(freq_base)))\n",
    "        monetary  = max(10, np.random.normal(mon_base, mon_base * 0.3)) * frequency\n",
    "        rows.append({'customer_id': cid, 'true_segment': seg,\n",
    "                     'last_purchase_date': last_purchase,\n",
    "                     'frequency': frequency,\n",
    "                     'total_spend': round(monetary, 2)})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_cust = generate_customer_data()\n",
    "\n",
    "# --- RFM Analysis ---\n",
    "analysis_date = datetime(2025, 1, 1)\n",
    "df_cust['recency_days'] = (analysis_date - df_cust['last_purchase_date']).dt.days\n",
    "df_cust['monetary_avg'] = (df_cust['total_spend'] / df_cust['frequency']).round(2)\n",
    "\n",
    "# RFM scoring (1-5, higher = better)\n",
    "df_cust['R_score'] = pd.qcut(df_cust['recency_days'], q=5, labels=[5,4,3,2,1]).astype(int)\n",
    "df_cust['F_score'] = pd.qcut(df_cust['frequency'].rank(method='first'), q=5, labels=[1,2,3,4,5]).astype(int)\n",
    "df_cust['M_score'] = pd.qcut(df_cust['total_spend'], q=5, labels=[1,2,3,4,5]).astype(int)\n",
    "df_cust['RFM_score'] = df_cust['R_score'] + df_cust['F_score'] + df_cust['M_score']\n",
    "\n",
    "print(f'Customer dataset: {df_cust.shape}')\n",
    "print(df_cust[['customer_id', 'recency_days', 'frequency', 'total_spend',\n",
    "               'R_score', 'F_score', 'M_score', 'RFM_score']].head(8))\n",
    "\n",
    "# --- K-Means Clustering ---\n",
    "rfm_features = df_cust[['recency_days', 'frequency', 'total_spend']].copy()\n",
    "scaler_rfm = MinMaxScaler()\n",
    "rfm_scaled = scaler_rfm.fit_transform(rfm_features)\n",
    "\n",
    "# Elbow method to find optimal K\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "K_range = range(2, 9)\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(rfm_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(rfm_scaled, km.labels_))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Elbow curve\n",
    "ax = axes[0]\n",
    "ax.plot(list(K_range), inertias, 'bo-')\n",
    "ax.set_title('Elbow Method'); ax.set_xlabel('K'); ax.set_ylabel('Inertia')\n",
    "ax.axvline(x=4, color='red', linestyle='--', label='K=4')\n",
    "ax.legend()\n",
    "\n",
    "# Silhouette score\n",
    "ax = axes[1]\n",
    "ax.plot(list(K_range), silhouettes, 'go-')\n",
    "ax.set_title('Silhouette Score'); ax.set_xlabel('K'); ax.set_ylabel('Silhouette Score')\n",
    "\n",
    "# Apply K=4 clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "df_cust['cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Cluster profiling\n",
    "cluster_profile = df_cust.groupby('cluster').agg(\n",
    "    avg_recency=('recency_days', 'mean'),\n",
    "    avg_frequency=('frequency', 'mean'),\n",
    "    avg_spend=('total_spend', 'mean'),\n",
    "    customer_count=('customer_id', 'count')\n",
    ").round(1)\n",
    "\n",
    "# Auto-label clusters\n",
    "cluster_labels = {}\n",
    "for cluster_id, row in cluster_profile.iterrows():\n",
    "    if row['avg_recency'] < 20 and row['avg_frequency'] > 40:\n",
    "        cluster_labels[cluster_id] = 'Champions'\n",
    "    elif row['avg_recency'] < 40 and row['avg_frequency'] > 15:\n",
    "        cluster_labels[cluster_id] = 'Loyal Customers'\n",
    "    elif row['avg_recency'] > 100:\n",
    "        cluster_labels[cluster_id] = 'Lost Customers'\n",
    "    else:\n",
    "        cluster_labels[cluster_id] = 'At-Risk Customers'\n",
    "\n",
    "df_cust['segment_label'] = df_cust['cluster'].map(cluster_labels)\n",
    "cluster_profile['label'] = cluster_profile.index.map(cluster_labels)\n",
    "\n",
    "print('\\n=== Cluster Profiles ===')\n",
    "print(cluster_profile.to_string())\n",
    "\n",
    "# Scatter plot: Recency vs Frequency colored by cluster\n",
    "ax = axes[2]\n",
    "colors_clust = ['gold', 'steelblue', 'tomato', 'seagreen', 'mediumpurple']\n",
    "for cluster_id in df_cust['cluster'].unique():\n",
    "    mask = df_cust['cluster'] == cluster_id\n",
    "    label = cluster_labels.get(cluster_id, f'Cluster {cluster_id}')\n",
    "    ax.scatter(df_cust.loc[mask, 'recency_days'],\n",
    "               df_cust.loc[mask, 'frequency'],\n",
    "               c=colors_clust[cluster_id], s=20, alpha=0.6, label=label)\n",
    "ax.set_title('Customer Segments: Recency vs Frequency')\n",
    "ax.set_xlabel('Recency (days)'); ax.set_ylabel('Frequency (purchases)')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Segment distribution\n",
    "seg_counts = df_cust['segment_label'].value_counts()\n",
    "print('\\n=== Customer Segment Distribution ===')\n",
    "print(seg_counts)\n",
    "\n",
    "# Marketing recommendations\n",
    "print('\\n=== Marketing Recommendations by Segment ===')\n",
    "recs = {\n",
    "    'Champions':        'Reward them! Ask for reviews. Launch referral program.',\n",
    "    'Loyal Customers':  'Offer membership upgrade. Exclusive early access deals.',\n",
    "    'At-Risk Customers':'Send personalized win-back campaign with discount offer.',\n",
    "    'Lost Customers':   'Reactivation campaign. Major discount or free gift.'\n",
    "}\n",
    "for seg, rec in recs.items():\n",
    "    count = seg_counts.get(seg, 0)\n",
    "    print(f'  [{seg}] ({count} customers): {rec}')"
   ],
   "id": "cell-project3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist Proyek Akhir\n",
    "\n",
    "Gunakan checklist berikut untuk memastikan proyek Anda lengkap sebelum dikumpulkan."
   ],
   "id": "cell-md-checklist"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = \"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                   CHECKLIST PROYEK AKHIR                              â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  DOKUMEN & STRUKTUR                                                   â•‘\n",
    "â•‘  [ ] Judul proyek jelas dan deskriptif                                â•‘\n",
    "â•‘  [ ] Nama, NIM, dan informasi mahasiswa lengkap                       â•‘\n",
    "â•‘  [ ] Notebook terstruktur dengan markdown yang jelas                  â•‘\n",
    "â•‘  [ ] Semua cell dapat dijalankan ulang (reproducible)                 â•‘\n",
    "â•‘  [ ] Repository GitHub tersedia dan terorganisir                      â•‘\n",
    "â•‘  [ ] Laporan tertulis (PDF) selesai                                   â•‘\n",
    "â•‘  [ ] Slide presentasi (15-20 halaman) siap                            â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  DATA & EDA                                                           â•‘\n",
    "â•‘  [ ] Sumber data teridentifikasi dan dikutip                          â•‘\n",
    "â•‘  [ ] Ukuran dataset cukup (minimal 500 baris)                         â•‘\n",
    "â•‘  [ ] EDA mencakup: distribusi, korelasi, outlier                      â•‘\n",
    "â•‘  [ ] Minimal 5 visualisasi informatif                                 â•‘\n",
    "â•‘  [ ] Insight dari EDA didokumentasikan                                â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  PREPROCESSING & MODELING                                             â•‘\n",
    "â•‘  [ ] Missing values ditangani                                         â•‘\n",
    "â•‘  [ ] Encoding kategoris dilakukan                                     â•‘\n",
    "â•‘  [ ] Scaling/normalisasi diterapkan                                   â•‘\n",
    "â•‘  [ ] Feature engineering dilakukan (min 1 fitur baru)                 â•‘\n",
    "â•‘  [ ] Minimal 2 algoritma ML dibandingkan                              â•‘\n",
    "â•‘  [ ] Cross-validation diterapkan                                      â•‘\n",
    "â•‘  [ ] Metrik evaluasi sesuai dengan jenis masalah                      â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  HASIL & KESIMPULAN                                                   â•‘\n",
    "â•‘  [ ] Model terbaik dipilih dengan justifikasi                         â•‘\n",
    "â•‘  [ ] Analisis error/misclassification dilakukan                       â•‘\n",
    "â•‘  [ ] Feature importance dianalisis                                    â•‘\n",
    "â•‘  [ ] Business insights dirumuskan (bukan hanya angka metrik)          â•‘\n",
    "â•‘  [ ] Limitasi proyek disebutkan                                       â•‘\n",
    "â•‘  [ ] Saran pengembangan ke depan diberikan                            â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "print(checklist)"
   ],
   "id": "cell-checklist"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubrik Penilaian\n",
    "\n",
    "Tabel rubrik penilaian UAS yang digunakan oleh dosen untuk menilai proyek akhir."
   ],
   "id": "cell-md-rubrik"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubrik_data = {\n",
    "    'Komponen': [\n",
    "        'Business Understanding',\n",
    "        'Data & EDA',\n",
    "        'Data Preprocessing',\n",
    "        'Modeling',\n",
    "        'Evaluasi Model',\n",
    "        'Presentasi',\n",
    "        'Dokumentasi & Kode'\n",
    "    ],\n",
    "    'Bobot (%)': [10, 20, 15, 20, 15, 10, 10],\n",
    "    'A (85â€“100)': [\n",
    "        'Masalah bisnis sangat jelas, hipotesis relevan & terukur',\n",
    "        'EDA sangat menyeluruh, visualisasi informatif & insights bermakna',\n",
    "        'Pipeline lengkap, feature engineering kreatif & berdampak',\n",
    "        'Multiple models, hyperparameter tuning, interpretasi mendalam',\n",
    "        'Metrik tepat, analisis error, business implications jelas',\n",
    "        'Komunikasi sangat jelas, narasi compelling, Q&A dijawab baik',\n",
    "        'Notebook bersih, kode terkomentari, laporan sangat profesional'\n",
    "    ],\n",
    "    'B (70â€“84)': [\n",
    "        'Masalah bisnis cukup jelas, success criteria ada',\n",
    "        'EDA memadai, beberapa visualisasi dan insights',\n",
    "        'Preprocessing standar, sedikit feature engineering',\n",
    "        '2 model dibandingkan, evaluasi dengan metrik standar',\n",
    "        'Metrik standar, confusion matrix, kurva training',\n",
    "        'Presentasi baik, menjelaskan metodologi dengan jelas',\n",
    "        'Notebook cukup bersih, kode mudah dibaca'\n",
    "    ],\n",
    "    'C (55â€“69)': [\n",
    "        'Masalah bisnis kurang fokus atau terlalu umum',\n",
    "        'EDA minimal, hanya statistik dasar',\n",
    "        'Preprocessing minimal, tidak ada feature engineering',\n",
    "        '1 model, evaluasi dasar',\n",
    "        'Hanya akurasi, tanpa analisis lebih dalam',\n",
    "        'Presentasi cukup, sebagian materi dapat dipahami',\n",
    "        'Dokumentasi minimal, beberapa error dalam kode'\n",
    "    ],\n",
    "    'D (<55)': [\n",
    "        'Tidak ada business framing yang jelas',\n",
    "        'Tidak ada EDA atau hanya print(df.head())',\n",
    "        'Data tidak diproses sama sekali',\n",
    "        'Tidak ada pemodelan ML',\n",
    "        'Tidak ada evaluasi yang bermakna',\n",
    "        'Presentasi tidak dapat dipahami',\n",
    "        'Kode tidak berjalan, tidak ada dokumentasi'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_rubrik = pd.DataFrame(rubrik_data)\n",
    "\n",
    "# Display rubrik\n",
    "print('=== RUBRIK PENILAIAN PROYEK AKHIR BIG DATA ANALYTICS ===')\n",
    "print()\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 200)\n",
    "print(df_rubrik.to_string(index=False))\n",
    "\n",
    "total_bobot = df_rubrik['Bobot (%)'].sum()\n",
    "print(f'\\nTotal Bobot: {total_bobot}%')\n",
    "print('\\nNilai Akhir = Î£ (Skor Komponen Ã— Bobot)')\n",
    "print('Konversi: A â‰¥ 85 | B: 70â€“84 | C: 55â€“69 | D: 40â€“54 | E: < 40')"
   ],
   "id": "cell-rubrik"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum (Proyek Akhir)\n",
    "\n",
    "### ğŸ¯ Proyek Akhir â€” Big Data Analytics\n",
    "\n",
    "Kembangkan sebuah proyek Big Data Analytics **end-to-end** yang original dan berkualitas.\n",
    "\n",
    "---\n",
    "\n",
    "### Persyaratan Minimum\n",
    "\n",
    "**Dataset:**\n",
    "- Minimal 1.000 baris data\n",
    "- Minimal 8 fitur (campuran numerik dan kategoris)\n",
    "- Sumber data harus dikutip dengan benar\n",
    "- Dataset harus relevan dengan masalah bisnis/penelitian yang dipilih\n",
    "\n",
    "**Analisis:**\n",
    "- EDA lengkap dengan minimal 8 visualisasi informatif\n",
    "- Minimal 2 teknik preprocessing berbeda\n",
    "- Minimal 1 fitur hasil feature engineering\n",
    "- Minimal 3 algoritma ML yang dibandingkan (+ 1 baseline)\n",
    "- Cross-validation (minimal 5-fold)\n",
    "- Minimal 1 teknik hyperparameter tuning\n",
    "- Evaluasi menggunakan minimal 3 metrik yang sesuai\n",
    "- Interpretasi model (feature importance / SHAP)\n",
    "\n",
    "**Deliverables:**\n",
    "1. `ğŸ““ Jupyter Notebook` â€” kode lengkap, bersih, dan reproducible\n",
    "2. `ğŸ“„ Laporan PDF` â€” 20-50 halaman, format akademik\n",
    "3. `ğŸï¸ Slide Presentasi` â€” 15-20 slide, untuk presentasi 10-15 menit\n",
    "4. `ğŸ“¦ GitHub Repository` â€” notebook, data sample, README.md\n",
    "\n",
    "---\n",
    "\n",
    "### Topik yang Disarankan (pilih salah satu atau usulkan sendiri)\n",
    "\n",
    "1. Analisis Sentimen Review Produk E-Commerce Indonesia\n",
    "2. Prediksi Churn Pelanggan (Telekomunikasi / Perbankan / SaaS)\n",
    "3. Sistem Rekomendasi Film / Musik / Produk\n",
    "4. Deteksi Fraud Transaksi Keuangan\n",
    "5. Prediksi Harga Properti (Jakarta / kota besar lainnya)\n",
    "6. Analisis Tren Media Sosial Indonesia\n",
    "7. Segmentasi Pelanggan Ritel dengan RFM + Clustering\n",
    "8. Prediksi Kualitas Udara / Cuaca\n",
    "9. Klasifikasi Berita Hoaks Bahasa Indonesia\n",
    "10. Topik pilihan sendiri (dengan persetujuan dosen)\n",
    "\n",
    "---\n",
    "\n",
    "### Jadwal Pengumpulan\n",
    "\n",
    "| Deliverable | Deadline |\n",
    "|---|---|\n",
    "| Proposal (1 halaman) | Minggu 9 |\n",
    "| Progress report | Minggu 13 |\n",
    "| Final notebook + laporan | Minggu 16 (hari terakhir kuliah) |\n",
    "| Presentasi UAS | Jadwal UAS |\n",
    "\n",
    "---\n",
    "\n",
    "### Tips Sukses\n",
    "\n",
    "âœ… Pilih topik yang Anda minati â€” antusiasme terlihat dalam hasil  \n",
    "âœ… Mulai lebih awal â€” data cleaning selalu memakan lebih banyak waktu  \n",
    "âœ… Dokumentasikan setiap keputusan â€” *mengapa* Anda memilih pendekatan tertentu  \n",
    "âœ… Fokus pada *business insights*, bukan hanya angka metrik  \n",
    "âœ… Kode yang bersih dan terdokumentasi mencerminkan profesionalisme  \n",
    "\n",
    "**Selamat mengerjakan! Manfaatkan semua yang telah dipelajari selama 16 minggu ini! ğŸš€**"
   ],
   "id": "cell-md-tugas"
  }
 ]
}

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 13: Text Analytics & Natural Language Processing\n",
    "## *Week 13 Lab: Text Analytics & NLP*\n",
    "\n",
    "**Mata Kuliah / Course:** Big Data Analytics  \n",
    "**Topik / Topic:** NLP Pipeline, TF-IDF, Word2Vec, Sentiment Analysis, Topic Modeling, NER  \n",
    "\n",
    "---\n",
    "### Deskripsi\n",
    "Pada praktikum ini kita akan mengimplementasikan pipeline NLP lengkap mulai dari preprocessing\n",
    "teks hingga model NLP lanjutan termasuk:\n",
    "- Text preprocessing (tokenization, stopword removal, stemming, lemmatization)\n",
    "- TF-IDF dengan Scikit-learn\n",
    "- Word2Vec dengan Gensim\n",
    "- Analisis Sentimen (VADER + ML)\n",
    "- Topic Modeling dengan LDA\n",
    "- Named Entity Recognition (NER)"
   ],
   "id": "cell-md-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk gensim scikit-learn --quiet\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('maxent_ne_chunker_tab', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print('All libraries loaded successfully!')"
   ],
   "id": "cell-imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Teks\n",
    "\n",
    "Pipeline preprocessing standar NLP: tokenization → lowercase → remove punctuation\n",
    "→ stopword removal → stemming & lemmatization."
   ],
   "id": "cell-md-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"The quick brown foxes are JUMPING over the lazy dogs! They've been running all day.\",\n",
    "    \"Machine learning algorithms are transforming how companies analyze Big Data in 2024.\",\n",
    "    \"Natural Language Processing enables computers to understand, interpret, and generate human language.\",\n",
    "    \"Apache Spark's DataFrame API makes it easy to process large-scale datasets efficiently.\"\n",
    "]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer    = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "    # Step 2: Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n",
    "    # Step 3: Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Step 4: Remove stopwords\n",
    "    tokens_no_stop = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    # Step 5: Stemming\n",
    "    stemmed = [stemmer.stem(t) for t in tokens_no_stop]\n",
    "    # Step 6: Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens_no_stop]\n",
    "    return {\n",
    "        'original': text.strip(),\n",
    "        'tokens': tokens,\n",
    "        'no_stopwords': tokens_no_stop,\n",
    "        'stemmed': stemmed,\n",
    "        'lemmatized': lemmatized\n",
    "    }\n",
    "\n",
    "print('=== NLP Preprocessing Pipeline ===')\n",
    "for i, text in enumerate(sample_texts[:2]):\n",
    "    result = preprocess_text(text)\n",
    "    print(f'\\nText {i+1}:')\n",
    "    print(f'  Original:     {sample_texts[i]}')\n",
    "    print(f'  Tokens:       {result[\"tokens\"]}')\n",
    "    print(f'  No stopwords: {result[\"no_stopwords\"]}')\n",
    "    print(f'  Stemmed:      {result[\"stemmed\"]}')\n",
    "    print(f'  Lemmatized:   {result[\"lemmatized\"]}')"
   ],
   "id": "cell-preprocessing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF dengan Scikit-learn\n",
    "\n",
    "TF-IDF mengukur kepentingan kata dalam dokumen relatif terhadap seluruh corpus.\n",
    "Kata yang sering muncul dalam satu dokumen tetapi jarang di dokumen lain mendapat skor tinggi."
   ],
   "id": "cell-md-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded corpus for TF-IDF (12 documents, 4 topics)\n",
    "corpus = [\n",
    "    # Technology\n",
    "    \"Machine learning and artificial intelligence are revolutionizing data science.\",\n",
    "    \"Deep learning neural networks achieve state of the art results in image recognition.\",\n",
    "    \"Python is the most popular programming language for machine learning and data analysis.\",\n",
    "    # Sports\n",
    "    \"The football team won the championship after an incredible season performance.\",\n",
    "    \"Basketball players need exceptional speed agility and coordination to excel on the court.\",\n",
    "    \"Olympic athletes train for years to achieve peak performance in competitive sports.\",\n",
    "    # Health\n",
    "    \"Regular exercise and balanced nutrition are essential for maintaining good health.\",\n",
    "    \"Medical research advances have significantly improved cancer treatment outcomes.\",\n",
    "    \"Mental health awareness is crucial for overall wellbeing and quality of life.\",\n",
    "    # Environment\n",
    "    \"Climate change poses serious threats to biodiversity and global ecosystems.\",\n",
    "    \"Renewable energy sources like solar and wind power reduce carbon emissions significantly.\",\n",
    "    \"Sustainable development requires balancing economic growth with environmental protection.\"\n",
    "]\n",
    "labels = ['tech']*3 + ['sports']*3 + ['health']*3 + ['environment']*3\n",
    "\n",
    "# Build TF-IDF matrix\n",
    "tfidf = TfidfVectorizer(max_features=30, stop_words='english', ngram_range=(1, 2))\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=feature_names,\n",
    "                          index=[f'Doc{i+1}({labels[i][:4]})' for i in range(len(corpus))])\n",
    "print(f'TF-IDF Matrix shape: {X_tfidf.shape}')\n",
    "print('\\nTF-IDF Matrix (first 6 docs, top 15 features):')\n",
    "print(df_tfidf.iloc[:6, :15].round(3).to_string())\n",
    "\n",
    "# Top terms per document\n",
    "print('\\n=== Top 5 Terms per Document ===')\n",
    "for i, doc_label in enumerate(labels[:6]):\n",
    "    top_indices = X_tfidf[i].toarray()[0].argsort()[-5:][::-1]\n",
    "    top_terms = [(feature_names[j], round(X_tfidf[i, j], 3)) for j in top_indices]\n",
    "    print(f'  Doc{i+1} ({doc_label}): {top_terms}')\n",
    "\n",
    "# Visualize TF-IDF heatmap\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.imshow(df_tfidf.iloc[:, :20].values, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='TF-IDF Score')\n",
    "plt.xticks(range(20), df_tfidf.columns[:20], rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(range(len(corpus)), df_tfidf.index, fontsize=8)\n",
    "plt.title('TF-IDF Matrix Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-tfidf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec dengan Gensim\n",
    "\n",
    "Word2Vec mempelajari representasi vektor kata dari konteksnya.\n",
    "Kata-kata dengan makna serupa akan memiliki vektor yang berdekatan dalam ruang embedding."
   ],
   "id": "cell-md-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training corpus for Word2Vec\n",
    "training_sentences = [\n",
    "    ['machine', 'learning', 'artificial', 'intelligence', 'data', 'science'],\n",
    "    ['deep', 'learning', 'neural', 'network', 'training', 'model'],\n",
    "    ['python', 'programming', 'code', 'algorithm', 'software', 'developer'],\n",
    "    ['data', 'analysis', 'statistics', 'visualization', 'pandas', 'numpy'],\n",
    "    ['spark', 'hadoop', 'big', 'data', 'distributed', 'processing', 'cluster'],\n",
    "    ['natural', 'language', 'processing', 'text', 'analysis', 'nlp', 'sentiment'],\n",
    "    ['classification', 'regression', 'clustering', 'model', 'prediction', 'accuracy'],\n",
    "    ['tensorflow', 'keras', 'pytorch', 'deep', 'learning', 'neural', 'network'],\n",
    "    ['feature', 'engineering', 'preprocessing', 'normalization', 'encoding'],\n",
    "    ['random', 'forest', 'gradient', 'boosting', 'ensemble', 'learning', 'xgboost'],\n",
    "    ['word', 'embedding', 'vector', 'representation', 'semantic', 'similarity'],\n",
    "    ['training', 'validation', 'testing', 'overfitting', 'regularization', 'dropout'],\n",
    "    ['database', 'sql', 'nosql', 'mongodb', 'cassandra', 'storage', 'query'],\n",
    "    ['cloud', 'aws', 'azure', 'gcp', 'computing', 'storage', 'scalable'],\n",
    "]\n",
    "\n",
    "# Train Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=training_sentences,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=2,\n",
    "    sg=0,  # CBOW\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f'Vocabulary size: {len(w2v_model.wv)}')\n",
    "print(f'Vector dimension: {w2v_model.vector_size}')\n",
    "\n",
    "# Semantic similarity\n",
    "print('\\n=== Word Similarity ===')\n",
    "test_words = ['learning', 'data', 'neural']\n",
    "for word in test_words:\n",
    "    if word in w2v_model.wv:\n",
    "        similar = w2v_model.wv.most_similar(word, topn=4)\n",
    "        print(f'  Similar to \"{word}\": {[(w, round(s,3)) for w,s in similar]}')\n",
    "\n",
    "# PCA visualization of word vectors\n",
    "words_to_plot = ['machine', 'learning', 'neural', 'network', 'data', 'analysis',\n",
    "                 'python', 'spark', 'hadoop', 'cloud', 'model', 'training']\n",
    "words_in_vocab = [w for w in words_to_plot if w in w2v_model.wv]\n",
    "vectors = np.array([w2v_model.wv[w] for w in words_in_vocab])\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "coords = pca.fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(coords[:, 0], coords[:, 1], s=100, c='steelblue', zorder=2)\n",
    "for i, word in enumerate(words_in_vocab):\n",
    "    plt.annotate(word, (coords[i, 0] + 0.01, coords[i, 1] + 0.01), fontsize=11)\n",
    "plt.title('Word2Vec Embeddings Visualized with PCA')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-word2vec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analisis Sentimen\n",
    "\n",
    "Dua pendekatan: (1) VADER lexicon-based untuk sentimen langsung,\n",
    "(2) TF-IDF + Logistic Regression untuk supervised classification."
   ],
   "id": "cell-md-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample reviews dataset\n",
    "reviews = [\n",
    "    (\"This product is absolutely amazing! Best purchase I've made this year.\", 'positive'),\n",
    "    (\"Terrible quality. Broke after 2 days. Complete waste of money.\", 'negative'),\n",
    "    (\"Decent product, nothing special. Works as expected.\", 'neutral'),\n",
    "    (\"Fantastic customer service and fast delivery. Very happy!\", 'positive'),\n",
    "    (\"Disappointed with the quality. Expected much better for the price.\", 'negative'),\n",
    "    (\"Average product. Does the job but nothing exceptional.\", 'neutral'),\n",
    "    (\"Excellent build quality and great performance. Highly recommended!\", 'positive'),\n",
    "    (\"Worst product ever. Total garbage. Do not buy!\", 'negative'),\n",
    "    (\"It's okay. Not great, not bad. Does what it says on the box.\", 'neutral'),\n",
    "    (\"Love it! Exceeded my expectations. Will buy again.\", 'positive'),\n",
    "    (\"Poor quality materials. Very flimsy and feels cheap.\", 'negative'),\n",
    "    (\"Reasonable product for the price. Good value overall.\", 'positive'),\n",
    "    (\"Not worth the money. Several issues from the start.\", 'negative'),\n",
    "    (\"Pretty good. Works well and looks nice. Happy with the purchase.\", 'positive'),\n",
    "    (\"Mediocre at best. The product exists and that's about all.\", 'neutral'),\n",
    "]\n",
    "\n",
    "texts, labels_sent = zip(*reviews)\n",
    "\n",
    "# --- VADER Lexicon-based Sentiment ---\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "print('=== VADER Lexicon-based Sentiment Analysis ===')\n",
    "vader_results = []\n",
    "for text, true_label in reviews:\n",
    "    scores = sia.polarity_scores(text)\n",
    "    compound = scores['compound']\n",
    "    pred = 'positive' if compound >= 0.05 else ('negative' if compound <= -0.05 else 'neutral')\n",
    "    vader_results.append({'text': text[:50]+'...', 'true': true_label, 'pred': pred,\n",
    "                           'compound': round(compound, 3), 'match': pred == true_label})\n",
    "\n",
    "df_vader = pd.DataFrame(vader_results)\n",
    "print(df_vader[['true', 'pred', 'compound', 'match']].to_string())\n",
    "print(f'\\nVADER Accuracy: {df_vader[\"match\"].mean():.2%}')\n",
    "\n",
    "# --- ML-based: TF-IDF + Logistic Regression ---\n",
    "print('\\n=== ML-based Sentiment (TF-IDF + Logistic Regression) ===')\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(labels_sent)\n",
    "\n",
    "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(\n",
    "    list(texts), y_encoded, list(labels_sent), test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "tfidf_sent = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "X_train_tfidf = tfidf_sent.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf_sent.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "y_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2%}')"
   ],
   "id": "cell-sentiment"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling dengan LDA\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) menemukan topik tersembunyi dalam koleksi dokumen.\n",
    "Setiap dokumen direpresentasikan sebagai campuran topik."
   ],
   "id": "cell-md-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA on the corpus (12 documents, 4 true topics)\n",
    "count_vec = CountVectorizer(stop_words='english', max_features=50, min_df=1)\n",
    "X_counts = count_vec.fit_transform(corpus)\n",
    "count_feature_names = count_vec.get_feature_names_out()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=4, random_state=42, max_iter=50,\n",
    "                                  learning_method='batch')\n",
    "lda.fit(X_counts)\n",
    "\n",
    "print('=== LDA Topic Modeling — Top Words per Topic ===')\n",
    "topic_labels_guess = ['Technology/AI', 'Sports/Fitness', 'Health/Medical', 'Environment/Energy']\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_word_indices = topic.argsort()[:-11:-1]\n",
    "    top_words = [count_feature_names[i] for i in top_word_indices]\n",
    "    print(f'\\nTopic {topic_idx} (est: {topic_labels_guess[topic_idx]}):')\n",
    "    print(f'  Top words: {top_words}')\n",
    "\n",
    "# Document-topic distribution\n",
    "doc_topics = lda.transform(X_counts)\n",
    "print('\\n=== Document-Topic Distribution (probability) ===')\n",
    "df_doc_topics = pd.DataFrame(\n",
    "    doc_topics.round(3),\n",
    "    columns=[f'Topic{i}' for i in range(4)],\n",
    "    index=[f'Doc{i+1}({labels[i][:4]})' for i in range(len(corpus))]\n",
    ")\n",
    "print(df_doc_topics.to_string())\n",
    "\n",
    "# Visualize topic distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "im = ax.imshow(doc_topics, cmap='Blues', aspect='auto')\n",
    "plt.colorbar(im, label='Topic Probability')\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels([f'Topic {i}' for i in range(4)])\n",
    "ax.set_yticks(range(len(corpus)))\n",
    "ax.set_yticklabels([f'Doc{i+1}({labels[i][:4]})' for i in range(len(corpus))], fontsize=8)\n",
    "ax.set_title('LDA: Document-Topic Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-lda"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "NER mengidentifikasi dan mengklasifikasikan entitas bernama dalam teks:\n",
    "Orang (PERSON), Organisasi (ORGANIZATION), Lokasi (GPE/LOC), dll."
   ],
   "id": "cell-md-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_texts = [\n",
    "    \"Elon Musk founded Tesla and SpaceX in California.\",\n",
    "    \"Google and Microsoft are competing in the artificial intelligence market.\",\n",
    "    \"The United Nations conference in New York addressed climate change issues.\",\n",
    "    \"Barack Obama served as the 44th President of the United States from 2009 to 2017.\",\n",
    "    \"Apple's headquarters is located in Cupertino, California, near San Francisco Bay.\"\n",
    "]\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    \"\"\"Extract named entities using NLTK's ne_chunk.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    tree = ne_chunk(pos_tags, binary=False)\n",
    "    entities = {'PERSON': [], 'ORGANIZATION': [], 'GPE': [], 'LOCATION': [], 'OTHER': []}\n",
    "    for subtree in tree:\n",
    "        if isinstance(subtree, Tree):\n",
    "            entity_label = subtree.label()\n",
    "            entity_text  = ' '.join([token for token, _ in subtree.leaves()])\n",
    "            if entity_label in entities:\n",
    "                entities[entity_label].append(entity_text)\n",
    "            else:\n",
    "                entities['OTHER'].append(f'{entity_label}: {entity_text}')\n",
    "    return entities\n",
    "\n",
    "print('=== Named Entity Recognition Results ===')\n",
    "all_entities = []\n",
    "for i, text in enumerate(ner_texts):\n",
    "    entities = extract_named_entities(text)\n",
    "    print(f'\\nText {i+1}: \"{text}\"')\n",
    "    for ent_type, ents in entities.items():\n",
    "        if ents:\n",
    "            print(f'  {ent_type}: {ents}')\n",
    "    all_entities.append(entities)\n",
    "\n",
    "# Aggregate entity counts\n",
    "from collections import defaultdict\n",
    "entity_counts = defaultdict(int)\n",
    "for ents in all_entities:\n",
    "    for ent_type, ent_list in ents.items():\n",
    "        entity_counts[ent_type] += len(ent_list)\n",
    "\n",
    "print('\\n=== Entity Type Distribution ===')\n",
    "for ent_type, count in sorted(entity_counts.items(), key=lambda x: -x[1]):\n",
    "    if count > 0:\n",
    "        print(f'  {ent_type}: {count}')\n",
    "\n",
    "# Bar chart\n",
    "if any(v > 0 for v in entity_counts.values()):\n",
    "    filtered = {k: v for k, v in entity_counts.items() if v > 0}\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(filtered.keys(), filtered.values(), color='steelblue')\n",
    "    plt.title('Named Entity Type Distribution (5 sample texts)')\n",
    "    plt.xlabel('Entity Type'); plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "cell-ner"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Selesaikan tugas-tugas berikut:\n",
    "\n",
    "1. **Tugas 1 — Preprocessing Bahasa Indonesia**: Kumpulkan 10 kalimat dalam Bahasa Indonesia\n",
    "   dari berita online. Implementasikan preprocessing (tokenisasi, stopword removal menggunakan\n",
    "   library `PySastrawi` atau daftar stopword manual). Bandingkan hasil dengan preprocessing\n",
    "   bahasa Inggris.\n",
    "\n",
    "2. **Tugas 2 — TF-IDF vs BoW**: Bandingkan representasi TF-IDF dengan Bag-of-Words (CountVectorizer)\n",
    "   untuk dokumen corpus yang ada. Menggunakan cosine similarity, tentukan dokumen mana yang\n",
    "   paling mirip untuk setiap dokumen dalam corpus. Apakah hasilnya berbeda antara TF-IDF dan BoW?\n",
    "\n",
    "3. **Tugas 3 — Word2Vec Skip-gram**: Latih ulang model Word2Vec dengan arsitektur **Skip-gram**\n",
    "   (`sg=1`) dan bandingkan top-5 kata paling mirip dengan `learning`, `data`, dan `model`\n",
    "   dibandingkan dengan model CBOW. Apa perbedaan yang Anda amati?\n",
    "\n",
    "4. **Tugas 4 — Aspect-based Sentiment**: Buat dataset 20 review produk elektronik dengan\n",
    "   label sentimen (positive/negative). Implementasikan analisis sentimen berbasis aspek sederhana:\n",
    "   identifikasi sentimen terhadap aspek 'harga', 'kualitas', 'pengiriman'. Gunakan pendekatan\n",
    "   keyword-based dan VADER.\n",
    "\n",
    "5. **Tugas 5 — LDA Hyperparameter Tuning**: Eksperimen dengan LDA menggunakan `n_components`\n",
    "   = 2, 3, 4, 5, 6 pada corpus yang ada. Untuk setiap konfigurasi, hitung **perplexity** dan\n",
    "   **log-likelihood** (`lda.perplexity()`, `lda.score()`). Plot hasilnya dan tentukan jumlah\n",
    "   topik optimal. Interpretasikan topik yang ditemukan secara naratif."
   ],
   "id": "cell-md-tugas"
  }
 ]
}

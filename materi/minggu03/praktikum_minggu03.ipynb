{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 3: Apache Spark & PySpark\n",
    "\n",
    "**Mata Kuliah:** Big Data Analitik  \n",
    "**Topik:** Apache Spark — RDD, DataFrame, dan Spark SQL  \n",
    "**Tujuan:** Mahasiswa mampu membuat dan memanipulasi RDD serta DataFrame menggunakan PySpark\n",
    "\n",
    "---\n",
    "\n",
    "*Week 3 Lab: Apache Spark & PySpark*  \n",
    "*Topics covered: SparkSession, RDD operations, DataFrame API, Spark SQL*"
   ],
   "id": "a1b2c3d4e5f60001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ],
   "id": "a1b2c3d4e5f60002"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library & Membuat SparkSession\n",
    "\n",
    "**SparkSession** adalah titik masuk utama untuk semua fungsionalitas Spark. Sejak Spark 2.0, SparkSession menggantikan SparkContext, SQLContext, dan HiveContext."
   ],
   "id": "a1b2c3d4e5f60003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PraktikumMinggu3\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Atur level log agar tidak terlalu verbose\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"SparkSession berhasil dibuat!\")\n",
    "print(f\"Versi Spark : {spark.version}\")\n",
    "print(f\"App Name   : {spark.sparkContext.appName}\")\n",
    "print(f\"Master     : {spark.sparkContext.master}\")"
   ],
   "id": "a1b2c3d4e5f60004"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Membuat RDD (Resilient Distributed Dataset)\n",
    "\n",
    "RDD adalah struktur data fundamental Spark. RDD bersifat:\n",
    "- **Immutable**: Tidak dapat diubah setelah dibuat\n",
    "- **Distributed**: Tersebar di beberapa partisi\n",
    "- **Lazy**: Transformasi tidak langsung dieksekusi\n",
    "- **Fault-tolerant**: Dapat direkonstruksi dari lineage"
   ],
   "id": "a1b2c3d4e5f60005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# --- Membuat RDD dari list Python ---\n",
    "angka = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd_angka = sc.parallelize(angka, numSlices=2)\n",
    "\n",
    "print(\"=== Informasi RDD ===\")\n",
    "print(f\"Jumlah partisi : {rdd_angka.getNumPartitions()}\")\n",
    "\n",
    "# Operasi dasar\n",
    "print(f\"\\ncount()  -> Jumlah elemen : {rdd_angka.count()}\")\n",
    "print(f\"first()  -> Elemen pertama: {rdd_angka.first()}\")\n",
    "print(f\"take(3)  -> 3 elemen awal : {rdd_angka.take(3)}\")\n",
    "print(f\"collect()-> Semua elemen  : {rdd_angka.collect()}\")\n",
    "\n",
    "# Operasi statistik\n",
    "print(f\"\\nsum()  = {rdd_angka.sum()}\")\n",
    "print(f\"min()  = {rdd_angka.min()}\")\n",
    "print(f\"max()  = {rdd_angka.max()}\")\n",
    "print(f\"mean() = {rdd_angka.mean()}\")\n",
    "\n",
    "# Transformasi map & filter\n",
    "rdd_kuadrat = rdd_angka.map(lambda x: x ** 2)\n",
    "print(f\"\\nmap(x**2)                   : {rdd_kuadrat.collect()}\")\n",
    "\n",
    "rdd_genap = rdd_angka.filter(lambda x: x % 2 == 0)\n",
    "print(f\"filter(x % 2 == 0)          : {rdd_genap.collect()}\")\n",
    "\n",
    "# reduce\n",
    "total = rdd_angka.reduce(lambda a, b: a + b)\n",
    "print(f\"\\nreduce(a + b) = {total}\")"
   ],
   "id": "a1b2c3d4e5f60006"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Membuat DataFrame dengan PySpark\n",
    "\n",
    "DataFrame adalah abstraksi data tingkat tinggi dengan skema kolom yang terdefinisi. DataFrame menawarkan performa lebih baik dari RDD berkat **Catalyst Optimizer** dan **Tungsten Execution Engine**."
   ],
   "id": "a1b2c3d4e5f60007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Membuat DataFrame dari list of tuples dengan skema ---\n",
    "data_mahasiswa = [\n",
    "    (1, \"Budi Santoso\",    \"Informatika\",        3.85, 2021),\n",
    "    (2, \"Ani Rahayu\",      \"Sistem Informasi\",   3.50, 2021),\n",
    "    (3, \"Citra Dewi\",      \"Informatika\",        3.92, 2022),\n",
    "    (4, \"Dodi Prakoso\",    \"Sistem Informasi\",   3.20, 2022),\n",
    "    (5, \"Eka Putri\",       \"Informatika\",        3.75, 2021),\n",
    "    (6, \"Fajar Nugroho\",   \"Teknik Komputer\",    3.60, 2023),\n",
    "    (7, \"Gita Lestari\",    \"Informatika\",        3.88, 2022),\n",
    "    (8, \"Hendra Wijaya\",   \"Teknik Komputer\",    3.45, 2023),\n",
    "    (9, \"Indah Permata\",   \"Sistem Informasi\",   3.70, 2021),\n",
    "    (10, \"Joko Susilo\",    \"Teknik Komputer\",    3.55, 2022),\n",
    "]\n",
    "\n",
    "skema = StructType([\n",
    "    StructField(\"id\",      IntegerType(), True),\n",
    "    StructField(\"nama\",    StringType(),  True),\n",
    "    StructField(\"jurusan\", StringType(),  True),\n",
    "    StructField(\"ipk\",     FloatType(),   True),\n",
    "    StructField(\"angkatan\",IntegerType(), True),\n",
    "])\n",
    "\n",
    "df_mhs = spark.createDataFrame(data_mahasiswa, schema=skema)\n",
    "\n",
    "print(\"=== Tampilan DataFrame ===\")\n",
    "df_mhs.show()\n",
    "\n",
    "print(\"\\n=== Skema DataFrame ===\")\n",
    "df_mhs.printSchema()\n",
    "\n",
    "print(\"\\n=== Statistik Deskriptif ===\")\n",
    "df_mhs.describe([\"ipk\"]).show()\n",
    "\n",
    "# Select kolom tertentu\n",
    "print(\"\\n=== Select: nama & IPK ===\")\n",
    "df_mhs.select(\"nama\", \"ipk\").show(5)\n",
    "\n",
    "# Filter\n",
    "print(\"\\n=== Filter: IPK >= 3.75 ===\")\n",
    "df_mhs.filter(F.col(\"ipk\") >= 3.75).show()\n",
    "\n",
    "# GroupBy & Agregasi\n",
    "print(\"\\n=== GroupBy Jurusan: Rata-rata IPK ===\")\n",
    "df_mhs.groupBy(\"jurusan\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"jumlah_mhs\"),\n",
    "        F.round(F.avg(\"ipk\"), 3).alias(\"rata_ipk\"),\n",
    "        F.max(\"ipk\").alias(\"ipk_tertinggi\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"rata_ipk\")) \\\n",
    "    .show()"
   ],
   "id": "a1b2c3d4e5f60008"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spark SQL\n",
    "\n",
    "Spark SQL memungkinkan kita menjalankan kueri SQL standar pada DataFrame yang didaftarkan sebagai **temporary view**."
   ],
   "id": "a1b2c3d4e5f60009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftarkan DataFrame sebagai temporary view\n",
    "df_mhs.createOrReplaceTempView(\"mahasiswa\")\n",
    "\n",
    "print(\"=== Kueri SQL 1: Semua Mahasiswa ===\")\n",
    "spark.sql(\"SELECT * FROM mahasiswa ORDER BY ipk DESC\").show()\n",
    "\n",
    "print(\"\\n=== Kueri SQL 2: Rata-rata IPK per Jurusan ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        jurusan,\n",
    "        COUNT(*) AS jumlah_mahasiswa,\n",
    "        ROUND(AVG(ipk), 3) AS rata_rata_ipk,\n",
    "        MAX(ipk) AS ipk_max,\n",
    "        MIN(ipk) AS ipk_min\n",
    "    FROM mahasiswa\n",
    "    GROUP BY jurusan\n",
    "    ORDER BY rata_rata_ipk DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n=== Kueri SQL 3: Mahasiswa IPK > 3.7 per Angkatan ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        angkatan,\n",
    "        COUNT(*) AS mhs_berprestasi\n",
    "    FROM mahasiswa\n",
    "    WHERE ipk > 3.7\n",
    "    GROUP BY angkatan\n",
    "    ORDER BY angkatan\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n=== Kueri SQL 4: Subquery — Mahasiswa di Atas Rata-rata ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT nama, jurusan, ipk\n",
    "    FROM mahasiswa\n",
    "    WHERE ipk > (SELECT AVG(ipk) FROM mahasiswa)\n",
    "    ORDER BY ipk DESC\n",
    "\"\"\").show()"
   ],
   "id": "a1b2c3d4e5f60010"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformasi & Aksi pada RDD\n",
    "\n",
    "Memahami perbedaan **transformasi** (lazy, menghasilkan RDD baru) dan **aksi** (eager, memicu eksekusi dan mengembalikan nilai)."
   ],
   "id": "a1b2c3d4e5f60011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========== TRANSFORMASI (Lazy) ==========\")\n",
    "\n",
    "kalimat = [\n",
    "    \"big data analytics dengan spark\",\n",
    "    \"apache spark adalah framework cepat\",\n",
    "    \"rdd dataframe dan spark sql\"\n",
    "]\n",
    "rdd_kalimat = sc.parallelize(kalimat)\n",
    "\n",
    "# map: ubah setiap kalimat menjadi uppercase\n",
    "rdd_upper = rdd_kalimat.map(lambda s: s.upper())\n",
    "print(\"\\nmap (uppercase):\")\n",
    "print(rdd_upper.collect())\n",
    "\n",
    "# flatMap: pecah kalimat menjadi kata-kata\n",
    "rdd_kata = rdd_kalimat.flatMap(lambda s: s.split(\" \"))\n",
    "print(\"\\nflatMap (split kata):\")\n",
    "print(rdd_kata.collect())\n",
    "\n",
    "# filter: ambil kata dengan panjang > 4 karakter\n",
    "rdd_kata_panjang = rdd_kata.filter(lambda w: len(w) > 4)\n",
    "print(\"\\nfilter (panjang > 4 karakter):\")\n",
    "print(rdd_kata_panjang.collect())\n",
    "\n",
    "# distinct: hapus duplikat\n",
    "rdd_unik = rdd_kata.distinct()\n",
    "print(\"\\ndistinct (kata unik):\")\n",
    "print(sorted(rdd_unik.collect()))\n",
    "\n",
    "# union: gabungkan dua RDD\n",
    "rdd_a = sc.parallelize([\"spark\", \"hadoop\", \"kafka\"])\n",
    "rdd_b = sc.parallelize([\"kafka\", \"flink\", \"spark\"])\n",
    "rdd_union = rdd_a.union(rdd_b)\n",
    "print(\"\\nunion:\")\n",
    "print(rdd_union.collect())\n",
    "\n",
    "# sortBy: urutkan kata berdasarkan panjang\n",
    "rdd_sort = rdd_kata_panjang.sortBy(lambda w: len(w), ascending=False)\n",
    "print(\"\\nsortBy (terpanjang):\")\n",
    "print(rdd_sort.collect())\n",
    "\n",
    "print(\"\\n========== AKSI (Eager) ==========\")\n",
    "rdd_nums = sc.parallelize([3, 7, 1, 9, 2, 5, 8, 4, 6, 10])\n",
    "\n",
    "print(f\"count()         : {rdd_nums.count()}\")\n",
    "print(f\"collect()       : {rdd_nums.collect()}\")\n",
    "print(f\"first()         : {rdd_nums.first()}\")\n",
    "print(f\"take(4)         : {rdd_nums.take(4)}\")\n",
    "print(f\"top(3)          : {rdd_nums.top(3)}\")\n",
    "print(f\"reduce(max)     : {rdd_nums.reduce(lambda a, b: a if a > b else b)}\")\n",
    "print(f\"sum()           : {rdd_nums.sum()}\")\n",
    "\n",
    "# countByValue\n",
    "rdd_warna = sc.parallelize([\"merah\",\"biru\",\"merah\",\"hijau\",\"biru\",\"merah\"])\n",
    "print(f\"\\ncountByValue()  : {dict(rdd_warna.countByValue())}\")"
   ],
   "id": "a1b2c3d4e5f60012"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Kerjakan soal-soal berikut secara mandiri:\n",
    "\n",
    "**Soal 1 — RDD Dasar:**  \n",
    "Buatlah RDD dari daftar 20 bilangan acak (antara 1–100). Hitung jumlah bilangan prima dalam RDD tersebut menggunakan transformasi `filter`.\n",
    "\n",
    "**Soal 2 — Word Count:**  \n",
    "Implementasikan program **Word Count** klasik menggunakan RDD. Gunakan teks berikut: `\"the quick brown fox jumps over the lazy dog the dog barked at the fox\"`. Tampilkan 5 kata yang paling sering muncul.\n",
    "\n",
    "**Soal 3 — DataFrame Penjualan:**  \n",
    "Buat DataFrame dengan kolom `produk`, `kategori`, `harga`, `jumlah_terjual`. Isi dengan minimal 15 baris data. Lakukan:\n",
    "- Hitung total pendapatan per kategori (harga × jumlah_terjual)\n",
    "- Temukan produk dengan pendapatan tertinggi\n",
    "- Filter produk dengan harga di atas rata-rata\n",
    "\n",
    "**Soal 4 — Spark SQL Lanjutan:**  \n",
    "Dengan DataFrame mahasiswa yang sudah dibuat, tulis kueri Spark SQL untuk:\n",
    "- Menampilkan peringkat IPK mahasiswa per jurusan menggunakan fungsi window `RANK()`\n",
    "- Menampilkan nama mahasiswa dengan IPK tertinggi di setiap angkatan\n",
    "\n",
    "**Soal 5 — Analisis Teks:**  \n",
    "Buat RDD dari file teks (gunakan `sc.parallelize` dengan daftar kalimat minimal 10 baris). Hitung:\n",
    "- Rata-rata panjang kata dalam setiap kalimat\n",
    "- Jumlah kalimat yang mengandung kata \"data\"\n",
    "- Daftar kata unik yang muncul lebih dari satu kali"
   ],
   "id": "a1b2c3d4e5f60013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hentikan SparkSession setelah selesai\n",
    "spark.stop()\n",
    "print(\"SparkSession dihentikan. Praktikum selesai!\")"
   ],
   "id": "a1b2c3d4e5f60014"
  }
 ]
}

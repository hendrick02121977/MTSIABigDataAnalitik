{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 14: Deep Learning untuk Big Data\n",
    "## *Week 14 Lab: Deep Learning for Big Data*\n",
    "\n",
    "**Mata Kuliah / Course:** Big Data Analytics  \n",
    "**Topik / Topic:** MLP, CNN, RNN/LSTM, Transfer Learning, Regularization  \n",
    "\n",
    "---\n",
    "### Deskripsi\n",
    "Praktikum ini mengimplementasikan deep learning menggunakan TensorFlow/Keras:\n",
    "- Neural Network sederhana (MLP) pada dataset MNIST\n",
    "- Convolutional Neural Network (CNN) untuk klasifikasi gambar\n",
    "- RNN/LSTM untuk data sekuensial (time series)\n",
    "- Transfer Learning dengan MobileNetV2\n",
    "- Teknik regularisasi (Dropout, Batch Normalization)\n",
    "\n",
    "**Catatan**: TensorFlow/Keras tersedia secara default di Google Colab."
   ],
   "id": "cell-md-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'Keras version: {keras.__version__}')\n",
    "print(f'GPU available: {len(tf.config.list_physical_devices(\"GPU\")) > 0}')\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "id": "cell-imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Sederhana (MLP)\n",
    "\n",
    "Multilayer Perceptron (MLP) adalah bentuk paling dasar neural network.\n",
    "Kita akan melatihnya pada dataset MNIST (gambar digit tulisan tangan 28×28 piksel, 10 kelas)."
   ],
   "id": "cell-md-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train_flat = X_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "X_test_flat  = X_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "print(f'Train shape: {X_train_flat.shape} | Test shape: {X_test_flat.shape}')\n",
    "print(f'Classes: {np.unique(y_train)}')\n",
    "\n",
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(20):\n",
    "    axes[i//10, i%10].imshow(X_train[i], cmap='gray')\n",
    "    axes[i//10, i%10].set_title(str(y_train[i]))\n",
    "    axes[i//10, i%10].axis('off')\n",
    "plt.suptitle('MNIST Sample Images')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Build MLP model\n",
    "mlp_model = models.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='MLP')\n",
    "\n",
    "mlp_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('\\n=== MLP Model Summary ===')\n",
    "mlp_model.summary()\n",
    "\n",
    "# Train for 5 epochs\n",
    "print('\\n=== Training MLP ===')\n",
    "mlp_history = mlp_model.fit(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = mlp_model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print(f'\\nMLP Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}')"
   ],
   "id": "cell-mlp"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN untuk Klasifikasi Gambar (MNIST)\n",
    "\n",
    "CNN menggunakan operasi konvolusi untuk mengekstrak fitur spasial dari gambar.\n",
    "Biasanya jauh lebih akurat daripada MLP untuk tugas computer vision."
   ],
   "id": "cell-md-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN: (samples, height, width, channels)\n",
    "X_train_cnn = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test_cnn  = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    # Block 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    # Block 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    # Classifier head\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='CNN')\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('=== CNN Model Summary ===')\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f'\\nCNN Test Accuracy: {test_acc_cnn:.4f} | Test Loss: {test_loss_cnn:.4f}')\n",
    "print(f'Improvement over MLP: +{(test_acc_cnn - test_acc)*100:.2f}%')"
   ],
   "id": "cell-cnn"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualisasi Hasil Training\n",
    "\n",
    "Memvisualisasikan kurva training loss/accuracy dan confusion matrix untuk mengevaluasi model."
   ],
   "id": "cell-md-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MLP history\n",
    "ax = axes[0, 0]\n",
    "ax.plot(mlp_history.history['loss'], label='Train Loss')\n",
    "ax.plot(mlp_history.history['val_loss'], label='Val Loss')\n",
    "ax.set_title('MLP: Training & Validation Loss'); ax.set_xlabel('Epoch')\n",
    "ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.plot(mlp_history.history['accuracy'], label='Train Acc')\n",
    "ax.plot(mlp_history.history['val_accuracy'], label='Val Acc')\n",
    "ax.set_title('MLP: Training & Validation Accuracy'); ax.set_xlabel('Epoch')\n",
    "ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "# CNN history\n",
    "ax = axes[1, 0]\n",
    "ax.plot(cnn_history.history['loss'], label='Train Loss')\n",
    "ax.plot(cnn_history.history['val_loss'], label='Val Loss')\n",
    "ax.set_title('CNN: Training & Validation Loss'); ax.set_xlabel('Epoch')\n",
    "ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.plot(cnn_history.history['accuracy'], label='Train Acc')\n",
    "ax.plot(cnn_history.history['val_accuracy'], label='Val Acc')\n",
    "ax.set_title('CNN: Training & Validation Accuracy'); ax.set_xlabel('Epoch')\n",
    "ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for CNN\n",
    "y_pred_cnn = np.argmax(cnn_model.predict(X_test_cnn, verbose=0), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred_cnn)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=range(10), yticklabels=range(10))\n",
    "plt.title(f'CNN Confusion Matrix (Test Acc: {test_acc_cnn:.4f})')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('=== CNN Classification Report ===')\n",
    "print(classification_report(y_test, y_pred_cnn))"
   ],
   "id": "cell-visualization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN/LSTM untuk Data Sekuensial\n",
    "\n",
    "LSTM dapat mempelajari pola jangka panjang dalam data sekuensial.\n",
    "Kita gunakan data time series sinusoidal untuk memprediksi nilai berikutnya."
   ],
   "id": "cell-md-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series: sine wave + noise\n",
    "t = np.linspace(0, 8 * np.pi, 1000)\n",
    "signal = np.sin(t) + 0.5 * np.sin(3 * t) + 0.15 * np.random.randn(len(t))\n",
    "signal = (signal - signal.min()) / (signal.max() - signal.min())  # normalize [0,1]\n",
    "\n",
    "# Create sequences: use last SEQ_LEN steps to predict next value\n",
    "SEQ_LEN = 30\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i + seq_len])\n",
    "        y.append(data[i + seq_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_seq, y_seq = create_sequences(signal, SEQ_LEN)\n",
    "X_seq = X_seq.reshape((-1, SEQ_LEN, 1))  # (samples, timesteps, features)\n",
    "\n",
    "split = int(0.8 * len(X_seq))\n",
    "X_tr, X_te = X_seq[:split], X_seq[split:]\n",
    "y_tr, y_te = y_seq[:split], y_seq[split:]\n",
    "\n",
    "print(f'Training sequences: {X_tr.shape} | Test sequences: {X_te.shape}')\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, 1)),\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "], name='LSTM_TimeSeries')\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "print('\\n=== LSTM Model Summary ===')\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_tr, y_tr, epochs=30, batch_size=32,\n",
    "    validation_split=0.1, callbacks=[early_stop], verbose=1\n",
    ")\n",
    "\n",
    "# Predict and plot\n",
    "y_pred_lstm = lstm_model.predict(X_te, verbose=0).flatten()\n",
    "test_mse = np.mean((y_te - y_pred_lstm) ** 2)\n",
    "print(f'\\nTest MSE: {test_mse:.5f} | Test MAE: {np.mean(np.abs(y_te - y_pred_lstm)):.4f}')\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "x_axis = range(len(y_te))\n",
    "plt.plot(x_axis, y_te, label='Actual', alpha=0.7)\n",
    "plt.plot(x_axis, y_pred_lstm, label='LSTM Prediction', alpha=0.8, linestyle='--')\n",
    "plt.title(f'LSTM Time Series Prediction (MSE={test_mse:.5f})')\n",
    "plt.xlabel('Time Step'); plt.ylabel('Value')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-lstm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transfer Learning (Konsep & Simulasi)\n",
    "\n",
    "Transfer Learning memanfaatkan model yang sudah dilatih pada dataset besar.\n",
    "Kita menggunakan MobileNetV2 pre-trained pada ImageNet dan menambahkan custom head."
   ],
   "id": "cell-md-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "print('=== Transfer Learning dengan MobileNetV2 ===')\n",
    "print('Loading pre-trained MobileNetV2 (ImageNet weights)...')\n",
    "\n",
    "# Load base model (without top classification layers)\n",
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(96, 96, 3)  # smaller input for demo\n",
    ")\n",
    "\n",
    "# Freeze all base layers\n",
    "base_model.trainable = False\n",
    "frozen_count = sum(1 for l in base_model.layers if not l.trainable)\n",
    "print(f'Frozen layers: {frozen_count}/{len(base_model.layers)}')\n",
    "\n",
    "# Add custom classification head (simulating 5-class problem)\n",
    "NUM_CLASSES = 5\n",
    "inputs = keras.Input(shape=(96, 96, 3))\n",
    "x = base_model(inputs, training=False)       # frozen inference\n",
    "x = layers.GlobalAveragePooling2D()(x)        # reduce spatial dimensions\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "tl_model = keras.Model(inputs, outputs, name='TransferLearning_MobileNetV2')\n",
    "tl_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('\\n=== Transfer Learning Model Summary (top layers) ===')\n",
    "# Show only trainable layers\n",
    "print(f'Total params: {tl_model.count_params():,}')\n",
    "trainable_params = sum(np.prod(v.shape) for v in tl_model.trainable_variables)\n",
    "non_trainable   = sum(np.prod(v.shape) for v in tl_model.non_trainable_variables)\n",
    "print(f'Trainable params: {trainable_params:,}')\n",
    "print(f'Non-trainable params (frozen base): {non_trainable:,}')\n",
    "print(f'Training efficiency: only {trainable_params/tl_model.count_params()*100:.1f}% of params trained!')\n",
    "\n",
    "# Demonstrate fine-tuning concept\n",
    "print('\\n=== Fine-tuning Strategy ===')\n",
    "print('Phase 1 (Feature Extraction): Freeze entire base, train only head')\n",
    "print('Phase 2 (Fine-tuning): Unfreeze top N layers of base, train with low LR')\n",
    "print('\\nUnfreezing last 20 layers for fine-tuning demonstration:')\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "trainable_after = sum(np.prod(v.shape) for v in tl_model.trainable_variables)\n",
    "print(f'Trainable params after unfreezing top 20 layers: {trainable_after:,}')\n",
    "\n",
    "# Re-freeze for demonstration (don't train)\n",
    "base_model.trainable = False\n",
    "print('\\n[Demo only — not running full training to save time]')\n",
    "print('In practice, you would call: tl_model.fit(train_ds, epochs=10, ...)')"
   ],
   "id": "cell-transfer-learning"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regularization Techniques\n",
    "\n",
    "Perbandingan model dengan dan tanpa regularisasi untuk melihat dampak terhadap overfitting."
   ],
   "id": "cell-md-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a small subset of MNIST for faster overfitting demonstration\n",
    "X_small = X_train_flat[:3000]\n",
    "y_small = y_train[:3000]\n",
    "\n",
    "def build_model(use_dropout=False, use_batchnorm=False, name='model'):\n",
    "    layers_list = [layers.Input(shape=(784,))]\n",
    "    for units in [512, 256, 128]:\n",
    "        layers_list.append(layers.Dense(units, activation='relu'))\n",
    "        if use_batchnorm:\n",
    "            layers_list.append(layers.BatchNormalization())\n",
    "        if use_dropout:\n",
    "            layers_list.append(layers.Dropout(0.4))\n",
    "    layers_list.append(layers.Dense(10, activation='softmax'))\n",
    "    model = models.Sequential(layers_list, name=name)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "fit_kwargs = dict(epochs=20, batch_size=64, validation_data=(X_test_flat, y_test), verbose=0)\n",
    "\n",
    "# Model 1: No regularization (prone to overfitting)\n",
    "m_plain = build_model(name='No_Regularization')\n",
    "h_plain = m_plain.fit(X_small, y_small, **fit_kwargs)\n",
    "\n",
    "# Model 2: Dropout only\n",
    "m_drop = build_model(use_dropout=True, name='Dropout_0.4')\n",
    "h_drop = m_drop.fit(X_small, y_small, **fit_kwargs)\n",
    "\n",
    "# Model 3: Dropout + BatchNorm\n",
    "m_full = build_model(use_dropout=True, use_batchnorm=True, name='Dropout+BatchNorm')\n",
    "h_full = m_full.fit(X_small, y_small, **fit_kwargs)\n",
    "\n",
    "# Compare training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "models_info = [\n",
    "    (h_plain, 'No Regularization', 'tomato'),\n",
    "    (h_drop,  'Dropout(0.4)', 'steelblue'),\n",
    "    (h_full,  'Dropout+BatchNorm', 'seagreen')\n",
    "]\n",
    "for hist, label, color in models_info:\n",
    "    axes[0].plot(hist.history['accuracy'],     color=color, linestyle='-',  label=f'{label} (train)')\n",
    "    axes[0].plot(hist.history['val_accuracy'], color=color, linestyle='--', label=f'{label} (val)')\n",
    "    axes[1].plot(hist.history['loss'],     color=color, linestyle='-',  label=f'{label} (train)')\n",
    "    axes[1].plot(hist.history['val_loss'], color=color, linestyle='--', label=f'{label} (val)')\n",
    "\n",
    "axes[0].set_title('Accuracy Comparison'); axes[0].set_xlabel('Epoch'); axes[0].legend(fontsize=7)\n",
    "axes[1].set_title('Loss Comparison');     axes[1].set_xlabel('Epoch'); axes[1].legend(fontsize=7)\n",
    "plt.suptitle('Effect of Regularization Techniques on Training vs Validation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "summary_rows = []\n",
    "for (h, label, _), model in zip(models_info, [m_plain, m_drop, m_full]):\n",
    "    train_acc = h.history['accuracy'][-1]\n",
    "    val_acc   = h.history['val_accuracy'][-1]\n",
    "    _, test_acc_val = model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "    overfit_gap = train_acc - val_acc\n",
    "    summary_rows.append({'Model': label, 'Train Acc': round(train_acc, 4),\n",
    "                          'Val Acc': round(val_acc, 4), 'Test Acc': round(test_acc_val, 4),\n",
    "                          'Overfit Gap': round(overfit_gap, 4)})\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "print('\\n=== Regularization Comparison Summary ===')\n",
    "print(df_summary.to_string(index=False))"
   ],
   "id": "cell-regularization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Selesaikan tugas-tugas berikut:\n",
    "\n",
    "1. **Tugas 1 — Arsitektur MLP**: Eksperimen dengan 3 konfigurasi MLP berbeda pada MNIST:\n",
    "   - Shallow: 1 hidden layer (128 neuron)\n",
    "   - Medium: 3 hidden layers (256, 128, 64)\n",
    "   - Deep: 5 hidden layers (512, 256, 128, 64, 32)\n",
    "   \n",
    "   Bandingkan test accuracy, training time, dan overfitting level.\n",
    "   Plot learning curves dalam satu grafik.\n",
    "\n",
    "2. **Tugas 2 — CNN Architecture Experiment**: Modifikasi CNN dengan menambahkan\n",
    "   satu Conv block ekstra (Conv2D + MaxPooling). Bandingkan jumlah parameter,\n",
    "   training time, dan accuracy dengan CNN awal.\n",
    "   Visualisasikan feature maps (output Conv2D layer pertama) untuk 5 gambar test.\n",
    "\n",
    "3. **Tugas 3 — LSTM Multivariate**: Perluas time series prediction menggunakan\n",
    "   dua fitur input (sine wave + cosine wave sebagai multivariate input).\n",
    "   Reshape input menjadi `(samples, SEQ_LEN, 2)`. Bandingkan MSE dengan\n",
    "   model univariate.\n",
    "\n",
    "4. **Tugas 4 — Learning Rate Scheduling**: Implementasikan `ReduceLROnPlateau`\n",
    "   callback pada CNN training. Bandingkan kurva training dengan dan tanpa\n",
    "   learning rate scheduling. Dokumentasikan kapan LR berubah.\n",
    "\n",
    "5. **Tugas 5 — Custom Dataset**: Cari dataset gambar bebas di Kaggle (min. 3 kelas,\n",
    "   min. 500 gambar total). Terapkan Transfer Learning (MobileNetV2 atau EfficientNetB0)\n",
    "   dengan 2 fase training (feature extraction → fine-tuning). Laporkan accuracy,\n",
    "   confusion matrix, dan contoh prediksi benar/salah."
   ],
   "id": "cell-md-tugas"
  }
 ]
}

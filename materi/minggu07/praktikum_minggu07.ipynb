{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum Minggu 7: Penyimpanan Big Data\n",
    "## *Week 7 Lab: Big Data Storage â€“ NoSQL, Cloud Storage & Data Lake*\n",
    "\n",
    "**Mata Kuliah:** Big Data Analytics  \n",
    "**Topik:** NoSQL Simulation, Cloud Storage Concepts, SQLite, BigQuery, Data Lake\n",
    "\n",
    "---\n",
    "### Tujuan Praktikum\n",
    "1. Mensimulasikan operasi NoSQL Document Store dan Key-Value Store\n",
    "2. Memahami perbedaan format file (CSV vs Parquet)\n",
    "3. Menggunakan SQLite untuk analisis data lokal\n",
    "4. Mengenal query BigQuery pada public dataset\n",
    "5. Mensimulasikan arsitektur Data Lake (Bronze/Silver/Gold)"
   ],
   "id": "cell-md-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library tambahan\n",
    "!pip install google-cloud-bigquery google-cloud-storage db-dtypes pyarrow fastparquet --quiet\n",
    "print('âœ… Instalasi selesai')"
   ],
   "id": "cell-code-install"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulasi NoSQL: Document Store dengan Python Dict\n",
    "\n",
    "MongoDB menyimpan data sebagai dokumen JSON. Kita akan mensimulasikan operasi CRUD (Create, Read, Update, Delete) menggunakan dictionary Python."
   ],
   "id": "cell-md-section1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# Simulasi Document Store (seperti MongoDB)\n",
    "# ============================================================\n",
    "\n",
    "class DocumentStore:\n",
    "    \"\"\"Simulasi sederhana MongoDB-like document store.\"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._store = {}  # {id: document}\n",
    "        self._counter = 1\n",
    "\n",
    "    def insert_one(self, document):\n",
    "        \"\"\"INSERT: Menyimpan dokumen baru.\"\"\"\n",
    "        doc = copy.deepcopy(document)\n",
    "        doc_id = f\"id_{self._counter:04d}\"\n",
    "        doc['_id'] = doc_id\n",
    "        doc['_created_at'] = datetime.now().isoformat()\n",
    "        self._store[doc_id] = doc\n",
    "        self._counter += 1\n",
    "        return doc_id\n",
    "\n",
    "    def insert_many(self, documents):\n",
    "        \"\"\"INSERT MANY: Menyimpan banyak dokumen sekaligus.\"\"\"\n",
    "        ids = [self.insert_one(doc) for doc in documents]\n",
    "        return ids\n",
    "\n",
    "    def find_all(self):\n",
    "        \"\"\"FIND ALL: Mengembalikan semua dokumen.\"\"\"\n",
    "        return list(self._store.values())\n",
    "\n",
    "    def find_one(self, doc_id):\n",
    "        \"\"\"FIND ONE: Mencari dokumen berdasarkan ID.\"\"\"\n",
    "        return self._store.get(doc_id, None)\n",
    "\n",
    "    def find_by(self, field, value):\n",
    "        \"\"\"FIND BY FIELD: Query sederhana.\"\"\"\n",
    "        return [doc for doc in self._store.values() if doc.get(field) == value]\n",
    "\n",
    "    def update_one(self, doc_id, updates):\n",
    "        \"\"\"UPDATE: Memperbarui field tertentu.\"\"\"\n",
    "        if doc_id in self._store:\n",
    "            self._store[doc_id].update(updates)\n",
    "            self._store[doc_id]['_updated_at'] = datetime.now().isoformat()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def delete_one(self, doc_id):\n",
    "        \"\"\"DELETE: Menghapus satu dokumen.\"\"\"\n",
    "        return self._store.pop(doc_id, None) is not None\n",
    "\n",
    "    def count(self):\n",
    "        return len(self._store)\n",
    "\n",
    "\n",
    "# Buat collection 'mahasiswa'\n",
    "db_mahasiswa = DocumentStore('mahasiswa')\n",
    "\n",
    "# CREATE - Insert banyak dokumen (perhatikan skema fleksibel!)\n",
    "mahasiswa_data = [\n",
    "    {'nama': 'Budi Santoso', 'nim': '2021001', 'prodi': 'Informatika', 'ipk': 3.75,\n",
    "     'alamat': {'kota': 'Jakarta', 'provinsi': 'DKI Jakarta'},\n",
    "     'mata_kuliah': ['Big Data', 'Machine Learning', 'Statistika']},\n",
    "    {'nama': 'Sari Dewi', 'nim': '2021002', 'prodi': 'Sistem Informasi', 'ipk': 3.91,\n",
    "     'alamat': {'kota': 'Bandung', 'provinsi': 'Jawa Barat'},\n",
    "     'mata_kuliah': ['Big Data', 'Database', 'Jaringan'],\n",
    "     'beasiswa': 'BidikMisi'},  # kolom ekstra! Schema-less\n",
    "    {'nama': 'Andi Wijaya', 'nim': '2021003', 'prodi': 'Informatika', 'ipk': 3.55,\n",
    "     'alamat': {'kota': 'Surabaya', 'provinsi': 'Jawa Timur'},\n",
    "     'mata_kuliah': ['Big Data', 'Algoritma']},\n",
    "    {'nama': 'Maya Putri', 'nim': '2021004', 'prodi': 'Data Science', 'ipk': 3.88,\n",
    "     'alamat': {'kota': 'Yogyakarta', 'provinsi': 'DIY'},\n",
    "     'mata_kuliah': ['Big Data', 'Deep Learning', 'Statistika', 'Visualisasi'],\n",
    "     'publikasi': 2},  # kolom ekstra!\n",
    "]\n",
    "\n",
    "ids = db_mahasiswa.insert_many(mahasiswa_data)\n",
    "print(f'âœ… INSERT: {len(ids)} dokumen berhasil disimpan. IDs: {ids}')\n",
    "\n",
    "# READ - Baca semua\n",
    "print(f'\\nðŸ“– Jumlah dokumen dalam collection: {db_mahasiswa.count()}')\n",
    "\n",
    "# READ - Baca satu\n",
    "doc = db_mahasiswa.find_one('id_0001')\n",
    "print(f'\\nðŸ“„ Dokumen id_0001:')\n",
    "print(json.dumps(doc, indent=2, ensure_ascii=False))\n",
    "\n",
    "# QUERY - Cari berdasarkan prodi\n",
    "informatika = db_mahasiswa.find_by('prodi', 'Informatika')\n",
    "print(f'\\nðŸ” Query: mahasiswa prodi Informatika = {len(informatika)} orang')\n",
    "for m in informatika:\n",
    "    print(f\"   - {m['nama']} (IPK: {m['ipk']})\")\n",
    "\n",
    "# UPDATE\n",
    "updated = db_mahasiswa.update_one('id_0001', {'ipk': 3.82, 'semester': 6})\n",
    "print(f'\\nâœï¸  UPDATE id_0001 berhasil: {updated}')\n",
    "print(f\"   IPK baru: {db_mahasiswa.find_one('id_0001')['ipk']}\")\n",
    "\n",
    "# DELETE\n",
    "deleted = db_mahasiswa.delete_one('id_0003')\n",
    "print(f'\\nðŸ—‘ï¸  DELETE id_0003 berhasil: {deleted}')\n",
    "print(f'   Sisa dokumen: {db_mahasiswa.count()}')\n",
    "\n",
    "print('\\nâœ… Simulasi Document Store selesai!')\n",
    "print('   Keunggulan utama: SKEMA FLEKSIBEL - tiap dokumen bisa memiliki field berbeda')"
   ],
   "id": "cell-code-docstore"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulasi Key-Value Store\n",
    "\n",
    "Redis adalah in-memory key-value store yang sangat cepat. Kita akan mensimulasikan operasi dasarnya, termasuk konsep TTL (Time-To-Live) untuk expiration."
   ],
   "id": "cell-md-section2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# Simulasi Key-Value Store (seperti Redis)\n",
    "# ============================================================\n",
    "\n",
    "class KeyValueStore:\n",
    "    \"\"\"Simulasi Redis-like in-memory key-value store.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._data = {}         # {key: value}\n",
    "        self._expiry = {}       # {key: expire_timestamp}\n",
    "\n",
    "    def _is_expired(self, key):\n",
    "        if key in self._expiry:\n",
    "            if time.time() > self._expiry[key]:\n",
    "                del self._data[key]\n",
    "                del self._expiry[key]\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def set(self, key, value, ex=None):\n",
    "        \"\"\"SET key value [EX seconds]: Simpan nilai.\"\"\"\n",
    "        self._data[key] = value\n",
    "        if ex:\n",
    "            self._expiry[key] = time.time() + ex\n",
    "        return 'OK'\n",
    "\n",
    "    def get(self, key):\n",
    "        \"\"\"GET key: Ambil nilai berdasarkan kunci.\"\"\"\n",
    "        if self._is_expired(key):\n",
    "            return None\n",
    "        return self._data.get(key, None)\n",
    "\n",
    "    def delete(self, *keys):\n",
    "        \"\"\"DEL key [key ...]: Hapus satu atau lebih kunci.\"\"\"\n",
    "        count = 0\n",
    "        for key in keys:\n",
    "            if key in self._data:\n",
    "                del self._data[key]\n",
    "                self._expiry.pop(key, None)\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def exists(self, key):\n",
    "        \"\"\"EXISTS key: Cek apakah kunci ada.\"\"\"\n",
    "        return not self._is_expired(key) and key in self._data\n",
    "\n",
    "    def keys(self, pattern='*'):\n",
    "        \"\"\"KEYS: Tampilkan semua kunci.\"\"\"\n",
    "        import fnmatch\n",
    "        return [k for k in self._data if not self._is_expired(k) and\n",
    "                (pattern == '*' or fnmatch.fnmatch(k, pattern))]\n",
    "\n",
    "    def ttl(self, key):\n",
    "        \"\"\"TTL key: Sisa waktu hidup.\"\"\"\n",
    "        if key not in self._expiry:\n",
    "            return -1  # tidak expire\n",
    "        remaining = self._expiry[key] - time.time()\n",
    "        return max(0, remaining)\n",
    "\n",
    "    def incr(self, key):\n",
    "        \"\"\"INCR key: Increment nilai integer.\"\"\"\n",
    "        val = int(self._data.get(key, 0)) + 1\n",
    "        self._data[key] = str(val)\n",
    "        return val\n",
    "\n",
    "    def hset(self, name, field, value):\n",
    "        \"\"\"HSET: Simpan hash (nested dict).\"\"\"\n",
    "        if name not in self._data:\n",
    "            self._data[name] = {}\n",
    "        self._data[name][field] = value\n",
    "\n",
    "    def hget(self, name, field):\n",
    "        \"\"\"HGET: Ambil field dari hash.\"\"\"\n",
    "        return self._data.get(name, {}).get(field, None)\n",
    "\n",
    "    def hgetall(self, name):\n",
    "        \"\"\"HGETALL: Ambil semua field dari hash.\"\"\"\n",
    "        return self._data.get(name, {})\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Demo Operasi Key-Value Store\n",
    "# ============================================================\n",
    "cache = KeyValueStore()\n",
    "\n",
    "print('=== Simulasi Key-Value Store (Redis-like) ===')\n",
    "\n",
    "# SET / GET\n",
    "print('\\n[1] SET & GET operasi dasar')\n",
    "cache.set('nama_aplikasi', 'BigDataApp v2.0')\n",
    "cache.set('max_connections', '100')\n",
    "cache.set('environment', 'production')\n",
    "print(f\"  GET nama_aplikasi â†’ {cache.get('nama_aplikasi')}\")\n",
    "print(f\"  GET max_connections â†’ {cache.get('max_connections')}\")\n",
    "\n",
    "# Session management dengan TTL\n",
    "print('\\n[2] Session Management dengan TTL (expiry)')\n",
    "cache.set('session:user_budi', 'token_abc123xyz', ex=2)  # expire dalam 2 detik\n",
    "print(f\"  GET session:user_budi (sebelum expire) â†’ {cache.get('session:user_budi')}\")\n",
    "print(f\"  TTL session:user_budi â†’ {cache.ttl('session:user_budi'):.2f} detik\")\n",
    "time.sleep(2.1)\n",
    "print(f\"  GET session:user_budi (setelah 2.1 detik) â†’ {cache.get('session:user_budi')} (None = expired)\")\n",
    "\n",
    "# Counter\n",
    "print('\\n[3] Counter (INCR)')\n",
    "for _ in range(5):\n",
    "    val = cache.incr('halaman_views')\n",
    "print(f\"  Halaman views counter: {cache.get('halaman_views')}\")\n",
    "\n",
    "# Hash - menyimpan objek\n",
    "print('\\n[4] Hash - Menyimpan objek pengguna')\n",
    "cache.hset('user:1001', 'nama', 'Sari Dewi')\n",
    "cache.hset('user:1001', 'email', 'sari@email.com')\n",
    "cache.hset('user:1001', 'role', 'admin')\n",
    "print(f\"  HGETALL user:1001 â†’ {cache.hgetall('user:1001')}\")\n",
    "print(f\"  HGET user:1001 email â†’ {cache.hget('user:1001', 'email')}\")\n",
    "\n",
    "# Keys pattern\n",
    "print('\\n[5] Semua kunci yang ada:')\n",
    "print(f\"  KEYS * â†’ {cache.keys()}\")\n",
    "\n",
    "# DELETE\n",
    "deleted = cache.delete('max_connections', 'environment')\n",
    "print(f'\\n[6] DELETE â†’ {deleted} kunci dihapus')\n",
    "print(f\"  KEYS * (setelah delete) â†’ {cache.keys()}\")\n",
    "\n",
    "print('\\nâœ… Simulasi Key-Value Store selesai!')"
   ],
   "id": "cell-code-kvstore"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bekerja dengan File di Google Colab (Simulasi Cloud Storage)\n",
    "\n",
    "Di cloud (AWS S3, GCS), file disimpan sebagai objek. Kita akan mensimulasikan ini dengan membandingkan format **CSV vs Parquet** dalam hal ukuran file dan kecepatan baca."
   ],
   "id": "cell-md-section3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "# Buat dataset simulasi besar\n",
    "print('Membuat dataset simulasi...')\n",
    "np.random.seed(42)\n",
    "n = 100_000\n",
    "\n",
    "df_sim = pd.DataFrame({\n",
    "    'id': range(1, n + 1),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=n, freq='min'),\n",
    "    'user_id': np.random.randint(1000, 9999, n),\n",
    "    'product_id': np.random.choice(['P001', 'P002', 'P003', 'P004', 'P005'], n),\n",
    "    'category': np.random.choice(['Elektronik', 'Pakaian', 'Makanan', 'Buku', 'Olahraga'], n),\n",
    "    'amount': np.round(np.random.exponential(150_000, n), 2),\n",
    "    'quantity': np.random.randint(1, 10, n),\n",
    "    'discount_pct': np.round(np.random.uniform(0, 30, n), 1),\n",
    "    'is_paid': np.random.choice([True, False], n, p=[0.85, 0.15]),\n",
    "    'region': np.random.choice(['Jawa', 'Sumatera', 'Kalimantan', 'Sulawesi', 'Papua'], n),\n",
    "})\n",
    "print(f'Dataset: {n:,} baris x {len(df_sim.columns)} kolom')\n",
    "\n",
    "# Simpan ke tempdir\n",
    "tmpdir = tempfile.mkdtemp()\n",
    "csv_path = os.path.join(tmpdir, 'transaksi.csv')\n",
    "parquet_path = os.path.join(tmpdir, 'transaksi.parquet')\n",
    "parquet_compressed_path = os.path.join(tmpdir, 'transaksi_snappy.parquet')\n",
    "\n",
    "# Tulis CSV\n",
    "t0 = time.time()\n",
    "df_sim.to_csv(csv_path, index=False)\n",
    "csv_write_time = time.time() - t0\n",
    "\n",
    "# Tulis Parquet (uncompressed)\n",
    "t0 = time.time()\n",
    "df_sim.to_parquet(parquet_path, index=False, compression=None)\n",
    "parquet_write_time = time.time() - t0\n",
    "\n",
    "# Tulis Parquet (Snappy compression)\n",
    "t0 = time.time()\n",
    "df_sim.to_parquet(parquet_compressed_path, index=False, compression='snappy')\n",
    "parquet_snappy_write_time = time.time() - t0\n",
    "\n",
    "# Baca kembali\n",
    "t0 = time.time()\n",
    "_ = pd.read_csv(csv_path)\n",
    "csv_read_time = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "_ = pd.read_parquet(parquet_path)\n",
    "parquet_read_time = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "_ = pd.read_parquet(parquet_compressed_path)\n",
    "parquet_snappy_read_time = time.time() - t0\n",
    "\n",
    "# Bandingkan\n",
    "csv_size = os.path.getsize(csv_path)\n",
    "parquet_size = os.path.getsize(parquet_path)\n",
    "parquet_snappy_size = os.path.getsize(parquet_compressed_path)\n",
    "\n",
    "print('\\n' + '=' * 65)\n",
    "print(f'{\"FORMAT\":<25} {\"UKURAN\":>12} {\"TULIS(s)\":>10} {\"BACA(s)\":>10}')\n",
    "print('=' * 65)\n",
    "print(f'{\"CSV\":<25} {csv_size/1024/1024:>10.2f}MB {csv_write_time:>10.3f}s {csv_read_time:>10.3f}s')\n",
    "print(f'{\"Parquet (uncompressed)\":<25} {parquet_size/1024/1024:>10.2f}MB {parquet_write_time:>10.3f}s {parquet_read_time:>10.3f}s')\n",
    "print(f'{\"Parquet (Snappy)\":<25} {parquet_snappy_size/1024/1024:>10.2f}MB {parquet_snappy_write_time:>10.3f}s {parquet_snappy_read_time:>10.3f}s')\n",
    "print('=' * 65)\n",
    "print(f'Penghematan ukuran Parquet Snappy vs CSV: {(1 - parquet_snappy_size/csv_size)*100:.1f}%')\n",
    "\n",
    "# Demo column pruning (keunggulan Parquet)\n",
    "print('\\n=== Demo Columnar Pruning (baca kolom tertentu saja) ===')\n",
    "t0 = time.time()\n",
    "_ = pd.read_csv(csv_path, usecols=['user_id', 'amount', 'category'])\n",
    "csv_col_time = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "_ = pd.read_parquet(parquet_compressed_path, columns=['user_id', 'amount', 'category'])\n",
    "parquet_col_time = time.time() - t0\n",
    "\n",
    "print(f'CSV (3 kolom)     : {csv_col_time:.3f}s')\n",
    "print(f'Parquet (3 kolom) : {parquet_col_time:.3f}s')\n",
    "print(f'Parquet {(csv_col_time/parquet_col_time):.1f}x lebih cepat untuk column-selective read')"
   ],
   "id": "cell-code-formats"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pengenalan SQLite sebagai Alternatif Lokal\n",
    "\n",
    "SQLite adalah database SQL ringan yang tidak memerlukan server. Cocok untuk prototyping dan simulasi analitik data warehouse lokal."
   ],
   "id": "cell-md-section4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Buat database SQLite\n",
    "db_path = os.path.join(tempfile.mkdtemp(), 'toko_online.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print('=== Membuat Skema Database ===')\n",
    "\n",
    "# Buat tabel\n",
    "cursor.executescript(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS produk (\n",
    "        id_produk   TEXT PRIMARY KEY,\n",
    "        nama        TEXT NOT NULL,\n",
    "        kategori    TEXT,\n",
    "        harga       REAL,\n",
    "        stok        INTEGER DEFAULT 0\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS pelanggan (\n",
    "        id_pelanggan INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        nama         TEXT NOT NULL,\n",
    "        kota         TEXT,\n",
    "        email        TEXT UNIQUE\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS transaksi (\n",
    "        id_transaksi  INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        id_pelanggan  INTEGER,\n",
    "        id_produk     TEXT,\n",
    "        jumlah        INTEGER,\n",
    "        total_harga   REAL,\n",
    "        tanggal       DATE,\n",
    "        FOREIGN KEY (id_pelanggan) REFERENCES pelanggan(id_pelanggan),\n",
    "        FOREIGN KEY (id_produk) REFERENCES produk(id_produk)\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print('âœ… Tabel produk, pelanggan, transaksi berhasil dibuat')\n",
    "\n",
    "# Insert data\n",
    "produk_data = [\n",
    "    ('P001', 'Laptop Asus', 'Elektronik', 8500000, 15),\n",
    "    ('P002', 'Mouse Wireless', 'Elektronik', 250000, 50),\n",
    "    ('P003', 'Buku Python DS', 'Buku', 150000, 30),\n",
    "    ('P004', 'Kaos Polos', 'Pakaian', 75000, 100),\n",
    "    ('P005', 'Headphone BT', 'Elektronik', 450000, 25),\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO produk VALUES (?,?,?,?,?)', produk_data)\n",
    "\n",
    "pelanggan_data = [\n",
    "    ('Budi Santoso', 'Jakarta', 'budi@email.com'),\n",
    "    ('Sari Dewi', 'Bandung', 'sari@email.com'),\n",
    "    ('Andi Wijaya', 'Surabaya', 'andi@email.com'),\n",
    "    ('Maya Putri', 'Yogyakarta', 'maya@email.com'),\n",
    "    ('Rizky Fajar', 'Medan', 'rizky@email.com'),\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO pelanggan (nama, kota, email) VALUES (?,?,?)', pelanggan_data)\n",
    "\n",
    "transaksi_data = [\n",
    "    (1, 'P001', 1, 8500000, '2024-01-15'), (2, 'P002', 2, 500000, '2024-01-15'),\n",
    "    (3, 'P003', 3, 450000, '2024-01-16'), (1, 'P005', 1, 450000, '2024-01-17'),\n",
    "    (4, 'P004', 5, 375000, '2024-01-18'), (2, 'P001', 1, 8500000, '2024-01-18'),\n",
    "    (5, 'P002', 3, 750000, '2024-01-19'), (3, 'P005', 2, 900000, '2024-01-20'),\n",
    "    (4, 'P001', 1, 8500000, '2024-01-20'), (1, 'P003', 5, 750000, '2024-01-21'),\n",
    "]\n",
    "cursor.executemany('INSERT INTO transaksi (id_pelanggan, id_produk, jumlah, total_harga, tanggal) VALUES (?,?,?,?,?)', transaksi_data)\n",
    "conn.commit()\n",
    "print(f'âœ… Data berhasil dimasukkan')\n",
    "\n",
    "# ============================================================\n",
    "# Query Analitik SQL\n",
    "# ============================================================\n",
    "print('\\n=== Query Analitik SQL ===')\n",
    "\n",
    "queries = {\n",
    "    'Total Revenue per Kategori': \"\"\"\n",
    "        SELECT p.kategori, \n",
    "               COUNT(t.id_transaksi) AS jumlah_transaksi,\n",
    "               SUM(t.total_harga) AS total_revenue,\n",
    "               AVG(t.total_harga) AS avg_order_value\n",
    "        FROM transaksi t\n",
    "        JOIN produk p ON t.id_produk = p.id_produk\n",
    "        GROUP BY p.kategori\n",
    "        ORDER BY total_revenue DESC\n",
    "    \"\"\",\n",
    "    'Top Pelanggan berdasarkan Total Belanja': \"\"\"\n",
    "        SELECT pl.nama, pl.kota,\n",
    "               COUNT(t.id_transaksi) AS jumlah_beli,\n",
    "               SUM(t.total_harga) AS total_belanja\n",
    "        FROM transaksi t\n",
    "        JOIN pelanggan pl ON t.id_pelanggan = pl.id_pelanggan\n",
    "        GROUP BY pl.id_pelanggan\n",
    "        ORDER BY total_belanja DESC\n",
    "        LIMIT 3\n",
    "    \"\"\",\n",
    "    'Revenue Harian': \"\"\"\n",
    "        SELECT tanggal,\n",
    "               COUNT(*) AS jumlah_transaksi,\n",
    "               SUM(total_harga) AS daily_revenue\n",
    "        FROM transaksi\n",
    "        GROUP BY tanggal\n",
    "        ORDER BY tanggal\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for title, query in queries.items():\n",
    "    print(f'\\n--- {title} ---')\n",
    "    result = pd.read_sql_query(query, conn)\n",
    "    print(result.to_string(index=False))\n",
    "\n",
    "conn.close()\n",
    "print('\\nâœ… Koneksi SQLite ditutup')"
   ],
   "id": "cell-code-sqlite"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BigQuery Public Dataset - Contoh Query\n",
    "\n",
    "Google BigQuery menyediakan banyak **public dataset** yang bisa diquery langsung. Berikut adalah contoh query SQL yang bisa dijalankan di BigQuery Console (https://console.cloud.google.com/bigquery)."
   ],
   "id": "cell-md-section5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Contoh Query BigQuery pada Public Datasets\n",
    "# CATATAN: Query ini ditampilkan sebagai string/referensi.\n",
    "# Untuk menjalankannya, Anda perlu autentikasi Google Cloud.\n",
    "# Bisa langsung dijalankan di: https://console.cloud.google.com/bigquery\n",
    "# ============================================================\n",
    "\n",
    "bigquery_queries = {\n",
    "    'Query 1: Jumlah Kelahiran per Tahun (USA)': {\n",
    "        'dataset': 'bigquery-public-data.samples.natality',\n",
    "        'sql': \"\"\"\n",
    "SELECT\n",
    "  year,\n",
    "  COUNT(*) AS total_births,\n",
    "  AVG(weight_pounds) AS avg_birth_weight_lbs\n",
    "FROM `bigquery-public-data.samples.natality`\n",
    "WHERE year BETWEEN 2000 AND 2008\n",
    "GROUP BY year\n",
    "ORDER BY year;\n",
    "\"\"\",\n",
    "        'penjelasan': 'Menghitung total kelahiran dan rata-rata berat bayi per tahun'\n",
    "    },\n",
    "    'Query 2: Top 10 GitHub Repo Paling Aktif': {\n",
    "        'dataset': 'bigquery-public-data.github_repos.commits',\n",
    "        'sql': \"\"\"\n",
    "SELECT\n",
    "  repo_name,\n",
    "  COUNT(*) AS commit_count,\n",
    "  COUNT(DISTINCT author.email) AS unique_contributors\n",
    "FROM `bigquery-public-data.github_repos.commits`,\n",
    "     UNNEST(repo_name) AS repo_name\n",
    "GROUP BY repo_name\n",
    "ORDER BY commit_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\",\n",
    "        'penjelasan': 'Menemukan repositori GitHub dengan commit terbanyak'\n",
    "    },\n",
    "    'Query 3: Analisis Cuaca Chicago': {\n",
    "        'dataset': 'bigquery-public-data.chicago_taxi_trips.taxi_trips',\n",
    "        'sql': \"\"\"\n",
    "SELECT\n",
    "  EXTRACT(HOUR FROM trip_start_timestamp) AS jam,\n",
    "  COUNT(*) AS jumlah_perjalanan,\n",
    "  AVG(fare) AS rata_rata_tarif,\n",
    "  AVG(trip_miles) AS rata_rata_jarak_mil\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE EXTRACT(YEAR FROM trip_start_timestamp) = 2022\n",
    "  AND fare IS NOT NULL\n",
    "  AND trip_miles > 0\n",
    "GROUP BY jam\n",
    "ORDER BY jam;\n",
    "\"\"\",\n",
    "        'penjelasan': 'Pola perjalanan taksi Chicago berdasarkan jam dalam sehari'\n",
    "    },\n",
    "    'Query 4: Window Function - Running Total': {\n",
    "        'dataset': 'bigquery-public-data.samples.shakespeare',\n",
    "        'sql': \"\"\"\n",
    "SELECT\n",
    "  corpus,\n",
    "  word_count,\n",
    "  SUM(word_count) OVER (\n",
    "    ORDER BY word_count DESC\n",
    "    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "  ) AS running_total,\n",
    "  RANK() OVER (ORDER BY word_count DESC) AS rank_by_words\n",
    "FROM (\n",
    "  SELECT corpus, SUM(word_count) AS word_count\n",
    "  FROM `bigquery-public-data.samples.shakespeare`\n",
    "  GROUP BY corpus\n",
    ")\n",
    "ORDER BY word_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\",\n",
    "        'penjelasan': 'Menggunakan Window Function untuk menghitung running total dan rank karya Shakespeare'\n",
    "    }\n",
    "}\n",
    "\n",
    "print('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')\n",
    "print('â•‘         CONTOH QUERY GOOGLE BIGQUERY PUBLIC DATASETS        â•‘')\n",
    "print('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')\n",
    "print('URL: https://console.cloud.google.com/bigquery')\n",
    "print()\n",
    "\n",
    "for title, qdata in bigquery_queries.items():\n",
    "    print(f'\\n{'='*65}')\n",
    "    print(f'ðŸ“Š {title}')\n",
    "    print(f'{'='*65}')\n",
    "    print(f'ðŸ“ Dataset: {qdata[\"dataset\"]}')\n",
    "    print(f'ðŸ’¡ Tujuan : {qdata[\"penjelasan\"]}')\n",
    "    print(f'\\nðŸ”· SQL Query:')\n",
    "    print(qdata['sql'])\n",
    "\n",
    "# Simulasi hasil query dengan data lokal\n",
    "print('\\n=== Simulasi Hasil Query 3 (Pola Taksi per Jam) ===')\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "sim_hours = list(range(24))\n",
    "sim_trips = [int(x) for x in np.random.poisson([500 if h in range(7,9) or h in range(17,20)\n",
    "                                                  else 200 if h > 6 else 50 for h in sim_hours])]\n",
    "sim_fare = [round(np.random.uniform(12, 25) + (2 if h in range(7,9) or h in range(17,20) else 0), 2)\n",
    "            for h in sim_hours]\n",
    "\n",
    "sim_result = pd.DataFrame({'jam': sim_hours, 'jumlah_perjalanan': sim_trips, 'rata_rata_tarif_usd': sim_fare})\n",
    "print(sim_result.to_string(index=False))"
   ],
   "id": "cell-code-bigquery"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Lake Simulation\n",
    "\n",
    "Implementasi struktur folder Data Lake dengan tiga zona: **Bronze** (raw), **Silver** (processed), dan **Gold** (curated/aggregated)."
   ],
   "id": "cell-md-section6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ============================================================\n",
    "# Buat struktur Data Lake\n",
    "# ============================================================\n",
    "lake_root = Path(tempfile.mkdtemp()) / 'data_lake'\n",
    "\n",
    "zones = {\n",
    "    'bronze': lake_root / 'bronze' / 'transaksi' / '2024' / '01',\n",
    "    'silver': lake_root / 'silver' / 'transaksi',\n",
    "    'gold':   lake_root / 'gold'   / 'analytics',\n",
    "    'meta':   lake_root / '_metadata',\n",
    "}\n",
    "\n",
    "for zone_path in zones.values():\n",
    "    zone_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('=== Struktur Data Lake Dibuat ===')\n",
    "for zone, path in zones.items():\n",
    "    print(f'  [{zone.upper():8s}] {path}')\n",
    "\n",
    "# ============================================================\n",
    "# BRONZE ZONE: Data mentah as-is\n",
    "# ============================================================\n",
    "print('\\n--- Menulis ke BRONZE ZONE (raw data) ---')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulasi data mentah dengan kualitas buruk\n",
    "raw_data = pd.DataFrame({\n",
    "    'TGL_TRANSAKSI': ['15/01/2024', '15/01/2024', '16-01-2024', '17/01/2024',  # format tidak konsisten\n",
    "                      '18/01/2024', '18/01/2024', '19/01/2024', '20/01/2024'],\n",
    "    'ID_PRODUK': ['P001', 'P002', 'P003', None, 'P005', 'P001', 'P002', 'P005'],  # ada null\n",
    "    'HARGA': ['8500000', '250000', '150000', '75000', '450000', '8500000', '250000', '450000'],  # string\n",
    "    'QTY': [1, 2, 3, 5, 1, 1, 3, 2],\n",
    "    'KOTA_PEMBELI': ['Jakarta', 'BANDUNG', 'surabaya', 'Yogyakarta', 'Medan',  # casing tidak konsisten\n",
    "                     'Jakarta', 'Bandung', 'Surabaya'],\n",
    "    'DISKON': ['0%', '5%', '10%', '0%', '15%', '0%', '5%', '10%'],  # ada simbol %\n",
    "})\n",
    "\n",
    "bronze_path = zones['bronze'] / 'raw_transaksi_20240115.csv'\n",
    "raw_data.to_csv(bronze_path, index=False)\n",
    "print(f'âœ… Bronze: {bronze_path.name} ({bronze_path.stat().st_size} bytes)')\n",
    "print('   Preview data mentah (kualitas buruk):')\n",
    "print(raw_data.head(4).to_string(index=False))\n",
    "\n",
    "# Simpan metadata\n",
    "meta = {\n",
    "    'source': 'sistem_pos_restoran',\n",
    "    'ingested_at': datetime.now().isoformat(),\n",
    "    'rows': len(raw_data),\n",
    "    'format': 'csv',\n",
    "    'zone': 'bronze'\n",
    "}\n",
    "with open(zones['meta'] / 'transaksi_meta.json', 'w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "# ============================================================\n",
    "# SILVER ZONE: Cleaning & Standardization\n",
    "# ============================================================\n",
    "print('\\n--- Proses ke SILVER ZONE (cleaned data) ---')\n",
    "\n",
    "silver_df = raw_data.copy()\n",
    "\n",
    "# Cleaning steps\n",
    "silver_df['ID_PRODUK'] = silver_df['ID_PRODUK'].fillna('UNKNOWN')\n",
    "silver_df['HARGA'] = silver_df['HARGA'].astype(float)\n",
    "silver_df['DISKON_PCT'] = silver_df['DISKON'].str.replace('%', '').astype(float)\n",
    "silver_df['KOTA_PEMBELI'] = silver_df['KOTA_PEMBELI'].str.title()\n",
    "silver_df['TGL_TRANSAKSI'] = pd.to_datetime(\n",
    "    silver_df['TGL_TRANSAKSI'].str.replace('-', '/'), format='%d/%m/%Y'\n",
    ")\n",
    "silver_df['TOTAL_HARGA'] = silver_df['HARGA'] * silver_df['QTY'] * (1 - silver_df['DISKON_PCT'] / 100)\n",
    "silver_df = silver_df.drop(columns=['DISKON'])\n",
    "\n",
    "silver_path = zones['silver'] / 'transaksi_clean_202401.parquet'\n",
    "silver_df.to_parquet(silver_path, index=False)\n",
    "print(f'âœ… Silver: {silver_path.name} ({silver_path.stat().st_size} bytes)')\n",
    "print('   Preview data yang sudah dibersihkan:')\n",
    "print(silver_df.head(4).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# GOLD ZONE: Aggregated / BI-ready data\n",
    "# ============================================================\n",
    "print('\\n--- Proses ke GOLD ZONE (aggregated/BI-ready) ---')\n",
    "\n",
    "gold_df = silver_df.groupby('ID_PRODUK').agg(\n",
    "    total_transaksi=('QTY', 'count'),\n",
    "    total_qty=('QTY', 'sum'),\n",
    "    total_revenue=('TOTAL_HARGA', 'sum'),\n",
    "    avg_diskon=('DISKON_PCT', 'mean')\n",
    ").reset_index().rename(columns={'ID_PRODUK': 'produk_id'})\n",
    "gold_df['revenue_share_pct'] = (gold_df['total_revenue'] / gold_df['total_revenue'].sum() * 100).round(1)\n",
    "\n",
    "gold_path = zones['gold'] / 'revenue_per_produk_202401.parquet'\n",
    "gold_df.to_parquet(gold_path, index=False)\n",
    "print(f'âœ… Gold: {gold_path.name} ({gold_path.stat().st_size} bytes)')\n",
    "print('   Tabel agregasi siap untuk BI/Dashboard:')\n",
    "print(gold_df.to_string(index=False))\n",
    "\n",
    "# Tampilkan struktur akhir\n",
    "print('\\n=== Struktur Lengkap Data Lake ===')\n",
    "for path in sorted(lake_root.rglob('*')):\n",
    "    if path.is_file():\n",
    "        rel = path.relative_to(lake_root)\n",
    "        size = path.stat().st_size\n",
    "        print(f'  {str(rel):<60} ({size:>6} bytes)')\n",
    "\n",
    "print('\\nâœ… Simulasi Data Lake selesai!')"
   ],
   "id": "cell-code-datalake"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "### Soal 1\n",
    "Perluas simulasi Document Store (`DocumentStore`) dengan menambahkan:\n",
    "- Method `find_by_range(field, min_val, max_val)` untuk mencari dokumen dengan nilai dalam rentang tertentu\n",
    "- Gunakan method ini untuk mencari mahasiswa dengan IPK antara 3.7 dan 4.0\n",
    "- Tambahkan minimal 5 dokumen mahasiswa baru dan tampilkan hasil query\n",
    "\n",
    "### Soal 2\n",
    "Tambahkan fungsionalitas pada `KeyValueStore`:\n",
    "- Method `lpush(key, *values)` dan `lrange(key, start, end)` untuk mensimulasikan List Redis\n",
    "- Simulasikan antrian (queue) tugas dengan operasi push dan pop\n",
    "- Contoh: antrian email notifikasi yang akan dikirim\n",
    "\n",
    "### Soal 3\n",
    "Lakukan perbandingan format file yang lebih komprehensif:\n",
    "- Buat dataset dengan 500.000 baris\n",
    "- Simpan dalam format: CSV, Parquet (gzip), Parquet (snappy), dan JSON\n",
    "- Bandingkan ukuran file dan waktu baca/tulis dalam tabel\n",
    "- Manakah format terbaik untuk analitik dan mengapa?\n",
    "\n",
    "### Soal 4\n",
    "Perluas simulasi Data Lake:\n",
    "- Tambahkan zona Bronze untuk data dari sumber kedua (misalnya data produk)\n",
    "- Lakukan JOIN antara data transaksi (Silver) dan data produk di Silver Zone\n",
    "- Buat tabel Gold baru: `revenue_per_kategori` dengan kolom kategori, total_revenue, jumlah_transaksi\n",
    "- Simpan hasilnya di Gold Zone dalam format Parquet"
   ],
   "id": "cell-md-tasks"
  }
 ]
}

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minggu 9: Machine Learning untuk Big Data\n",
    "## Week 9: Machine Learning for Big Data\n",
    "\n",
    "**Mata Kuliah / Course:** Big Data Analytics  \n",
    "**Topik / Topic:** Supervised & Unsupervised Machine Learning  \n",
    "\n",
    "---\n",
    "\n",
    "### Deskripsi\n",
    "Praktikum ini membahas penerapan algoritma Machine Learning menggunakan scikit-learn, mencakup:\n",
    "- Regresi Linear\n",
    "- Klasifikasi (Decision Tree, Random Forest, Logistic Regression)\n",
    "- Clustering (K-Means)\n",
    "- Evaluasi dan perbandingan model"
   ],
   "id": "cell-md-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Import Libraries\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn - Datasets\n",
    "from sklearn.datasets import load_iris, make_regression, make_classification\n",
    "\n",
    "# Sklearn - Preprocessing & Model Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score,\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Sklearn - Algorithms\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print('Libraries loaded successfully!')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "print(f'Pandas: {pd.__version__}')\n",
    "print(f'Matplotlib: {matplotlib.__version__}')\n",
    "print(f'Seaborn: {sns.__version__}')"
   ],
   "id": "cell-imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset: Iris & Synthetic Data\n",
    "\n",
    "Kita akan menggunakan dua dataset:\n",
    "1. **Dataset Iris** – dataset klasifikasi klasik (150 sampel, 4 fitur, 3 kelas)\n",
    "2. **Synthetic Regression Data** – data sintetis untuk demonstrasi regresi"
   ],
   "id": "cell-md-dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Iris Dataset\n",
    "# ============================================================\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_iris = pd.Series(iris.target, name='species')\n",
    "species_names = iris.target_names\n",
    "\n",
    "print('=== Iris Dataset ===')\n",
    "print(f'Shape: {X_iris.shape}')\n",
    "print(f'Features: {list(X_iris.columns)}')\n",
    "print(f'Classes: {list(species_names)}')\n",
    "print(f'Class distribution:\\n{y_iris.value_counts().rename(dict(enumerate(species_names)))}')\n",
    "print()\n",
    "print(X_iris.head())\n",
    "\n",
    "# ============================================================\n",
    "# Generate Synthetic Regression Data\n",
    "# ============================================================\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=300, n_features=1, noise=20, random_state=42\n",
    ")\n",
    "print('\\n=== Synthetic Regression Data ===')\n",
    "print(f'X shape: {X_reg.shape}, y shape: {y_reg.shape}')\n",
    "\n",
    "# Visualize datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Regression data scatter\n",
    "axes[0].scatter(X_reg, y_reg, alpha=0.6, color='steelblue', edgecolors='white', s=60)\n",
    "axes[0].set_title('Synthetic Regression Data', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature X')\n",
    "axes[0].set_ylabel('Target y')\n",
    "\n",
    "# Iris pairplot (sepal features)\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a']\n",
    "for i, (cls, color) in enumerate(zip(species_names, colors)):\n",
    "    mask = y_iris == i\n",
    "    axes[1].scatter(\n",
    "        X_iris.loc[mask, 'sepal length (cm)'],\n",
    "        X_iris.loc[mask, 'sepal width (cm)'],\n",
    "        label=cls, alpha=0.7, color=color, s=60\n",
    "    )\n",
    "axes[1].set_title('Iris Dataset (Sepal Features)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Sepal Length (cm)')\n",
    "axes[1].set_ylabel('Sepal Width (cm)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-load-data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regresi Linear\n",
    "\n",
    "**Linear Regression** memodelkan hubungan antara fitur input dan target kontinu sebagai fungsi linear:  \n",
    "`ŷ = β₀ + β₁x`\n",
    "\n",
    "Model dilatih dengan meminimalkan **Mean Squared Error (MSE)**."
   ],
   "id": "cell-md-regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Linear Regression\n",
    "# ============================================================\n",
    "\n",
    "# Train-test split\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f'Training samples: {len(X_train_r)}, Test samples: {len(X_test_r)}')\n",
    "\n",
    "# Fit model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = lr_model.predict(X_train_r)\n",
    "y_pred_test  = lr_model.predict(X_test_r)\n",
    "\n",
    "# Metrics\n",
    "train_mse  = mean_squared_error(y_train_r, y_pred_train)\n",
    "test_mse   = mean_squared_error(y_test_r, y_pred_test)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse  = np.sqrt(test_mse)\n",
    "train_r2   = r2_score(y_train_r, y_pred_train)\n",
    "test_r2    = r2_score(y_test_r, y_pred_test)\n",
    "\n",
    "print('\\n=== Linear Regression Results ===')\n",
    "print(f'Intercept (β₀): {lr_model.intercept_:.4f}')\n",
    "print(f'Coefficient (β₁): {lr_model.coef_[0]:.4f}')\n",
    "print(f'\\n{\"Metric\":<12} {\"Train\":>10} {\"Test\":>10}')\n",
    "print('-' * 34)\n",
    "print(f'{\"MSE\":<12} {train_mse:>10.2f} {test_mse:>10.2f}')\n",
    "print(f'{\"RMSE\":<12} {train_rmse:>10.2f} {test_rmse:>10.2f}')\n",
    "print(f'{\"R²\":<12} {train_r2:>10.4f} {test_r2:>10.4f}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter + regression line\n",
    "x_line = np.linspace(X_reg.min(), X_reg.max(), 200).reshape(-1, 1)\n",
    "y_line = lr_model.predict(x_line)\n",
    "\n",
    "axes[0].scatter(X_train_r, y_train_r, alpha=0.5, label='Train', color='steelblue', s=40)\n",
    "axes[0].scatter(X_test_r,  y_test_r,  alpha=0.7, label='Test',  color='orange',   s=60, marker='D')\n",
    "axes[0].plot(x_line, y_line, color='red', linewidth=2.5, label='Regression Line')\n",
    "axes[0].set_title('Linear Regression Fit', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature X')\n",
    "axes[0].set_ylabel('Target y')\n",
    "axes[0].legend()\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test_r - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.7, color='purple', s=50)\n",
    "axes[1].axhline(y=0, color='red', linewidth=2, linestyle='--')\n",
    "axes[1].set_title('Residuals Plot (Test Set)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-linear-regression"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Klasifikasi dengan Decision Tree\n",
    "\n",
    "**Decision Tree** membagi data secara rekursif berdasarkan fitur yang paling informatif.  \n",
    "Mudah diinterpretasi — kita bisa memvisualisasikan seluruh logika keputusan."
   ],
   "id": "cell-md-dt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Decision Tree Classifier\n",
    "# ============================================================\n",
    "\n",
    "# Split data\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_iris.values, y_iris.values, test_size=0.2, random_state=42, stratify=y_iris.values\n",
    ")\n",
    "\n",
    "# Train Decision Tree\n",
    "dt_clf = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dt_clf.fit(X_train_c, y_train_c)\n",
    "y_pred_dt = dt_clf.predict(X_test_c)\n",
    "\n",
    "# Metrics\n",
    "print('=== Decision Tree Classifier ===')\n",
    "print(f'Accuracy: {accuracy_score(y_test_c, y_pred_dt):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test_c, y_pred_dt, target_names=species_names))\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_c, y_pred_dt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_names)\n",
    "disp.plot(ax=axes[0], colorbar=False, cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix – Decision Tree', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Decision Tree visualization\n",
    "plot_tree(\n",
    "    dt_clf,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=species_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    ax=axes[1],\n",
    "    fontsize=8\n",
    ")\n",
    "axes[1].set_title('Decision Tree Structure (max_depth=4)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-decision-tree"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Klasifikasi dengan Random Forest\n",
    "\n",
    "**Random Forest** adalah ensemble dari banyak decision tree yang dilatih pada bootstrap samples.  \n",
    "Keunggulan utama: **Feature Importance** — menunjukkan seberapa penting setiap fitur."
   ],
   "id": "cell-md-rf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Random Forest Classifier\n",
    "# ============================================================\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_clf.fit(X_train_c, y_train_c)\n",
    "y_pred_rf = rf_clf.predict(X_test_c)\n",
    "\n",
    "print('=== Random Forest Classifier ===')\n",
    "print(f'Accuracy: {accuracy_score(y_test_c, y_pred_rf):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test_c, y_pred_rf, target_names=species_names))\n",
    "\n",
    "# Comparison: DT vs RF\n",
    "print('\\n=== Comparison: Decision Tree vs Random Forest ===')\n",
    "print(f'{\"\":<20} {\"Decision Tree\":>15} {\"Random Forest\":>15}')\n",
    "print('-' * 52)\n",
    "print(f'{\"Accuracy\":<20} {accuracy_score(y_test_c, y_pred_dt):>15.4f} {accuracy_score(y_test_c, y_pred_rf):>15.4f}')\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Feature importance\n",
    "importances = rf_clf.feature_importances_\n",
    "feature_names = [name.replace(' (cm)', '') for name in iris.feature_names]\n",
    "sorted_idx = np.argsort(importances)\n",
    "\n",
    "colors_imp = ['#2196F3' if i != sorted_idx[-1] else '#FF5722' for i in range(len(importances))]\n",
    "bars = axes[0].barh(\n",
    "    [feature_names[i] for i in sorted_idx],\n",
    "    importances[sorted_idx],\n",
    "    color=[colors_imp[i] for i in sorted_idx]\n",
    ")\n",
    "axes[0].set_title('Random Forest – Feature Importance', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Importance Score')\n",
    "for bar, val in zip(bars, importances[sorted_idx]):\n",
    "    axes[0].text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{val:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# DT vs RF accuracy comparison bar chart\n",
    "models = ['Decision Tree', 'Random Forest']\n",
    "accuracies = [\n",
    "    accuracy_score(y_test_c, y_pred_dt),\n",
    "    accuracy_score(y_test_c, y_pred_rf)\n",
    "]\n",
    "bar_colors = ['#FF9800', '#4CAF50']\n",
    "axes[1].bar(models, accuracies, color=bar_colors, width=0.5, edgecolor='white', linewidth=1.5)\n",
    "axes[1].set_ylim(0.8, 1.05)\n",
    "axes[1].set_title('Accuracy: DT vs Random Forest', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "for i, acc in enumerate(accuracies):\n",
    "    axes[1].text(i, acc + 0.005, f'{acc:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-random-forest"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regresi Logistik\n",
    "\n",
    "**Logistic Regression** menggunakan fungsi sigmoid untuk menghasilkan probabilitas kelas.  \n",
    "Di sini kita buat problem biner: **setosa vs bukan setosa**.  \n",
    "Kita evaluasi dengan **ROC Curve** dan **AUC Score**."
   ],
   "id": "cell-md-logreg"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Logistic Regression – Binary: setosa vs rest\n",
    "# ============================================================\n",
    "\n",
    "# Create binary target: setosa (0) vs non-setosa (1)\n",
    "y_binary = (y_iris.values != 0).astype(int)  # 0=setosa, 1=non-setosa\n",
    "\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n",
    "    X_iris.values, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_b_sc = scaler.fit_transform(X_train_b)\n",
    "X_test_b_sc  = scaler.transform(X_test_b)\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_b_sc, y_train_b)\n",
    "\n",
    "y_pred_lr   = log_reg.predict(X_test_b_sc)\n",
    "y_proba_lr  = log_reg.predict_proba(X_test_b_sc)[:, 1]\n",
    "\n",
    "print('=== Logistic Regression (Binary Classification) ===')\n",
    "print(f'Accuracy: {accuracy_score(y_test_b, y_pred_lr):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test_b, y_pred_lr, target_names=['setosa', 'non-setosa']))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_b, y_proba_lr)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, color='darkorange', lw=2.5, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', label='Random Classifier')\n",
    "axes[0].fill_between(fpr, tpr, alpha=0.1, color='darkorange')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[0].set_title('ROC Curve – Logistic Regression', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='lower right', fontsize=10)\n",
    "\n",
    "# Probability distribution\n",
    "setosa_proba    = y_proba_lr[y_test_b == 0]\n",
    "nonsetosa_proba = y_proba_lr[y_test_b == 1]\n",
    "axes[1].hist(setosa_proba,    bins=10, alpha=0.7, color='steelblue', label='Setosa (actual)')\n",
    "axes[1].hist(nonsetosa_proba, bins=10, alpha=0.7, color='tomato',    label='Non-setosa (actual)')\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', label='Threshold = 0.5')\n",
    "axes[1].set_title('Predicted Probability Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Probability (non-setosa)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-logistic-regression"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering dengan K-Means\n",
    "\n",
    "**K-Means** mengelompokkan data menjadi K cluster berdasarkan kedekatan ke centroid.  \n",
    "Kita gunakan **Elbow Method** dan **Silhouette Score** untuk menemukan K optimal."
   ],
   "id": "cell-md-kmeans"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# K-Means Clustering\n",
    "# ============================================================\n",
    "\n",
    "X_cluster = X_iris.values\n",
    "\n",
    "# Elbow Method & Silhouette Score\n",
    "inertias      = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 9)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_cluster)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_cluster, labels))\n",
    "\n",
    "print('=== K-Means: Elbow & Silhouette Analysis ===')\n",
    "print(f'  {\"K\":>3}  {\"Inertia\":>12}  {\"Silhouette\":>12}')\n",
    "print('  ' + '-' * 30)\n",
    "for k, inertia, sil in zip(K_range, inertias, silhouette_scores):\n",
    "    marker = ' ← optimal' if k == 3 else ''\n",
    "    print(f'  {k:>3}  {inertia:>12.2f}  {sil:>12.4f}{marker}')\n",
    "\n",
    "# Final K-Means with K=3\n",
    "best_k = 3\n",
    "km_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "cluster_labels = km_final.fit_predict(X_cluster)\n",
    "final_silhouette = silhouette_score(X_cluster, cluster_labels)\n",
    "\n",
    "print(f'\\nFinal K-Means (K={best_k})')\n",
    "print(f'  Inertia (WCSS): {km_final.inertia_:.2f}')\n",
    "print(f'  Silhouette Score: {final_silhouette:.4f}')\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(list(K_range), inertias, 'bo-', linewidth=2.5, markersize=8)\n",
    "axes[0].axvline(x=best_k, color='red', linestyle='--', alpha=0.7, label=f'K={best_k} (elbow)')\n",
    "axes[0].set_title('Elbow Method', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Clusters K')\n",
    "axes[0].set_ylabel('Inertia (WCSS)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Silhouette score plot\n",
    "axes[1].plot(list(K_range), silhouette_scores, 'rs-', linewidth=2.5, markersize=8)\n",
    "axes[1].axvline(x=best_k, color='blue', linestyle='--', alpha=0.7, label=f'K={best_k}')\n",
    "axes[1].set_title('Silhouette Score vs K', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Clusters K')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].legend()\n",
    "\n",
    "# Cluster visualization\n",
    "cluster_colors = ['#e41a1c', '#377eb8', '#4daf4a']\n",
    "for c in range(best_k):\n",
    "    mask = cluster_labels == c\n",
    "    axes[2].scatter(\n",
    "        X_cluster[mask, 0], X_cluster[mask, 1],\n",
    "        label=f'Cluster {c+1}', alpha=0.7,\n",
    "        color=cluster_colors[c], s=60\n",
    "    )\n",
    "# Plot centroids\n",
    "centroids = km_final.cluster_centers_\n",
    "axes[2].scatter(\n",
    "    centroids[:, 0], centroids[:, 1],\n",
    "    marker='X', s=200, color='black',\n",
    "    zorder=10, label='Centroids'\n",
    ")\n",
    "axes[2].set_title(f'K-Means Clusters (K={best_k})', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Sepal Length (cm)')\n",
    "axes[2].set_ylabel('Sepal Width (cm)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-kmeans"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluasi & Perbandingan Model\n",
    "\n",
    "Kita bandingkan semua classifier menggunakan **5-fold Cross-Validation** untuk evaluasi yang lebih andal."
   ],
   "id": "cell-md-eval"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cross-Validation Comparison\n",
    "# ============================================================\n",
    "\n",
    "classifiers = {\n",
    "    'Decision Tree (d=4)': DecisionTreeClassifier(max_depth=4, random_state=42),\n",
    "    'Random Forest':       RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "\n",
    "print('=== 5-Fold Cross-Validation Results ===')\n",
    "print(f'{\"Model\":<25} {\"Mean Acc\":>10} {\"Std\":>8} {\"Min\":>8} {\"Max\":>8}')\n",
    "print('-' * 62)\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, X_iris.values, y_iris.values, cv=cv, scoring='accuracy')\n",
    "    results[name] = scores\n",
    "    print(f'{name:<25} {scores.mean():>10.4f} {scores.std():>8.4f} {scores.min():>8.4f} {scores.max():>8.4f}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot of CV scores\n",
    "names  = list(results.keys())\n",
    "scores = list(results.values())\n",
    "bp = axes[0].boxplot(scores, labels=[n.split(' (')[0] for n in names],\n",
    "                     patch_artist=True, notch=True)\n",
    "box_colors = ['#FF9800', '#4CAF50', '#2196F3']\n",
    "for patch, color in zip(bp['boxes'], box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[0].set_title('Cross-Validation Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim(0.85, 1.02)\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Bar chart of mean accuracy\n",
    "means = [s.mean() for s in scores]\n",
    "stds  = [s.std() for s in scores]\n",
    "short_names = [n.split(' (')[0] for n in names]\n",
    "bars = axes[1].bar(short_names, means, color=box_colors, yerr=stds,\n",
    "                   capsize=6, edgecolor='white', linewidth=1.5, alpha=0.85)\n",
    "axes[1].set_ylim(0.8, 1.05)\n",
    "axes[1].set_title('Mean Accuracy Comparison (5-fold CV)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Mean Accuracy')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "for bar, mean in zip(bars, means):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f'{mean:.4f}', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': names,\n",
    "    'Mean CV Accuracy': [s.mean() for s in scores],\n",
    "    'Std': [s.std() for s in scores],\n",
    "    'Min': [s.min() for s in scores],\n",
    "    'Max': [s.max() for s in scores]\n",
    "}).set_index('Model').round(4)\n",
    "\n",
    "print('\\n=== Summary Table ===')\n",
    "print(summary_df.to_string())"
   ],
   "id": "cell-comparison"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Kerjakan tugas berikut dan lampirkan hasil beserta analisis singkat:\n",
    "\n",
    "---\n",
    "\n",
    "**Tugas 1 – Regresi Lanjutan**  \n",
    "Muat dataset `california_housing` dari `sklearn.datasets`. Latih `LinearRegression` dan evaluasi dengan MSE, RMSE, MAE, dan R². Bandingkan hasilnya dengan `Ridge` dan `Lasso` regression. Fitur mana yang paling berpengaruh?\n",
    "\n",
    "**Tugas 2 – Klasifikasi Multi-kelas**  \n",
    "Gunakan dataset `digits` (handwritten digit recognition) dari sklearn. Latih `RandomForestClassifier` dengan berbagai nilai `n_estimators` (50, 100, 200, 500). Buat plot akurasi vs jumlah estimator. Visualisasikan beberapa digit beserta prediksinya.\n",
    "\n",
    "**Tugas 3 – Perbandingan Algoritma Klasifikasi**  \n",
    "Muat dataset `breast_cancer` dari sklearn. Bandingkan `DecisionTreeClassifier`, `RandomForestClassifier`, dan `LogisticRegression` menggunakan 10-fold cross-validation. Buat heatmap confusion matrix untuk masing-masing model.\n",
    "\n",
    "**Tugas 4 – Clustering Lanjutan**  \n",
    "Gunakan dataset `make_blobs` untuk membuat data sintetis dengan 5 cluster dan 2 fitur. Terapkan K-Means, DBSCAN (`from sklearn.cluster import DBSCAN`), dan Agglomerative Clustering. Bandingkan hasilnya menggunakan Silhouette Score dan visualisasikan ketiga hasilnya berdampingan.\n",
    "\n",
    "**Tugas 5 – Pipeline Dasar**  \n",
    "Bangun `sklearn.pipeline.Pipeline` yang menggabungkan `StandardScaler` → `PCA(n_components=2)` → `RandomForestClassifier` pada dataset Iris. Gunakan 5-fold cross-validation untuk mengevaluasi. Bandingkan hasilnya dengan Random Forest tanpa preprocessing."
   ],
   "id": "cell-md-tasks"
  }
 ]
}

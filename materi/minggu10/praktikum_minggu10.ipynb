{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minggu 10: Advanced Machine Learning\n",
    "## Week 10: Advanced Machine Learning Techniques\n",
    "\n",
    "**Mata Kuliah / Course:** Big Data Analytics  \n",
    "**Topik / Topic:** Ensemble Methods, Feature Engineering, Pipeline, Hyperparameter Tuning, Class Imbalance\n",
    "\n",
    "---\n",
    "\n",
    "### Deskripsi\n",
    "Praktikum ini membahas teknik-teknik Machine Learning tingkat lanjut, meliputi:\n",
    "- Ensemble methods: Random Forest, AdaBoost, Gradient Boosting, XGBoost\n",
    "- Feature engineering dan seleksi fitur\n",
    "- ML Pipeline yang reproducible\n",
    "- Hyperparameter tuning dengan Grid Search dan Random Search\n",
    "- Penanganan class imbalance dengan SMOTE"
   ],
   "id": "cell-md-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install xgboost imbalanced-learn --quiet\n",
    "\n",
    "# ============================================================\n",
    "# Import Libraries\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "\n",
    "# Preprocessing & Model Selection\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, AdaBoostClassifier,\n",
    "    GradientBoostingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    f1_score, roc_auc_score, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print('All libraries imported successfully!')"
   ],
   "id": "cell-imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ensemble Methods\n",
    "\n",
    "Kita bandingkan tiga ensemble method utama pada dataset **Breast Cancer**:\n",
    "- **Random Forest** (Bagging)\n",
    "- **AdaBoost** (Boosting – adaptive weights)\n",
    "- **Gradient Boosting** (Boosting – residual fitting)"
   ],
   "id": "cell-md-ensemble"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ensemble Methods Comparison – Breast Cancer Dataset\n",
    "# ============================================================\n",
    "\n",
    "# Load data\n",
    "bc = load_breast_cancer()\n",
    "X_bc = pd.DataFrame(bc.data, columns=bc.feature_names)\n",
    "y_bc = pd.Series(bc.target)\n",
    "\n",
    "print('=== Breast Cancer Dataset ===')\n",
    "print(f'Shape: {X_bc.shape}')\n",
    "print(f'Classes: {bc.target_names}')\n",
    "print(f'Class distribution: {dict(zip(bc.target_names, np.bincount(y_bc)))}')\n",
    "\n",
    "# Split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_bc.values, y_bc.values,\n",
    "                                            test_size=0.2, random_state=42, stratify=y_bc.values)\n",
    "\n",
    "# Define ensemble models\n",
    "ensemble_models = {\n",
    "    'Random Forest':      RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost':           AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting':  GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "ensemble_results = {}\n",
    "\n",
    "print('\\n=== Ensemble Methods – 5-Fold Cross-Validation ===')\n",
    "print(f'{\"Model\":<22} {\"CV Mean\":>9} {\"CV Std\":>8} {\"Test Acc\":>10} {\"F1\":>8} {\"AUC\":>8}')\n",
    "print('-' * 70)\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    cv_scores = cross_val_score(model, X_bc.values, y_bc.values, cv=cv, scoring='accuracy')\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_prob = model.predict_proba(X_te)[:, 1]\n",
    "    test_acc = accuracy_score(y_te, y_pred)\n",
    "    f1 = f1_score(y_te, y_pred)\n",
    "    auc = roc_auc_score(y_te, y_prob)\n",
    "    ensemble_results[name] = {'cv': cv_scores, 'test_acc': test_acc, 'f1': f1, 'auc': auc}\n",
    "    print(f'{name:<22} {cv_scores.mean():>9.4f} {cv_scores.std():>8.4f} {test_acc:>10.4f} {f1:>8.4f} {auc:>8.4f}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot of CV scores\n",
    "cv_data = [v['cv'] for v in ensemble_results.values()]\n",
    "bp = axes[0].boxplot(cv_data, labels=list(ensemble_results.keys()),\n",
    "                     patch_artist=True, notch=False)\n",
    "colors_box = ['#4CAF50', '#FF9800', '#2196F3']\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[0].set_title('CV Accuracy Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim(0.88, 1.02)\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Grouped bar chart: test metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {name: [v['test_acc'], v['f1'], v['auc']] for name, v in ensemble_results.items()},\n",
    "    index=['Accuracy', 'F1-Score', 'ROC-AUC']\n",
    ")\n",
    "metrics_df.plot(kind='bar', ax=axes[1], color=colors_box, alpha=0.8, edgecolor='white')\n",
    "axes[1].set_title('Test Set Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_ylim(0.88, 1.02)\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-ensemble"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost\n",
    "\n",
    "**XGBoost** adalah implementasi Gradient Boosting yang dioptimalkan — mendukung regularisasi, missing values, dan komputasi paralel.  \n",
    "Sering menjadi pilihan utama dalam kompetisi ML."
   ],
   "id": "cell-md-xgb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# XGBoost Classifier\n",
    "# ============================================================\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_clf.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_te, y_te)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "y_pred_xgb  = xgb_clf.predict(X_te)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_te)[:, 1]\n",
    "\n",
    "print('=== XGBoost Classifier ===')\n",
    "print(f'Test Accuracy : {accuracy_score(y_te, y_pred_xgb):.4f}')\n",
    "print(f'F1 Score      : {f1_score(y_te, y_pred_xgb):.4f}')\n",
    "print(f'ROC-AUC       : {roc_auc_score(y_te, y_proba_xgb):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_te, y_pred_xgb, target_names=bc.target_names))\n",
    "\n",
    "# Feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top-15 important features\n",
    "feat_imp = pd.Series(xgb_clf.feature_importances_, index=bc.feature_names)\n",
    "top15 = feat_imp.nlargest(15)\n",
    "top15.sort_values().plot(kind='barh', ax=axes[0], color='steelblue', alpha=0.8)\n",
    "axes[0].set_title('XGBoost – Top 15 Feature Importance', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Importance Score')\n",
    "\n",
    "# Training loss curve (logloss)\n",
    "evals_result = xgb_clf.evals_result()\n",
    "train_loss = evals_result['validation_0']['logloss']\n",
    "axes[1].plot(range(len(train_loss)), train_loss, 'b-', linewidth=2, label='Validation LogLoss')\n",
    "axes[1].set_title('XGBoost – Training Curve (Validation LogLoss)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Boosting Round')\n",
    "axes[1].set_ylabel('Log Loss')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add XGBoost to comparison\n",
    "print('\\n=== Adding XGBoost to Comparison ===')\n",
    "all_models = list(ensemble_results.keys()) + ['XGBoost']\n",
    "all_aucs = [v['auc'] for v in ensemble_results.values()] + [roc_auc_score(y_te, y_proba_xgb)]\n",
    "for m, a in zip(all_models, all_aucs):\n",
    "    print(f'  {m:<22} AUC = {a:.4f}')"
   ],
   "id": "cell-xgboost"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Selection\n",
    "\n",
    "Feature selection memilih subset fitur paling relevan, sedangkan PCA mengekstrak komponen utama dari semua fitur.  \n",
    "Kita demonstrasikan **SelectKBest**, **SelectFromModel**, dan **PCA**."
   ],
   "id": "cell-md-feature"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Feature Selection & Extraction\n",
    "# ============================================================\n",
    "\n",
    "print(f'Original number of features: {X_bc.shape[1]}')\n",
    "print()\n",
    "\n",
    "# 1. SelectKBest (filter method)\n",
    "k_best = 10\n",
    "selector_kbest = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_kbest = selector_kbest.fit_transform(X_bc.values, y_bc.values)\n",
    "selected_kbest = X_bc.columns[selector_kbest.get_support()].tolist()\n",
    "print(f'SelectKBest (k={k_best}): {X_kbest.shape[1]} features selected')\n",
    "print(f'  Selected: {selected_kbest}')\n",
    "\n",
    "# 2. SelectFromModel (embedded method) using Random Forest\n",
    "rf_for_sel = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_for_sel.fit(X_bc.values, y_bc.values)\n",
    "selector_sfm = SelectFromModel(rf_for_sel, threshold='mean', prefit=True)\n",
    "X_sfm = selector_sfm.transform(X_bc.values)\n",
    "selected_sfm = X_bc.columns[selector_sfm.get_support()].tolist()\n",
    "print(f'\\nSelectFromModel (RF): {X_sfm.shape[1]} features selected')\n",
    "print(f'  Selected: {selected_sfm}')\n",
    "\n",
    "# 3. PCA\n",
    "scaler_pca = StandardScaler()\n",
    "X_scaled = scaler_pca.fit_transform(X_bc.values)\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "print(f'\\nPCA (10 components):')\n",
    "print(f'  Total explained variance: {explained_var.sum()*100:.2f}%')\n",
    "print(f'  Variance per component: {[f\"{v*100:.1f}%\" for v in explained_var[:5]]} ...')\n",
    "\n",
    "# Compare CV accuracy with different feature sets\n",
    "clf_comp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "cv_full   = cross_val_score(clf_comp, X_bc.values, y_bc.values, cv=5).mean()\n",
    "cv_kbest  = cross_val_score(clf_comp, X_kbest, y_bc.values, cv=5).mean()\n",
    "cv_sfm    = cross_val_score(clf_comp, X_sfm, y_bc.values, cv=5).mean()\n",
    "cv_pca    = cross_val_score(clf_comp, X_pca, y_bc.values, cv=5).mean()\n",
    "\n",
    "print('\\n=== CV Accuracy with Different Feature Sets ===')\n",
    "feat_comparison = {\n",
    "    f'All features ({X_bc.shape[1]})': cv_full,\n",
    "    f'SelectKBest ({k_best})': cv_kbest,\n",
    "    f'SelectFromModel ({len(selected_sfm)})': cv_sfm,\n",
    "    'PCA (10 components)': cv_pca,\n",
    "}\n",
    "for method, score in feat_comparison.items():\n",
    "    print(f'  {method:<35} Accuracy = {score:.4f}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PCA scree plot (explained variance)\n",
    "pca_full = PCA(n_components=20)\n",
    "pca_full.fit(X_scaled)\n",
    "cum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "axes[0].bar(range(1, 21), pca_full.explained_variance_ratio_[:20],\n",
    "            color='steelblue', alpha=0.8, label='Individual')\n",
    "ax2 = axes[0].twinx()\n",
    "ax2.plot(range(1, 21), cum_var[:20], 'ro-', linewidth=2, label='Cumulative')\n",
    "ax2.axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "ax2.set_ylabel('Cumulative Explained Variance', color='red')\n",
    "ax2.set_ylim(0, 1.05)\n",
    "axes[0].set_title('PCA Scree Plot', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio', color='steelblue')\n",
    "lines1, labels1 = axes[0].get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "axes[0].legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "# Accuracy comparison\n",
    "bar_data = list(feat_comparison.values())\n",
    "bar_labels = list(feat_comparison.keys())\n",
    "bar_colors = ['#4CAF50', '#2196F3', '#FF9800', '#9C27B0']\n",
    "bars = axes[1].bar(range(len(bar_data)), bar_data, color=bar_colors, alpha=0.8,\n",
    "                   edgecolor='white', linewidth=1.5)\n",
    "axes[1].set_xticks(range(len(bar_data)))\n",
    "axes[1].set_xticklabels(bar_labels, rotation=20, ha='right', fontsize=9)\n",
    "axes[1].set_ylim(0.90, 1.02)\n",
    "axes[1].set_title('Accuracy vs Feature Selection Method', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Mean CV Accuracy')\n",
    "for bar, val in zip(bars, bar_data):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                 f'{val:.4f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-feature-selection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML Pipeline dengan sklearn\n",
    "\n",
    "**Pipeline** menggabungkan preprocessing dan modeling dalam satu objek yang aman dari data leakage."
   ],
   "id": "cell-md-pipeline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ML Pipeline dengan ColumnTransformer\n",
    "# ============================================================\n",
    "\n",
    "# Create a mixed-type dataset (numeric + categorical simulation)\n",
    "# Use breast_cancer numeric features + add a simulated categorical feature\n",
    "X_mixed = X_bc.copy()\n",
    "X_mixed['risk_category'] = pd.cut(\n",
    "    X_bc['mean radius'], bins=3,\n",
    "    labels=['low', 'medium', 'high']\n",
    ").astype(str)\n",
    "\n",
    "numeric_features = X_bc.columns.tolist()\n",
    "categorical_features = ['risk_category']\n",
    "\n",
    "print('=== Mixed-Type Dataset ===')\n",
    "print(f'Numeric features: {len(numeric_features)}')\n",
    "print(f'Categorical features: {categorical_features}')\n",
    "print(f'\\nCategory distribution:\\n{X_mixed[\"risk_category\"].value_counts()}')\n",
    "\n",
    "# Build Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Full pipeline: preprocessor + classifier\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Split & Evaluate\n",
    "X_mix_tr, X_mix_te, y_mix_tr, y_mix_te = train_test_split(\n",
    "    X_mixed, y_bc.values, test_size=0.2, random_state=42, stratify=y_bc.values\n",
    ")\n",
    "\n",
    "full_pipeline.fit(X_mix_tr, y_mix_tr)\n",
    "y_pred_pipe = full_pipeline.predict(X_mix_te)\n",
    "\n",
    "print('\\n=== Full Pipeline Results ===')\n",
    "print(f'Accuracy: {accuracy_score(y_mix_te, y_pred_pipe):.4f}')\n",
    "print(f'F1-Score: {f1_score(y_mix_te, y_pred_pipe):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_mix_te, y_pred_pipe, target_names=bc.target_names))\n",
    "\n",
    "# Pipeline diagram\n",
    "print('\\n=== Pipeline Structure ===')\n",
    "print('Pipeline([')\n",
    "print('  (\"preprocessor\", ColumnTransformer([')\n",
    "print('    (\"num\", Pipeline([imputer, scaler]), numeric_features),')\n",
    "print('    (\"cat\", Pipeline([imputer, onehot]), categorical_features)')\n",
    "print('  ])),')\n",
    "print('  (\"classifier\", RandomForestClassifier())')\n",
    "print('])')\n",
    "\n",
    "# CV on pipeline\n",
    "cv_pipe_scores = cross_val_score(full_pipeline, X_mixed, y_bc.values, cv=5)\n",
    "print(f'\\n5-fold CV: {cv_pipe_scores.mean():.4f} ± {cv_pipe_scores.std():.4f}')"
   ],
   "id": "cell-pipeline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning\n",
    "\n",
    "Kita gunakan **GridSearchCV** (exhaustive) dan **RandomizedSearchCV** (random sampling) pada Random Forest."
   ],
   "id": "cell-md-tuning"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Hyperparameter Tuning: GridSearch vs RandomSearch\n",
    "# ============================================================\n",
    "\n",
    "X_ht = X_bc.values\n",
    "y_ht = y_bc.values\n",
    "\n",
    "# Base model\n",
    "base_rf = RandomForestClassifier(random_state=42)\n",
    "base_cv = cross_val_score(base_rf, X_ht, y_ht, cv=5).mean()\n",
    "print(f'Baseline RF (default params) CV Accuracy: {base_cv:.4f}')\n",
    "\n",
    "# ---- Grid Search ----\n",
    "grid_param = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(f'\\nGrid Search – total combinations: {3*3*2*2} (n_est x depth x split x features)')\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=grid_param,\n",
    "    cv=5, scoring='accuracy',\n",
    "    n_jobs=-1, verbose=0\n",
    ")\n",
    "grid_search.fit(X_ht, y_ht)\n",
    "print(f'Grid Search – Best CV Accuracy: {grid_search.best_score_:.4f}')\n",
    "print(f'Grid Search – Best Params: {grid_search.best_params_}')\n",
    "\n",
    "# ---- Random Search ----\n",
    "from scipy.stats import randint\n",
    "rand_param = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'max_depth': [None, 3, 5, 7, 10, 15, 20],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions=rand_param,\n",
    "    n_iter=50, cv=5, scoring='accuracy',\n",
    "    n_jobs=-1, random_state=42, verbose=0\n",
    ")\n",
    "rand_search.fit(X_ht, y_ht)\n",
    "print(f'\\nRandom Search (50 iter) – Best CV Accuracy: {rand_search.best_score_:.4f}')\n",
    "print(f'Random Search – Best Params: {rand_search.best_params_}')\n",
    "\n",
    "# Compare results\n",
    "print('\\n=== Tuning Results Summary ===')\n",
    "print(f'{\"Method\":<25} {\"Best Accuracy\":>15} {\"# Evaluations\":>15}')\n",
    "print('-' * 57)\n",
    "print(f'{\"Baseline (default)\":<25} {base_cv:>15.4f} {\"N/A\":>15}')\n",
    "print(f'{\"Grid Search\":<25} {grid_search.best_score_:>15.4f} {grid_search.cv_results_[\"params\"].__len__():>15}')\n",
    "print(f'{\"Random Search (50)\":<25} {rand_search.best_score_:>15.4f} {\"50\":>15}')\n",
    "\n",
    "# Visualization: CV score distribution for GridSearch\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "gs_scores = grid_search.cv_results_['mean_test_score']\n",
    "axes[0].hist(gs_scores, bins=15, color='steelblue', alpha=0.8, edgecolor='white')\n",
    "axes[0].axvline(x=grid_search.best_score_, color='red', linestyle='--',\n",
    "                linewidth=2, label=f'Best = {grid_search.best_score_:.4f}')\n",
    "axes[0].set_title('Grid Search – Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('CV Mean Accuracy')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "\n",
    "rs_scores = rand_search.cv_results_['mean_test_score']\n",
    "axes[1].hist(rs_scores, bins=15, color='darkorange', alpha=0.8, edgecolor='white')\n",
    "axes[1].axvline(x=rand_search.best_score_, color='red', linestyle='--',\n",
    "                linewidth=2, label=f'Best = {rand_search.best_score_:.4f}')\n",
    "axes[1].set_title('Random Search – Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('CV Mean Accuracy')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-hypertuning"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Menangani Class Imbalance\n",
    "\n",
    "Class imbalance menyebabkan model bias ke kelas mayoritas.  \n",
    "Kita bandingkan: **tanpa treatment**, **SMOTE oversampling**, dan **class_weight='balanced'**."
   ],
   "id": "cell-md-imbalance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Class Imbalance: SMOTE vs class_weight vs no treatment\n",
    "# ============================================================\n",
    "\n",
    "# Create highly imbalanced dataset\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    weights=[0.90, 0.10],   # 90% majority, 10% minority\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print('=== Imbalanced Dataset ===')\n",
    "unique, counts = np.unique(y_imb, return_counts=True)\n",
    "for cls, cnt in zip(unique, counts):\n",
    "    print(f'  Class {cls}: {cnt} samples ({cnt/len(y_imb)*100:.1f}%)')\n",
    "\n",
    "# Split\n",
    "X_imb_tr, X_imb_te, y_imb_tr, y_imb_te = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "# ---- Strategy 1: No treatment ----\n",
    "clf_no_treat = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_no_treat.fit(X_imb_tr, y_imb_tr)\n",
    "y_pred_no_treat = clf_no_treat.predict(X_imb_te)\n",
    "\n",
    "# ---- Strategy 2: SMOTE ----\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_imb_tr, y_imb_tr)\n",
    "print(f'\\nAfter SMOTE resampling:')\n",
    "unique_s, counts_s = np.unique(y_smote, return_counts=True)\n",
    "for cls, cnt in zip(unique_s, counts_s):\n",
    "    print(f'  Class {cls}: {cnt} samples')\n",
    "\n",
    "clf_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_smote.fit(X_smote, y_smote)\n",
    "y_pred_smote = clf_smote.predict(X_imb_te)\n",
    "\n",
    "# ---- Strategy 3: class_weight='balanced' ----\n",
    "clf_weighted = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight='balanced', random_state=42\n",
    ")\n",
    "clf_weighted.fit(X_imb_tr, y_imb_tr)\n",
    "y_pred_weighted = clf_weighted.predict(X_imb_te)\n",
    "\n",
    "# Compare results – focus on minority class (class 1) recall\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    return {\n",
    "        'Strategy': label,\n",
    "        'Overall Accuracy': f'{acc:.4f}',\n",
    "        'Majority Precision': f'{p[0]:.4f}',\n",
    "        'Minority Precision': f'{p[1]:.4f}',\n",
    "        'Majority Recall':    f'{r[0]:.4f}',\n",
    "        'Minority Recall':    f'{r[1]:.4f}',\n",
    "        'Minority F1':        f'{f[1]:.4f}'\n",
    "    }\n",
    "\n",
    "results_imb = [\n",
    "    get_metrics(y_imb_te, y_pred_no_treat, 'No Treatment'),\n",
    "    get_metrics(y_imb_te, y_pred_smote,    'SMOTE'),\n",
    "    get_metrics(y_imb_te, y_pred_weighted, 'class_weight=balanced'),\n",
    "]\n",
    "df_imb = pd.DataFrame(results_imb).set_index('Strategy')\n",
    "print('\\n=== Class Imbalance Strategies Comparison ===')\n",
    "print(df_imb.to_string())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Distribution comparison\n",
    "dist_data = {\n",
    "    'Original\\nTrain': [np.sum(y_imb_tr == 0), np.sum(y_imb_tr == 1)],\n",
    "    'After SMOTE': [np.sum(y_smote == 0), np.sum(y_smote == 1)],\n",
    "}\n",
    "x_pos = np.arange(len(dist_data))\n",
    "width = 0.35\n",
    "keys = list(dist_data.keys())\n",
    "axes[0].bar(x_pos - width/2, [dist_data[k][0] for k in keys], width,\n",
    "            label='Majority (Class 0)', color='steelblue', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, [dist_data[k][1] for k in keys], width,\n",
    "            label='Minority (Class 1)', color='tomato', alpha=0.8)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(keys)\n",
    "axes[0].set_title('Class Distribution Before/After SMOTE', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Sample Count')\n",
    "axes[0].legend()\n",
    "\n",
    "# Minority recall comparison\n",
    "strategies = ['No Treatment', 'SMOTE', 'class_weight']\n",
    "min_recalls = [\n",
    "    float(df_imb.loc['No Treatment', 'Minority Recall']),\n",
    "    float(df_imb.loc['SMOTE', 'Minority Recall']),\n",
    "    float(df_imb.loc['class_weight=balanced', 'Minority Recall'])\n",
    "]\n",
    "axes[1].bar(strategies, min_recalls, color=['#FF5252', '#4CAF50', '#2196F3'],\n",
    "            alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "axes[1].set_title('Minority Class Recall', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Recall')\n",
    "for i, val in enumerate(min_recalls):\n",
    "    axes[1].text(i, val + 0.02, f'{val:.4f}', ha='center', fontweight='bold', fontsize=10)\n",
    "axes[1].tick_params(axis='x', rotation=10)\n",
    "\n",
    "# Minority F1 comparison\n",
    "min_f1s = [\n",
    "    float(df_imb.loc['No Treatment', 'Minority F1']),\n",
    "    float(df_imb.loc['SMOTE', 'Minority F1']),\n",
    "    float(df_imb.loc['class_weight=balanced', 'Minority F1'])\n",
    "]\n",
    "axes[2].bar(strategies, min_f1s, color=['#FF5252', '#4CAF50', '#2196F3'],\n",
    "            alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "axes[2].set_ylim(0, 1.1)\n",
    "axes[2].set_title('Minority Class F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('F1-Score')\n",
    "for i, val in enumerate(min_f1s):\n",
    "    axes[2].text(i, val + 0.02, f'{val:.4f}', ha='center', fontweight='bold', fontsize=10)\n",
    "axes[2].tick_params(axis='x', rotation=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-imbalance"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stacking Classifier\n",
    "\n",
    "**Stacking** menggunakan prediksi dari beberapa base learners sebagai input untuk meta-learner.  \n",
    "Ini menggabungkan kelebihan dari setiap model secara cerdas."
   ],
   "id": "cell-md-stacking"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Stacking Classifier\n",
    "# ============================================================\n",
    "\n",
    "# Define base learners (diverse models)\n",
    "base_estimators = [\n",
    "    ('rf',   RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb',   GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('dt',   DecisionTreeClassifier(max_depth=5, random_state=42)),\n",
    "    ('svc',  SVC(probability=True, random_state=42)),\n",
    "]\n",
    "\n",
    "# Meta-learner\n",
    "meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Build StackingClassifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    passthrough=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Evaluate with 5-fold CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Compare all models\n",
    "final_models = {\n",
    "    'Decision Tree':      DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest':      RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting':  GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost':            XGBClassifier(n_estimators=100, eval_metric='logloss', random_state=42),\n",
    "    'Stacking':           stacking_clf,\n",
    "}\n",
    "\n",
    "print('=== Final Model Comparison (5-fold CV on Breast Cancer) ===')\n",
    "print(f'{\"Model\":<22} {\"Mean Accuracy\":>15} {\"Std\":>8}')\n",
    "print('-' * 47)\n",
    "\n",
    "final_results = {}\n",
    "for name, model in final_models.items():\n",
    "    scores = cross_val_score(model, X_bc.values, y_bc.values, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    final_results[name] = scores\n",
    "    marker = ' ★' if scores.mean() == max([s.mean() for s in final_results.values()]) else ''\n",
    "    print(f'{name:<22} {scores.mean():>15.4f} {scores.std():>8.4f}{marker}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Box plot\n",
    "names_f = list(final_results.keys())\n",
    "scores_f = list(final_results.values())\n",
    "bp = axes[0].boxplot(\n",
    "    scores_f,\n",
    "    labels=[n.replace(' ', '\\n') for n in names_f],\n",
    "    patch_artist=True,\n",
    "    notch=False,\n",
    "    medianprops={'color': 'red', 'linewidth': 2}\n",
    ")\n",
    "bp_colors = ['#FF9800', '#4CAF50', '#2196F3', '#9C27B0', '#F44336']\n",
    "for patch, color in zip(bp['boxes'], bp_colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[0].set_title('CV Score Distribution – All Models', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim(0.88, 1.02)\n",
    "\n",
    "# Mean accuracy bar chart\n",
    "means_f = [s.mean() for s in scores_f]\n",
    "stds_f  = [s.std() for s in scores_f]\n",
    "bars = axes[1].bar(\n",
    "    range(len(names_f)), means_f,\n",
    "    color=bp_colors, alpha=0.85,\n",
    "    yerr=stds_f, capsize=5,\n",
    "    edgecolor='white', linewidth=1.5\n",
    ")\n",
    "axes[1].set_xticks(range(len(names_f)))\n",
    "axes[1].set_xticklabels([n.replace(' ', '\\n') for n in names_f], fontsize=9)\n",
    "axes[1].set_ylim(0.88, 1.04)\n",
    "axes[1].set_title('Mean Accuracy ± Std (5-fold CV)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Mean Accuracy')\n",
    "best_idx = np.argmax(means_f)\n",
    "for i, (bar, mean) in enumerate(zip(bars, means_f)):\n",
    "    style = dict(fontweight='bold', fontsize=9)\n",
    "    if i == best_idx:\n",
    "        style['color'] = 'darkred'\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2,\n",
    "                 bar.get_height() + stds_f[i] + 0.002,\n",
    "                 f'{mean:.4f}', ha='center', **style)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n★ Best model:', names_f[np.argmax(means_f)])"
   ],
   "id": "cell-stacking"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas Praktikum\n",
    "\n",
    "Kerjakan tugas berikut dan lampirkan hasil beserta analisis singkat:\n",
    "\n",
    "---\n",
    "\n",
    "**Tugas 1 – Ensemble Comparison**  \n",
    "Gunakan dataset `wine` dari sklearn. Bandingkan `BaggingClassifier`, `AdaBoostClassifier`, `GradientBoostingClassifier`, dan `XGBClassifier` menggunakan 10-fold CV. Buat radar chart (atau bar chart) yang menampilkan accuracy, precision, recall, dan F1 untuk setiap model.\n",
    "\n",
    "**Tugas 2 – Feature Engineering**  \n",
    "Muat dataset `diabetes` dari sklearn. Implementasikan: (a) PolynomialFeatures degree 2, (b) SelectKBest dengan k=5, (c) PCA dengan n_components yang menjelaskan 95% variansi. Bandingkan RMSE prediksi `Ridge` regression menggunakan ketiga pendekatan tersebut.\n",
    "\n",
    "**Tugas 3 – Pipeline Lengkap**  \n",
    "Bangun pipeline ML menggunakan dataset `titanic` (dari seaborn atau sumber lain). Pipeline harus menangani: missing values (imputer), fitur numerik (scaler), fitur kategorikal (OHE), dan classifier (pilih sendiri). Lakukan hyperparameter tuning menggunakan GridSearchCV.\n",
    "\n",
    "**Tugas 4 – Class Imbalance Deep Dive**  \n",
    "Buat dataset imbalanced dengan rasio 95:5 menggunakan `make_classification`. Bandingkan setidaknya 5 strategi: no treatment, random oversampling, SMOTE, ADASYN, dan undersampling (RandomUnderSampler). Plot confusion matrix untuk setiap strategi dan bandingkan recall kelas minoritas.\n",
    "\n",
    "**Tugas 5 – Custom Stacking**  \n",
    "Bangun `StackingClassifier` dengan kombinasi base estimator yang berbeda dari contoh di atas (minimal 4 model berbeda jenis). Coba dua meta-learner yang berbeda. Bandingkan performa stack dengan base estimator terbaik menggunakan nested cross-validation."
   ],
   "id": "cell-md-tasks"
  }
 ]
}
